<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />
<style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1&gt;p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1&gt;p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]&gt;div&gt;h1,#sbo-rt-content section[data-type="preface"]&gt;div&gt;h1,#sbo-rt-content section[data-type="appendix"]&gt;div&gt;h1,#sbo-rt-content section[data-type="glossary"]&gt;div&gt;h1,#sbo-rt-content section[data-type="bibliography"]&gt;div&gt;h1,#sbo-rt-content section[data-type="index"]&gt;div&gt;h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000;padding-top:.25em !important;margin-top:0 !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]&gt;div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dl{margin-bottom:1.5em !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important;line-height:1.25rem;font-style:italic}#sbo-rt-content dd{margin:10px 0 .25em 1.5em !important;line-height:1.65em !important}#sbo-rt-content dd p{padding:0 !important;margin:0 0 10px !important}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul&gt;li,#sbo-rt-content ol ul,#sbo-rt-content ol ul&gt;li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul&gt;li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul&gt;li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul&gt;li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol&gt;li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol&gt;li,#sbo-rt-content ul ol,#sbo-rt-content ul ol&gt;li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol&gt;li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol&gt;li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol&gt;li&gt;ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol&gt;li&gt;ol&gt;li&gt;ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content table li{margin:10px 0 0 .25em !important}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:top;font-size:80%}#sbo-rt-content th{vertical-align:bottom}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller;word-break:break-all}#sbo-rt-content table.border tbody&gt;tr:last-child&gt;td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:2em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content pre.break-code,#sbo-rt-content code.break-code,#sbo-rt-content .break-code pre,#sbo-rt-content .break-code code{word-break:break-all}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10,#sbo-rt-content figure.width-10 img{width:10% !important}#sbo-rt-content .width-20,#sbo-rt-content figure.width-20 img{width:20% !important}#sbo-rt-content .width-30,#sbo-rt-content figure.width-30 img{width:30% !important}#sbo-rt-content .width-40,#sbo-rt-content figure.width-40 img{width:40% !important}#sbo-rt-content .width-50,#sbo-rt-content figure.width-50 img{width:50% !important}#sbo-rt-content .width-60,#sbo-rt-content figure.width-60 img{width:60% !important}#sbo-rt-content .width-70,#sbo-rt-content figure.width-70 img{width:70% !important}#sbo-rt-content .width-80,#sbo-rt-content figure.width-80 img{width:80% !important}#sbo-rt-content .width-90,#sbo-rt-content figure.width-90 img{width:90% !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100% !important}#sbo-rt-content .sc{text-transform:none !important}#sbo-rt-content .right{float:none !important}#sbo-rt-content a.totri-footnote{padding:0 !important}#sbo-rt-content figure.width-10,#sbo-rt-content figure.width-20,#sbo-rt-content figure.width-30,#sbo-rt-content figure.width-40,#sbo-rt-content figure.width-50,#sbo-rt-content figure.width-60,#sbo-rt-content figure.width-70,#sbo-rt-content figure.width-80,#sbo-rt-content figure.width-90{width:auto !important}#sbo-rt-content p img,#sbo-rt-content pre img{width:1.25em;line-height:1em;margin:0 .15em -.2em}#sbo-rt-content figure.no-frame div.border-box{border:none}#sbo-rt-content .right{text-align:right !important}
    </style>
<style type="text/css" id="font-styles">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }</style>
<style type="text/css" id="font-family">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }</style>
<style type="text/css" id="column-width">#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }</style>

<style type="text/css">body{margin:1em;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}body{background-color:transparent!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Text Classification"><div class="chapter" id="text_classification">
<h1><span class="label">Chapter 4. </span>Text Classification</h1>

<blockquote class="right">
<p class="right"><em>Organizing is what you do before you do something,</em><br/> <em>so that when you do it, it is not all mixed up.</em></p>

<p data-type="attribution" style="text-align:right"><em>A.A. Milne</em></p>
</blockquote>

<p>All<a contenteditable="false" data-primary="Milne, A.A." data-type="indexterm" id="idm45969608521608"/> of us check email every day, possibly multiple times. A useful feature of most email service providers is the ability to automatically segregate spam emails away from regular emails. This is a use case of a popular NLP task known as <em>text classification<a contenteditable="false" data-primary="text classification" data-type="indexterm" id="term1"/></em>, which is the focus of this chapter. <a contenteditable="false" data-primary="text classification" data-secondary="definition" data-type="indexterm" id="idm45969608518264"/>Text classification is the task of assigning one or more categories to a given piece of text from a larger set of possible categories. In the email spam–identifier example, we have two categories—spam and non-spam—and each incoming email is assigned to one of these categories. This task of categorizing texts based on some properties has a wide range of applications across diverse domains, such as social media, e-commerce, healthcare, law, and marketing, to name a few. Even though the purpose and application of text classification may vary from domain to domain, the underlying abstract problem remains the same. This invariance of the core problem and its applications in a myriad of domains makes text classification by far the most widely used NLP task in industry and the most researched in academia. In this chapter, we’ll discuss the usefulness of text classification and how to build text classifiers for our use cases, along with some practical tips for real-world scenarios.</p>

<p>In machine learning, classification<a contenteditable="false" data-primary="classification" data-seealso="text classification" data-type="indexterm" id="idm45969608515368"/> is the problem of categorizing a data instance into one or more known classes. The data point can be originally of different formats, such as text, speech, image, or numeric. Text classification is a special instance of the classification problem, where the input data point(s) is text and the goal is to categorize the piece of text into one or more buckets (called a class) from a set of pre-defined buckets (classes). The “text” can be of arbitrary length: a character, a word, a sentence, a paragraph, or a full document. Consider a scenario where we want to <span class="keep-together">classify</span> all customer reviews for a product into three categories: positive, negative, and neutral. The challenge of text classification is to “learn” this categorization from a collection of examples for each of these categories and predict the categories for new, unseen products and new customer reviews. This categorization need not always result in a single category, though, and there can be any number of categories available. Let’s take a quick look at the taxonomy of text classification to understand this.</p>

<p>Any supervised classification approach, including text classification, can be further distinguished into three types based on the number of categories involved: binary, multiclass, and multilabel classification. If the number of classes is two, it’s called <em>binary classification</em><a contenteditable="false" data-primary="binary classification" data-type="indexterm" id="idm45969608510856"/><a contenteditable="false" data-primary="classification" data-secondary="binary" data-type="indexterm" id="idm45969608509752"/>. If the number of classes is more than two, it’s referred to as <em>multiclass classification</em><a contenteditable="false" data-primary="classification" data-secondary="multiclass" data-type="indexterm" id="idm45969608507880"/><a contenteditable="false" data-primary="multiclass classification" data-type="indexterm" id="idm45969608506536"/>. Thus, classifying an email as spam or not-spam is an example of binary classification setting. Classifying the sentiment of a customer review as negative, neutral, or positive is an example of multiclass classification. In both binary and multiclass settings, each document belongs to exactly one class from <em>C</em>, where <em>C</em> is the set of all possible classes. In <em>multilabel classification</em>, a document can have one or more labels/classes attached to it. For example, a news article on a soccer match may belong to more than one category, such as “sports” and “soccer,” simultaneously, whereas another news article on US elections may have the labels “politics,” “USA,” and “elections.” Thus, each document has labels that are a subset of <em>C</em>. Each article can be in no class, exactly one class, multiple classes, or all of the classes. Sometimes, the number of labels in the set <em>C</em> can be very large (known as “extreme classification”). In some other scenarios, we may have a hierarchical classification system, which may result in each text getting different labels at different levels in the hierarchy. In this chapter, we’ll focus only on binary and multiclass classification, as those are the most common use cases of text classification in the industry.</p>

<p>Text classification is sometimes also referred to as <em>topic classification</em><a contenteditable="false" data-primary="topic classification" data-seealso="text classification" data-type="indexterm" id="idm45969608501064"/><a contenteditable="false" data-primary="classification" data-secondary="topic" data-type="indexterm" id="idm45969608499688"/>, <em>text categorization</em><a contenteditable="false" data-primary="text categorization" data-seealso="text classification" data-type="indexterm" id="idm45969608497864"/>, or <em>document categorization</em><a contenteditable="false" data-primary="document categorization" data-seealso="text classification" data-type="indexterm" id="idm45969608496040"/>. For the rest of this book, we’ll stick to the term “text classification.” Note that topic classification is different from <em>topic detection</em><a contenteditable="false" data-primary="topic detection" data-type="indexterm" id="idm45969608494104"/>, which refers to the problem of uncovering or extracting “topics” from texts, which we’ll study in <a data-type="xref" href="ch07.xhtml#topics_in_brief">Chapter 7</a>.</p>

<p>In this chapter, we’ll take a closer look at text classification and build text classifiers using different approaches. Our aim is to provide an overview of some of the most commonly applied techniques along with practical advice on handling different scenarios and decisions that have to be made when building text classification systems in practice. We’ll start by introducing some common applications of text classification, then we’ll discuss what an NLP pipeline for text classification looks like and illustrate the use of this pipeline to train and test text classifiers using different approaches, ranging from the traditional methods to the state of the art. We’ll then tackle the problem of training data collection/sparsity and different methods to handle it. We’ll end the chapter by summarizing what we learned in all these sections along with some practical advice and a case study.</p>

<p>Note that, in this chapter, we’ll only deal with the aspect of training and evaluating the text classifiers. Issues related to deploying NLP systems in general and performing quality assurance will be discussed in <a data-type="xref" href="ch11.xhtml#the_end_to_end_nlp_process">Chapter 11</a>.</p>

<section data-type="sect1" data-pdf-bookmark="Applications"><div class="sect1" id="application">
<h1>Applications</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="applications" data-type="indexterm" id="term2"/>Text classification has been of interest in a number of application scenarios, ranging from identifying the author of an unknown text in the 1800s to the efforts of USPS in the 1960s to perform optical character recognition on addresses and zip codes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608485080-marker" href="ch04.xhtml#idm45969608485080">1</a>]. In the 1990s, researchers began to successfully apply ML algorithms for text classification for large datasets. Email filtering, popularly known as “spam classification,” is one of the earliest examples of automatic text classification, which impacts our lives to this day. From manual analyses of text documents to purely statistical, computer-based approaches and state-of-the-art deep neural networks, we’ve come a long way with text classification. Let’s briefly discuss some of the popular applications before diving into the different approaches to perform text classification. These examples will also be useful in identifying problems that can be solved using text classification methods in your organization.</p>

<dl>
	<dt>Content classification and organization<a contenteditable="false" data-primary="content classification and organization" data-type="indexterm" id="idm45969608481736"/></dt> 
		<dd><p>This refers to the task of classifying/tagging large amounts of textual data. This, in turn, is used to power use cases like content organization, search engines, and recommendation systems, to name a few. Examples of such data include news websites, blogs, online bookshelves, product reviews, tweets, etc.; tagging product descriptions in an e-commerce website; routing customer service requests in a company to the appropriate support team; and organizing emails into personal, social, and promotions in Gmail are all examples of using text classification for content classification and organization.</p></dd>
	<dt><a contenteditable="false" data-primary="customer support" data-secondary="on social channels" data-type="indexterm" id="term3"/>Customer support</dt> 
		<dd><p>Customers often use social media<a contenteditable="false" data-primary="social media" data-type="indexterm" id="idm45969608476728"/> to express their opinions about and experiences of products or services. Text classification is often used to identify the tweets that brands must respond to (i.e., those that are actionable) and those that don’t require a response (i.e., noise) [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608475224-marker" href="ch04.xhtml#idm45969608475224">2</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608474296-marker" href="ch04.xhtml#idm45969608474296">3</a>]. To illustrate, consider the three tweets about the brand Macy’s shown in <a data-type="xref" href="#tweets_reaching_out_to_brandsdotthe_one">Figure 4-1</a>.</p>
		
		<p>Although all three tweets mention the brand Macy’s explicitly, only the first one necessitates a reply from Macy’s customer support team.</p>

		<figure><div id="tweets_reaching_out_to_brandsdotthe_one" class="figure"><img alt="Tweets reaching out to brands. The one is actionable, the other two are noise." src="Images/pnlp_0401.png" width="1199" height="677"/>
		<h6><span class="label">Figure 4-1. </span>Tweets reaching out to brands: one is actionable, the other two are noise</h6>
		</div></figure>
		</dd>
	<dt>E-commerce</dt> 
		<dd><p>Customers leave reviews for a range of products on e-commerce<a contenteditable="false" data-primary="e-commerce and retail" data-type="indexterm" data-secondary="text classification" id="term4"/> websites like Amazon<a contenteditable="false" data-primary="Amazon" data-type="indexterm" id="idm45969608466344"/>, <a contenteditable="false" data-primary="eBay" data-type="indexterm" id="idm45969608465080"/>eBay, etc. An example use of text classification in this kind of scenario is to understand and analyze customers’ perception of a product or service based on their comments. This is commonly known as “sentiment analysis<a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="idm45969608463576"/>.” It’s used extensively by brands across the globe to better understand whether they’re getting closer to or farther away from their customers. Rather than categorizing customer feedback as simply positive, negative, or neutral, over a period of time, sentiment analysis has evolved into a more sophisticated paradigm: “aspect”-based sentiment analysis<a contenteditable="false" data-primary="sentiment analysis" data-secondary="aspect-based" data-type="indexterm" id="term5"/><a contenteditable="false" data-primary="aspect-based sentiment analysis" data-type="indexterm" id="term6"/>. To understand this, consider the customer review of a restaurant shown in <a data-type="xref" href="#a_review_that_praises_some_aspects_and">Figure 4-2</a>.</p>

		<figure><div id="a_review_that_praises_some_aspects_and" class="figure"><img alt="A review that praises some aspects and criticizes few" src="Images/pnlp_0402.png" width="575" height="170"/>
		<h6><span class="label">Figure 4-2. </span>A review that praises some aspects and criticizes few</h6>
		</div></figure>

		<p>Would you call the review in <a data-type="xref" href="#a_review_that_praises_some_aspects_and">Figure 4-2</a> negative, positive, or neutral? It’s difficult to answer this—the food was great, but the service was bad. Practitioners and brands working with sentiment analysis have realized that many products or services have multiple facets. In order to understand overall sentiment, understanding each and every facet is important. Text classification plays a major role in performing such fine-grained analysis of customer feedback. We’ll discuss this specific application in detail in <a data-type="xref" href="ch09.xhtml#e_commerce_and_retail">Chapter 9</a>.<a contenteditable="false" data-primary="e-commerce and retail" data-secondary="text classification in" data-startref="term4" data-type="indexterm" id="idm45969608453208"/><a contenteditable="false" data-primary="sentiment analysis" data-secondary="aspect-based" data-startref="term5" data-type="indexterm" id="idm45969608451528"/><a contenteditable="false" data-primary="aspect-based sentiment analysis" data-startref="term6" data-type="indexterm" id="idm45969608449880"/></p>
	</dd>
	<dt>Other applications</dt> 
		<dd><p>Apart from the above-mentioned areas, text classification is also used in several other applications in various domains:</p>

	<ul>
		<li>
		<p>Text classification is used in language identification<a contenteditable="false" data-primary="language identification" data-type="indexterm" id="idm45969608446024"/>, like identifying the language of new tweets or posts. For example, <a contenteditable="false" data-primary="Google Translate" data-secondary="language identification" data-type="indexterm" id="idm45969608444792"/>Google Translate has an automatic language identification feature.</p>
		</li>
		<li>
		<p><a contenteditable="false" data-primary="authorship attribution" data-type="indexterm" id="idm45969608442392"/>Authorship attribution, or identifying the unknown authors of texts from a pool of authors, is another popular use case of text classification, and it’s used in a range of fields from forensic analysis to literary studies.</p>
		</li>
		<li>
		<p>Text classification has been used in the recent past for triaging posts in an online support<a contenteditable="false" data-primary="online support" data-type="indexterm" id="idm45969608439944"/> forum for mental health services<a contenteditable="false" data-primary="mental healthcare monitoring" data-type="indexterm" id="idm45969608438712"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608437416-marker" href="ch04.xhtml#idm45969608437416">4</a>]. In the NLP community, annual competitions are conducted (e.g., <a contenteditable="false" data-primary="clpsych.org" data-type="indexterm" id="idm45969608436056"/>clpsych.org) for solving such text classification problems originating from clinical research.</p>
		</li>
		<li>
		<p>In the recent past, text classification has also been used to segregate <a contenteditable="false" data-primary="fake news" data-type="indexterm" id="idm45969608433800"/>fake news from real news<a contenteditable="false" data-primary="news classification" data-type="indexterm" id="idm45969608432568"/>.</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Note that this section only serves as an illustration of the wide range of applications of text classification, and the list is not exhaustive, but we hope it gives you enough background to identify text classification problems in your workplace projects when you encounter them. Let’s now look at how to build such text classification<a contenteditable="false" data-primary="text classification" data-secondary="applications" data-startref="term2" data-type="indexterm" id="idm45969608430152"/> models.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="A Pipeline for Building Text Classification Systems"><div class="sect1" id="a_pipeline_for_building_text_classifica">
<h1>A Pipeline for Building Text Classification Systems</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="pipeline for building systems" data-type="indexterm" id="term7"/>In <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a>, we discussed some of the common NLP pipelines. The text classification pipeline shares some of its steps with the pipelines we learned in that chapter.</p>

<p>One typically follows these steps when building a text classification system:</p>

<ol>
	<li>
	<p>Collect or create a labeled dataset suitable for the task.</p>
	</li>
	<li>
	<p>Split the dataset into two (training and test) or three parts: training, validation (i.e., development), and test sets, then decide on evaluation metric(s).</p>
	</li>
	<li>
	<p>Transform raw text into feature vectors.</p>
	</li>
	<li>
	<p>Train a classifier using the feature vectors and the corresponding labels from the training set.</p>
	</li>
	<li>
	<p>Using the evaluation metric(s) from Step 2, benchmark the model performance on the test set.</p>
	</li>
	<li>
	<p>Deploy the model to serve the real-world use case and monitor its performance.</p>
	</li>
</ol>

<p><a data-type="xref" href="#flowchart_of_a_text_classification_pipe">Figure 4-3</a> shows these typical steps in building a text classification system.</p>

<figure><div id="flowchart_of_a_text_classification_pipe" class="figure"><img alt="Flowchart of a text classification pipeline" src="Images/pnlp_0403.png" width="1169" height="997"/>
<h6><span class="label">Figure 4-3. </span>Flowchart of a text classification pipeline</h6>
</div></figure>

<p>Steps 3 through 5 are iterated on to explore different variants of features and classification algorithms and their parameters and to tune the hyperparameters before proceeding to Step 6, deploying the optimal model in production.</p>

<p>Some of the individual steps related to data collection and pre-processing were discussed in past chapters. For example, Steps 1 and 2 were discussed in detail in <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a>. <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a> focused entirely on Step 3. Our focus in this chapter is on Steps 4 through 5. Toward the end of this chapter, we’ll revisit Step 1 to discuss issues specific to text classification. We’ll deal with Step 6 in <a data-type="xref" href="ch11.xhtml#the_end_to_end_nlp_process">Chapter 11</a>. To be able to perform Steps 4 through 5 (i.e., to benchmark the performance of a model or compare multiple classifiers), we need the right measure(s) of evaluation. <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a> discussed various general metrics used in evaluating NLP systems. For evaluating classifiers specifically, among the metrics introduced in <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a>, the following are used more commonly: classification accuracy, precision, recall, F1 score, and area under ROC curve. In this chapter, we’ll use some of these measures to evaluate our models and also look at confusion matrices to understand the model performance in detail.</p>

<p>Apart from these, when classification systems are deployed in real-world applications, key performance indicators (KPIs)<a contenteditable="false" data-primary="key performance indicators (KPIs)" data-type="indexterm" id="idm45969608405928"/><a contenteditable="false" data-primary="KPIs (key performance indicators)" data-type="indexterm" id="idm45969608404808"/> specific to a given business use case are also used to evaluate their impact and return on investment (ROI). These are often the metrics business teams care about. For example, if we’re using text classification to automatically route customer service requests, a possible KPI could be the reduction in wait time before the request is responded to compared to manual routing. In this chapter, we’ll focus on the NLP evaluation measures. In <a data-type="xref" href="part03.xhtml#applied">Part III</a> of the book, where we’ll discuss NLP use cases specific to industry verticals, we’ll introduce some KPIs that are often used in those verticals.</p>

<p>Before we start looking at how to build text classifiers using the pipeline we just discussed, let’s take a look at the scenarios where this pipeline is not at all necessary or where it’s not possible to use it.</p>

<section data-type="sect2" data-pdf-bookmark="A Simple Classifier Without the Text Classification Pipeline"><div class="sect2" id="a_simple_classifier_without_the_text_cl">
<h2>A Simple Classifier Without the Text Classification Pipeline</h2>

<p><a contenteditable="false" data-primary="text classification" data-secondary="simple" data-type="indexterm" id="term8"/>When we talk about the text classification pipeline, we’re referring to a supervised machine learning scenario. However, it’s possible to build a simple classifier without machine learning and without this pipeline. Consider the following problem scenario: we’re given a corpus of tweets where each tweet is labeled with its corresponding sentiment: negative or positive. For example, a tweet that says, “The new James Bond movie is great!” is clearly expressing a positive sentiment, whereas a tweet that says, “I would never visit this restaurant again, horrible place!!” has a negative sentiment. We want to build a classification system that will predict the sentiment of an unseen tweet using only the text of the tweet. A simple solution could be to create lists of positive and negative words in English—i.e., words that have a positive or negative sentiment. We then compare the usage of positive versus negative words in the input tweet and make a prediction based on this information. Further enhancements to this approach may involve creating more sophisticated dictionaries with degrees of positive, negative, and neutral sentiment of words or formulating specific heuristics (e.g., usage of certain smileys indicate positive sentiment) and using them to make predictions. This approach is called <em>lexicon-based sentiment analysis</em><a contenteditable="false" data-primary="lexicon-based sentiment analysis" data-type="indexterm" id="idm45969608396152"/><a contenteditable="false" data-primary="sentiment analysis" data-secondary="lexicon-based" data-type="indexterm" id="idm45969608395128"/>.</p>

<p>Clearly, this does not involve any “learning” of text classification; that is, it’s based on a set of heuristics or rules and custom-built resources such as dictionaries of words with sentiment. While this approach may seem too simple to perform reasonably well for many real-world scenarios, it may enable us to deploy a minimum viable product (MVP) quickly. Most importantly, this simple model can lead to better understanding of the problem and give us a simple baseline for our evaluation metric and speed. From our experience, it’s always good to start with such simpler approaches when tackling a new NLP problem, where possible. However, eventually, we’ll need ML methods that can infer more insights from large collections of text data and perform better than the baseline<a contenteditable="false" data-primary="text classification" data-secondary="simple" data-startref="term8" data-type="indexterm" id="idm45969608392376"/> approach.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Using Existing Text Classification APIs"><div class="sect2" id="using_existing_text_classification_apis">
<h2>Using Existing Text Classification APIs</h2>

<p><a contenteditable="false" data-primary="text classification" data-secondary="with existing APIs or libraries" data-secondary-sortas="existing" data-type="indexterm" id="idm45969608388856"/>Another scenario where we may not have to “learn” a classifier or follow this pipeline is when our task is more generic in nature, such as identifying a general category of a text (e.g., whether it’s about technology or music). In such cases, we can use existing APIs<a contenteditable="false" data-primary="APIs" data-secondary="text classification with" data-type="indexterm" id="idm45969608386328"/>, such as<a contenteditable="false" data-primary="Google Cloud" data-secondary="Natural Language" data-type="indexterm" id="idm45969608384808"/> Google Cloud Natural Language [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_43-marker" href="ch04.xhtml#footnote_43">5</a>], that provide off-the-shelf content classification models that can identify close to 700 different categories of text. Another popular classification task is sentiment analysis. All major service providers (e.g., Google<a contenteditable="false" data-primary="Google APIs" data-type="indexterm" id="idm45969608381304"/>, Microsoft<a contenteditable="false" data-primary="Microsoft" data-secondary="sentiment analysis APIs" data-type="indexterm" id="idm45969608380104"/>, and Amazon<a contenteditable="false" data-primary="Amazon" data-secondary="sentiment analysis APIs" data-type="indexterm" id="idm45969608378568"/>) serve sentiment analysis APIs<a contenteditable="false" data-primary="sentiment analysis APIs" data-type="indexterm" id="idm45969608376984"/> [<a data-type="noteref" href="ch04.xhtml#footnote_43">5</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608374888-marker" href="ch04.xhtml#idm45969608374888">6</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608373592-marker" href="ch04.xhtml#idm45969608373592">7</a>] with varying payment structures. If we’re tasked with building a sentiment classifier, we may not have to build our own system if an existing API addresses our business needs.</p>

<p>However, many classification tasks could be specific to our organization’s business needs. For the rest of this chapter, we’ll address the scenario of building our own classifier by considering the pipeline described earlier in this section.<a contenteditable="false" data-primary="text classification" data-secondary="pipeline for building systems" data-startref="term7" data-type="indexterm" id="idm45969608371800"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="One Pipeline, Many Classifiers"><div class="sect1" id="one_pipelinecomma_many_classifiers">
<h1>One Pipeline, Many Classifiers</h1>

<p>Let’s now look at building text classifiers by altering Steps 3 through 5 in the pipeline and keeping the remaining steps constant. A good dataset is a prerequisite to start using the pipeline. When we say “good” dataset, we mean a dataset that is a true representation of the data we’re likely to see in production. Throughout this chapter, we’ll use some of the publicly available datasets for text classification. A wide range of NLP-related datasets<a contenteditable="false" data-primary="datasets" data-secondary="text classification with" data-type="indexterm" id="term9"/><a contenteditable="false" data-primary="text classification" data-secondary="with public datasets" data-secondary-sortas="public" data-type="indexterm" id="term10"/>, including ones for text classification, are listed online [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608363832-marker" href="ch04.xhtml#idm45969608363832">8</a>]. Additionally, Figure Eight<a contenteditable="false" data-primary="Figure Eight" data-type="indexterm" id="idm45969608362200"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_4_5-marker" href="ch04.xhtml#footnote_4_5">9</a>] contains a collection of crowdsourced datasets, some of which are relevant to text classification. The UCI Machine Learning Repository<a contenteditable="false" data-primary="UCI Machine Learning Repository" data-type="indexterm" id="idm45969608359048"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608357816-marker" href="ch04.xhtml#idm45969608357816">10</a>] also contains a few text classification datasets. Google<a contenteditable="false" data-primary="Google" data-secondary="dataset search system" data-type="indexterm" id="idm45969608356296"/> recently launched a dedicated search system for datasets for machine learning [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608354776-marker" href="ch04.xhtml#idm45969608354776">11</a>]. We’ll use multiple datasets throughout this chapter instead of sticking to one to illustrate any dataset-specific issues you may come across.</p>

<p>Note that our goal in this chapter is to give you an overview of different approaches. No single approach is known to work universally well on all kinds of data and all classification problems. In the real world, we experiment with multiple approaches, evaluate them, and choose one final approach to deploy in practice.</p>

<p>For the rest of this section, we’ll use the “Economic News Article Tone and Relevance”<a contenteditable="false" data-primary="Economic News Article Tone and Relevance dataset (Figure Eight)" data-type="indexterm" id="idm45969608352040"/> dataset from Figure Eight to demonstrate text classification. It consists of 8,000 news articles annotated with whether or not they’re relevant to the US economy (i.e., a yes/no binary classification). The dataset is also imbalanced, with ~1,500 relevant and ~6,500 non-relevant articles, which poses the challenge of guarding against learning a bias toward the majority category (in this case, non-relevant articles). Clearly, learning what a relevant news article is is more challenging with this dataset than learning what is irrelevant. After all, just guessing that everything is irrelevant already gives us 80% accuracy!<a contenteditable="false" data-primary="datasets" data-secondary="text classification with" data-startref="term9" data-type="indexterm" id="idm45969608350120"/><a contenteditable="false" data-primary="text classification" data-secondary="with public datasets" data-secondary-sortas="public" data-startref="term10" data-type="indexterm" id="idm45969608348456"/></p>

<p>Let’s explore how a BoW representation (introduced in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>) can be used with this dataset following the pipeline described earlier in this chapter. We’ll build classifiers using three well-known algorithms: Naive Bayes, logistic regression, and support vector machines. The notebook related to this section (<em>Ch4/OnePipeline_ManyClassifiers.ipynb</em>) shows the step-by-step process of following our pipeline using these three algorithms. We’ll discuss some of the important aspects in this section.</p>

<section data-type="sect2" data-pdf-bookmark="Naive Bayes Classifier"><div class="sect2" id="naive_bayes_classifier">
<h2>Naive Bayes Classifier</h2>

<p><a contenteditable="false" data-primary="Naive Bayes classifier" data-type="indexterm" id="term11"/><a contenteditable="false" data-primary="text classification" data-secondary="Naive Bayes classifier" data-type="indexterm" id="term12"/>Naive Bayes is a probabilistic classifier that uses Bayes’ theorem to classify texts based on the evidence seen in training data. It estimates the conditional probability of each feature of a given text for each class based on the occurrence of that feature in that class and multiplies the probabilities of all the features of a given text to compute the final probability of classification for each class. Finally, it chooses the class with maximum probability. A detailed step-by-step explanation of the classifier is beyond the scope of this book. However, a reader interested in Naive Bayes with a detailed explanation in the context of text classification can look at Chapter 4 of Jurafsky and Martin [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_4_7-marker" href="ch04.xhtml#footnote_4_7">12</a>]. Although simple, Naive Bayes is commonly used as a baseline algorithm in classification experiments.</p>

<p>Let’s walk through the key steps of an implementation of the pipeline described earlier for our dataset. For this, we use a Naive Bayes implementation in scikit-learn. Once the dataset is loaded, we split the data into train and test data, as shown in the code snippet below:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Step 1: train-test split</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">our_data</code><code class="o">.</code><code class="n">text</code> 
<code class="c1">#the column text contains textual data to extract features from.</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">our_data</code><code class="o">.</code><code class="n">relevance</code> 
<code class="c1">#this is the column we are learning to predict.</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="c1">#split X and y into training and testing sets. By default, </code>
<code class="n">it</code> <code class="n">splits</code> <code class="mi">75</code><code class="o">%</code> <code class="c1">#training and 25% test. random_state=1 for reproducibility.</code></pre>

<p>The next step is to pre-process the texts and then convert them into feature vectors. While there are many different ways to do the pre-processing, let’s say we want to do the following: lowercasing and removal of punctuation, digits and any custom strings, and stop words. The code snippet below shows this pre-processing and converting the train and test data into feature vectors using <code>CountVectorizer</code> in scikit-learn, which is the implementation of the BoW approach we discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Step 2-3: Pre-process and Vectorize train and test data</code>
<code class="n">vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">preprocessor</code><code class="o">=</code><code class="n">clean</code><code class="p">)</code> 
<code class="c1">#clean is a function we defined for pre-processing, seen in the notebook.</code>
<code class="n">X_train_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_test_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">X_train_dtm</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code> <code class="n">X_test_dtm</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p>Once we run this in the notebook, we’ll see that we ended up having a feature vector with over 45,000 features! We now have the data in a format we want: feature vectors. So, the next step is to train and evaluate a classifier. The code snippet below shows how to do the training and evaluation of a Naive Bayes classifier with the features we extracted above:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nb</code> <code class="o">=</code> <code class="n">MultinomialNB</code><code class="p">()</code> <code class="c1">#instantiate a Multinomial Naive Bayes classifier</code>
<code class="n">nb</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_dtm</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="c1">#train the mode </code>
<code class="n">y_pred_class</code> <code class="o">=</code> <code class="n">nb</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_dtm</code><code class="p">)</code><code class="c1">#make class predictions for test data</code></pre>

<p><a data-type="xref" href="#confusion_matrix_for_naive_bayes_classi">Figure 4-4</a> shows the confusion matrix of this classifier with test data.</p>

<figure class="width-75"><div id="confusion_matrix_for_naive_bayes_classi" class="figure"><img alt="Confusion matrix for Naive Bayes classifier" src="Images/pnlp_0404.png" width="1385" height="1209"/>
<h6><span class="label">Figure 4-4. </span>Confusion matrix for Naive Bayes classifier</h6>
</div></figure>

<p>As evident from <a data-type="xref" href="#confusion_matrix_for_naive_bayes_classi">Figure 4-4</a>, the classifier is doing fairly well with identifying the non-relevant articles correctly, only making errors 14% of the time. However, it does not perform well in comparison to the second category: relevance. The category is identified correctly only 42% of the time. An obvious thought may be to collect more data. This is correct and often the most rewarding approach. But in the interest of covering other approaches, we assume that we cannot change it or collect additional data. This is not a far-fetched assumption—in industry, we often don’t have the luxury of collecting more data; we have to work with what we have. We can think of a few possible reasons for this performance and ways to improve this classifier. <a contenteditable="false" data-primary="text classification" data-secondary="reasons for poor performance" data-type="indexterm" id="term13"/>These are summarized in <a data-type="xref" href="#potential_reasons_for_poor_classifier_p">Table 4-1</a>, and we’ll look into some of them as we progress in this chapter.</p>



<table class="border" id="potential_reasons_for_poor_classifier_p">
	<caption><span class="label">Table 4-1. </span>Potential reasons for poor classifier performance</caption>
	<tbody>
		<tr>
			<td>Reason 1</td>
			<td>Since we extracted all possible features, we ended up in a large, sparse feature vector, where most features are too rare and end up being noise. A sparse feature set also makes training hard.</td>
		</tr>
		<tr>
			<td>Reason 2</td>
			<td>There are very few examples of relevant articles (~20%) compared to the non-relevant articles (~80%) in the dataset. This class imbalance makes the learning process skewed toward the non-relevant articles category, as there are very few examples of “relevant” articles.</td>
		</tr>
		<tr>
			<td>Reason 3</td>
			<td>Perhaps we need a better learning algorithm.</td>
		</tr>
		<tr>
			<td>Reason 4</td>
			<td>Perhaps we need a better pre-processing and feature extraction mechanism.</td>
		</tr>
		<tr>
			<td>Reason 5</td>
			<td>Perhaps we should look to tuning the classifier’s parameters and hyperparameters.</td>
		</tr>
	</tbody>
</table>


<p>Let’s see how to improve<a contenteditable="false" data-primary="text classification" data-secondary="reasons for poor performance" data-startref="term13" data-type="indexterm" id="idm45969608187960"/> our classification performance by addressing some of the possible reasons for it. One way to approach Reason 1 is to reduce noise in the feature vectors. The approach in the previous code example had close to 40,000 features (refer to the Jupyter notebook for details). A large number of features introduce sparsity; i.e., most of the features in the feature vector are zero, and only a few values are non-zero. This, in turn, affects the ability of the text classification algorithm to learn. Let’s see what happens if we restrict this to 5,000 and rerun the training and evaluation process. This requires us to change the <code>CountVectorizer</code> instantiation in the process, as shown in the code snippet below, and repeat all the steps:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">preprocessor</code><code class="o">=</code><code class="n">clean</code><code class="p">,</code> <code class="n">max_features</code><code class="o">=</code><code class="mi">5000</code><code class="p">)</code> <code class="c1">#Step-1</code>
<code class="n">X_train_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="c1">#combined step 2 and 3</code>
<code class="n">X_test_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="n">nb</code> <code class="o">=</code> <code class="n">MultinomialNB</code><code class="p">()</code> <code class="c1">#instantiate a Multinomial Naive Bayes model</code>
<code class="o">%</code><code class="n">time</code> <code class="n">nb</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_dtm</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="c1">#train the model(timing it with an IPython "magic command")</code>
<code class="n">y_pred_class</code> <code class="o">=</code> <code class="n">nb</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_dtm</code><code class="p">)</code>
<code class="c1">#make class predictions for X_test_dtm</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy: "</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred_class</code><code class="p">))</code></pre>

<p><a data-type="xref" href="#improved_classification_performance_wit">Figure 4-5</a> shows the new confusion matrix with this setting.</p>



<p>Now, clearly, while the average performance seems lower than before, the correct identification of relevant articles increased by over 20%. At that point, one may wonder whether this is what we want. The answer to that question depends on the problem we’re trying to solve. If we care about doing reasonably well with non-relevant article identification and doing as well as possible with relevant article identification, or doing equally well with both, we could conclude that reducing the feature vector size with the Naive Bayes classifier was useful for this dataset.</p>

<figure class="width-75"><div id="improved_classification_performance_wit" class="figure"><img alt="Improved classification performance with Naive Bayes and feature selection" src="Images/pnlp_0405.png" width="1402" height="1209"/>
<h6><span class="label">Figure 4-5. </span>Improved classification performance with Naive Bayes and feature selection</h6>
</div></figure>

<div data-type="tip"><h6>Tip</h6>
<p>Consider reducing the number of features if there are too many to reduce data sparsity.</p>
</div>

<p>Reason 2 in our list was the problem of skew in data toward the majority class. There are several ways to address this. Two typical approaches are oversampling the instances belonging to minority classes or undersampling the majority class to create a balanced dataset. Imbalanced-Learn<a contenteditable="false" data-primary="Imbalanced-Learn" data-type="indexterm" id="idm45969608051496"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608050264-marker" href="ch04.xhtml#idm45969608050264">13</a>] is a Python library that incorporates some of the sampling methods to address this issue. While we won’t delve into the details of this library here, classifiers also have a built-in mechanism to address such imbalanced datasets. <a contenteditable="false" data-primary="Naive Bayes classifier" data-startref="term11" data-type="indexterm" id="idm45969608048008"/><a contenteditable="false" data-primary="text classification" data-secondary="Naive Bayes classifier" data-startref="term12" data-type="indexterm" id="idm45969608046696"/>We’ll see how to use that by taking another classifier, logistic regression, in the next subsection.</p>

<div data-type="tip"><h6>Tip</h6>
<p>Class imbalance is one of the most common reasons for a classifier to not do well. We must always check if this is the case for our task and address it.</p>
</div>

<p>To address Reason 3, let’s try using other algorithms, beginning with logistic <span class="keep-together">regression.</span></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Logistic Regression"><div class="sect2" id="logistic_regression">
<h2>Logistic Regression</h2>

<p><a contenteditable="false" data-primary="logistic regression" data-type="indexterm" id="term14"/><a contenteditable="false" data-primary="regression, logistic" data-type="indexterm" id="term15"/><a contenteditable="false" data-primary="text classification" data-secondary="logistic regression" data-type="indexterm" id="term16"/>When we described the Naive Bayes classifier, we mentioned that it learns the probability of a text for each class and chooses the one with maximum probability. Such a classifier is called a <em>generative classifier</em><a contenteditable="false" data-primary="generative classifier" data-type="indexterm" id="idm45969608035992"/>. In contrast, there’s a <em>discriminative classifier</em><a contenteditable="false" data-primary="discriminative classifier" data-type="indexterm" id="idm45969608034472"/> that aims to learn the probability distribution over all classes. Logistic regression is an example of a discriminative classifier and is commonly used in text classification, as a baseline in research, and as an MVP in real-world industry scenarios.</p>

<p>Unlike Naive Bayes, which estimates probabilities based on feature occurrence in classes, logistic regression “learns” the weights for individual features based on how important they are to make a classification decision. The goal of logistic regression is to learn a linear separator between classes in the training data with the aim of maximizing the probability of the data. This “learning” of feature weights and probability distribution over all classes is done through a function called “logistic” function, and (hence the name) logistic regression [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608031928-marker" href="ch04.xhtml#idm45969608031928">14</a>].</p>

<p>Let’s take the 5,000-dimensional feature vector from the last step of the Naive Bayes example and train a logistic regression classifier instead of Naive Bayes. The code snippet below shows how to use logistic regression for this task:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code> 
<code class="n">logreg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">)</code>
<code class="n">logreg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_dtm</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code> 
<code class="n">y_pred_class</code> <code class="o">=</code> <code class="n">logreg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_dtm</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy: "</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred_class</code><code class="p">))</code></pre>

<p>This results in a classifier with an accuracy of 73.7%. <a data-type="xref" href="#classification_performance_with_logisti">Figure 4-6</a> shows the confusion matrix with this approach.</p>



<p>Our logistic regression classifier instantiation has an argument <code>class_weight</code>, which is given a value <code>“balanced”</code>. This tells the classifier to boost the weights for classes in inverse proportion to the number of samples for that class. So, we expect to see better performance for the less-represented classes. We can experiment with this code by removing that argument and retraining the classifier, to witness a fall (by approximately 5%) in the bottom-right cell of the confusion matrix. However, logistic regression clearly seems to perform worse than Naive Bayes for this dataset.</p>



<p>Reason 3 in our list was: “Perhaps we need a better learning algorithm.” This gives rise to the question: “What is a better learning algorithm?” A general rule of thumb when working with ML approaches is that there is no one algorithm that learns well on all datasets. A common approach is to experiment with various algorithms and compare them.</p>

<figure class="width-75"><div id="classification_performance_with_logisti" class="figure"><img alt="Classification performance with logistic regression" src="Images/pnlp_0406.png" width="1385" height="1209"/>
<h6><span class="label">Figure 4-6. </span>Classification performance with logistic regression</h6>
</div></figure>

<p>Let’s see if this idea helps us by replacing logistic regression with another well-known classification algorithm that was shown to be useful for several text classification tasks, called the “support vector machine.”</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Support Vector Machine"><div class="sect2" id="support_vector_machine-id00042">
<h2>Support Vector Machine</h2>

<p><a contenteditable="false" data-primary="support vector machines (SVMs)" data-type="indexterm" id="term17"/><a contenteditable="false" data-type="indexterm" data-primary="SVMs (support vector machines)" id="term117"/><a contenteditable="false" data-primary="text classification" data-secondary="with SVMs" data-secondary-sortas="SVMs" data-type="indexterm" id="term18"/>We described logistic regression as a discriminative classifier that learns the weights for individual features and predicts a probability distribution over the classes. A <em>support vector machine (SVM)</em>, first invented in the early 1960s, is a discriminative classifier like logistic regression. However, unlike logistic regression, it aims to look for an optimal hyperplane in a higher dimensional space, which can separate the classes in the data by a maximum possible margin. Further, SVMs are capable of learning even non-linear separations between classes, unlike logistic regression. However, they may also take longer to train.</p>

<p>SVMs come in various flavors in sklearn. Let’s see how one of them is used by keeping everything else the same and altering maximum features to 1,000 instead of the previous example’s 5,000. We restrict to 1,000 features, keeping in mind the time an SVM algorithm takes to train. The code snippet below shows how to do this, and <a data-type="xref" href="#confusion_matrix_for_classification_wit">Figure 4-7</a> shows the resultant confusion matrix:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>
<code class="n">vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">preprocessor</code><code class="o">=</code><code class="n">clean</code><code class="p">,</code> <code class="n">max_features</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code> <code class="c1">#Step-1</code>
<code class="n">X_train_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="c1">#combined step 2 and 3</code>
<code class="n">X_test_dtm</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">class_weight</code><code class="o">=</code><code class="s1">'balanced'</code><code class="p">)</code> <code class="c1">#notice the “balanced” option</code>
<code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_dtm</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code> <code class="c1">#fit the model with training data</code>
<code class="n">y_pred_class</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_dtm</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy: "</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred_class</code><code class="p">))</code></pre>

<figure class="width-75"><div id="confusion_matrix_for_classification_wit" class="figure"><img alt="Confusion matrix for classification with SVM" src="Images/pnlp_0407.png" width="1402" height="1209"/>
<h6><span class="label">Figure 4-7. </span>Confusion matrix for classification with SVM</h6>
</div></figure>

<p>When compared to logistic regression, SVMs seem to have done better with the relevant articles category, although, among this small set of experiments we did, Naive Bayes, with the smaller set of features, seems to be the best classifier for this dataset.<a contenteditable="false" data-primary="support vector machines (SVMs)" data-startref="term17" data-type="indexterm" id="idm45969607810984"/><a contenteditable="false" data-type="indexterm" data-primary="SVMs (support vector machines)" data-startref="term117" id="idm45969607809640"/><a contenteditable="false" data-primary="text classification" data-secondary="with SVMs" data-secondary-sortas="SVMs" data-startref="term18" data-type="indexterm" id="idm45969607808296"/></p>

<p>All the examples in this section demonstrate how changes in different steps affected the classification performance and how to interpret the results. Clearly, we excluded many other possibilities, such as exploring other text classification algorithms, changing different parameters of various classifiers, coming up with better pre-processing methods, etc. We leave them as further exercises for the reader, using the notebook as a playground. A real-world text classification project involves exploring multiple options like this, starting with the simplest approach in terms of modeling, deployment, and scaling, and gradually increasing the complexity. Our eventual goal is to build the classifier that best meets our business needs given all the other constraints.</p>

<p>Let’s now consider a part of Reason 4 in <a data-type="xref" href="#potential_reasons_for_poor_classifier_p">Table 4-1</a>: better feature representation. So far in this chapter, we’ve used BoW features. Let’s see how we can use other feature representation techniques we saw in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a> for text classification.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Using Neural Embeddings in Text Classification"><div class="sect1" id="using_neural_embeddings_in_text_classif">
<h1>Using Neural Embeddings in Text Classification</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="with neural embeddings" data-secondary-sortas="neural embeddings" data-type="indexterm" id="term20"/><a contenteditable="false" data-primary="neural embeddings" data-type="indexterm" id="term19"/>In the latter half of <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>, we discussed feature engineering techniques using neural networks, such as word embeddings, character embeddings, and document embeddings. The advantage of using embedding-based features is that they create a dense, low-dimensional feature representation instead of the sparse, high-dimensional structure of BoW/TF-IDF and other such features. There are different ways of designing and using features based on neural embeddings. In this section, let’s look at some ways of using such embedding representations for text <span class="keep-together">classification.</span></p>

<section data-type="sect2" data-pdf-bookmark="Word Embeddings"><div class="sect2" id="word_embeddings-id00052">
<h2>Word Embeddings</h2>

<p><a contenteditable="false" data-primary="word embeddings" data-secondary="text classification with" data-type="indexterm" id="term21"/>Words and n-grams have been used primarily as features in text classification for a long time. Different ways of vectorizing words have been proposed, and we used one such representation in the last section, <code>CountVectorizer</code>. In the past few years, neural network–based architectures have become popular for “learning” word representations, which are known as “word embeddings.” We surveyed some of the intuitions behind this in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>. Let’s now take a look at how to use word embeddings as features for text classification. We’ll use the sentiment-labeled sentences dataset from the UCI repository, consisting of 1,500 positive-sentiment and 1,500 negative-sentiment sentences from Amazon, Yelp, and IMDB. All the steps are detailed in the notebook <em>Ch4/Word2Vec_Example.ipynb</em>. Let’s walk through the important steps and where this approach differs from the previous section’s procedures.</p>

<p>Loading and pre-processing the text data remains a common step. However, instead of vectorizing the texts using BoW-based features, we’ll now rely on neural embedding models. As mentioned earlier, we’ll use a pre-trained embedding model. Word2vec is a popular algorithm we discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a> for training word embedding models. There are several pre-trained <a contenteditable="false" data-primary="Word2vec model (Google)" data-secondary="pre-trained" data-type="indexterm" id="idm45969607787096"/>Word2vec models trained on large corpora available on the internet. Here, we’ll use the one from <a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="idm45969607785480"/>Google [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607783976-marker" href="ch04.xhtml#idm45969607783976">15</a>]. The following code snippet shows how to load this model into Python using gensim:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">data_path</code><code class="o">=</code> <code class="s2">"/your/folder/path"</code>
<code class="n">path_to_model</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">data_path</code><code class="p">,</code><code class="s1">'GoogleNews-vectors-negative300.bin'</code><code class="p">)</code>
<code class="n">training_data_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">data_path</code><code class="p">,</code> <code class="s2">"sentiment_sentences.txt"</code><code class="p">)</code>
<code class="c1">#Load W2V model. This will take some time.</code>
<code class="n">w2v_model</code> <code class="o">=</code> <code class="n">KeyedVectors</code><code class="o">.</code><code class="n">load_word2vec_format</code><code class="p">(</code><code class="n">path_to_model</code><code class="p">,</code> <code class="n">binary</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'done loading Word2Vec'</code><code class="p">)</code></pre>

<p>This is a large model that can be seen as a dictionary where the keys are words in the vocabulary and the values are their learned embedding representations. Given a query word, if the word’s embedding is present in the dictionary, it will return the same. How do we use this pre-learned embedding to represent features? As we discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>, there are multiple ways of doing this. A simple approach is just to average the embeddings for individual words in text. The code snippet below shows a simple function to do this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Creating a feature vector by averaging all embeddings for all sentences</code>
<code class="k">def</code> <code class="nf">embedding_feats</code><code class="p">(</code><code class="n">list_of_lists</code><code class="p">):</code>
    <code class="n">DIMENSION</code> <code class="o">=</code> <code class="mi">300</code>
    <code class="n">zero_vector</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">DIMENSION</code><code class="p">)</code>
    <code class="n">feats</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">tokens</code> <code class="ow">in</code> <code class="n">list_of_lists</code><code class="p">:</code>
          <code class="n">feat_for_this</code> <code class="o">=</code>  <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">DIMENSION</code><code class="p">)</code>
          <code class="n">count_for_this</code> <code class="o">=</code> <code class="mi">0</code>
          <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokens</code><code class="p">:</code>
                     <code class="k">if</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">w2v_model</code><code class="p">:</code>
                          <code class="n">feat_for_this</code> <code class="o">+=</code> <code class="n">w2v_model</code><code class="p">[</code><code class="n">token</code><code class="p">]</code>
                          <code class="n">count_for_this</code> <code class="o">+=</code><code class="mi">1</code>
          <code class="n">feats</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">feat_for_this</code><code class="o">/</code><code class="n">count_for_this</code><code class="p">)</code>         
    <code class="k">return</code> <code class="n">feats</code>

<code class="n">train_vectors</code> <code class="o">=</code> <code class="n">embedding_feats</code><code class="p">(</code><code class="n">texts_processed</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">train_vectors</code><code class="p">))</code></pre>

<p>Note that it uses embeddings only for the words that are present in the dictionary. It ignores the words for which embeddings are absent. Also, note that the above code will give a single vector with <code>DIMENSION(=300)</code> components. We treat the resulting embedding vector as the feature vector that represents the entire text. Once this feature engineering is done, the final step is similar to what we did in the previous section: use these features and train a classifier. We leave that as an exercise to the reader (refer to the notebook for the full code).</p>

<p>When trained with a logistic regression classifier, these features gave a classification accuracy of 81% on our dataset (see the notebook for more details). Considering that we just used an existing word embeddings model and followed only basic pre-processing steps, this is a great model to have as a baseline! We saw in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a> that there are other pre-trained embedding approaches, such as GloVe<a contenteditable="false" data-primary="GloVe" data-type="indexterm" id="idm45969607604712"/>, which can be experimented with for this approach. Gensim<a contenteditable="false" data-primary="gensim library" data-type="indexterm" id="idm45969607603480"/>, which we used in this example, also supports training our own word embeddings if necessary. If we’re working on a custom domain whose vocabulary is remarkably different from that of the pre-trained news embeddings we used here, it would make sense to train our own embeddings to extract features.</p>

<p>In order to decide whether to train our own embeddings or use pre-trained embeddings, a good rule of thumb is to compute the vocabulary overlap. If the overlap between the vocabulary of our custom domain and that of pre-trained word <span class="keep-together">embeddings</span> is greater than 80%, pre-trained word embeddings tend to give good results in text classification.</p>

<p>An important factor to consider when deploying models with embedding-based feature extraction approaches is that the learned or pre-trained embedding models have to be stored and loaded into memory while using these approaches. If the model itself is bulky (e.g., the pre-trained model we used takes 3.6 GB), we need to factor this into our deployment needs.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Subword Embeddings and fastText"><div class="sect2" id="subword_embeddings_and_fasttext">
<h2>Subword Embeddings and fastText</h2>

<p><a contenteditable="false" data-primary="subword embeddings" data-type="indexterm" id="term22"/><a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="subword" id="term2222"/><a contenteditable="false" data-primary="fastText library (Facebook)" data-type="indexterm" id="term23"/><a contenteditable="false" data-type="indexterm" data-primary="Facebook" data-secondary="fastText library" id="term222"/>Word embeddings, as the name indicates, are about word representations. Even off-the-shelf embeddings seem to work well on classification tasks, as we saw earlier. However, if a word in our dataset was not present in the pre-trained model’s vocabulary, how will we get a representation for this word? This problem is popularly known as <em>out of vocabulary (OOV)</em><a contenteditable="false" data-primary="OOV (out of vocabulary) problem" data-type="indexterm" id="idm45969607591112"/><a contenteditable="false" data-primary="out of vocabulary (OOV) problem" data-type="indexterm" id="idm45969607589992"/>. In our previous example, we just ignored such words from feature extraction. Is there a better way?</p>

<p>We discussed fastText embeddings<a contenteditable="false" data-type="indexterm" data-primary="fastText embeddings (Facebook)" id="idm45969607588216"/><a contenteditable="false" data-type="indexterm" data-primary="Facebook" data-secondary="fastText embeddings" id="idm45969607587048"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607585544-marker" href="ch04.xhtml#idm45969607585544">16</a>] in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>. They’re based on the idea of enriching word embeddings with subword-level information. Thus, the embedding representation for each word is represented as a sum of the representations of individual character n-grams. While this may seem like a longer process compared to just estimating word-level embeddings, it has two advantages:</p>

<ul>
	<li>
	<p>This approach can handle words that did not appear in training data (OOV).</p>
	</li>
	<li>
	<p>The implementation facilitates extremely fast learning on even very large <span class="keep-together">corpora.</span></p>
	</li>
</ul>

<p>While fastText is a general-purpose library to learn the embeddings, it also supports off-the-shelf text classification by providing end-to-end classifier training and testing; i.e., we don’t have to handle feature extraction separately. The remaining part of this subsection shows how to use the fastText classifier [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607579464-marker" href="ch04.xhtml#idm45969607579464">17</a>] for text classification. We’ll work with the DBpedia dataset<a contenteditable="false" data-primary="DBpedia dataset" data-type="indexterm" id="idm45969607578088"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607576808-marker" href="ch04.xhtml#idm45969607576808">18</a>]. It’s a balanced dataset consisting of 14 classes, with 40,000 training and 5,000 testing examples per class. Thus, the total size of the dataset is 560,000 training and 70,000 testing data points. Clearly, this is a much larger dataset than what we saw before. Can we build a fast training model using fastText? Let’s check it out!</p>

<p>The training and test sets are provided as CSV files in this dataset. So, the first step involves reading these files into your Python environment and cleaning the text to remove extraneous characters, similar to what we did in the pre-processing steps for the other classifier examples we’ve seen so far. Once this is done, the process to use fastText is quite simple. The code snippet below shows a simple fastText model. The step-by-step process is detailed in the associated Jupyter notebook (<em>Ch4/FastText_Example.ipynb</em>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">## Using fastText for feature extraction and training</code>
<code class="kn">from</code> <code class="nn">fasttext</code> <code class="kn">import</code> <code class="n">supervised</code>
<code class="sd">"""fastText expects and training file (csv), a model name as input arguments.</code>
<code class="sd">label_prefix refers to the prefix before label string in the dataset.</code>
<code class="sd">default is __label__. In our dataset, it is __class__.</code>
<code class="sd">There are several other parameters which can be seen in:</code>
<code class="sd">https://pypi.org/project/fasttext/</code>
<code class="sd">"""</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">supervised</code><code class="p">(</code><code class="n">train_file</code><code class="p">,</code> <code class="s1">'temp'</code><code class="p">,</code> <code class="n">label_prefix</code><code class="o">=</code><code class="s2">"__class__"</code><code class="p">)</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">test</code><code class="p">(</code><code class="n">test_file</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">results</code><code class="o">.</code><code class="n">nexamples</code><code class="p">,</code> <code class="n">results</code><code class="o">.</code><code class="n">precision</code><code class="p">,</code> <code class="n">results</code><code class="o">.</code><code class="n">recall</code><code class="p">)</code></pre>

<p>If we run this code in the notebook, we’ll notice that, despite the fact that this is a huge dataset and we gave the classifier raw text and not the feature vector, the training takes only a few seconds, and we get close to 98% precision and recall! As an exercise, try to build a classifier using the same dataset but with either BoW or word embedding features and algorithms like logistic regression. Notice how long it takes for the individual steps of feature extraction and classification learning!</p>

<p>When we have a large dataset, and when learning seems infeasible with the approaches described so far, fastText is a good option to use to set up a strong working baseline. However, there’s one concern to keep in mind when using fastText, as was the case with Word2vec embeddings: it uses pre-trained character n-gram embeddings. Thus, when we save the trained model, it carries the entire character n-gram embeddings dictionary with it. This results in a bulky model and can result in engineering issues. For example, the model stored with the name “temp” in the above code snippet has a size close to 450 MB. However, fastText implementation also comes with options to reduce the memory footprint of its classification models with minimal reduction in classification performance [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607490152-marker" href="ch04.xhtml#idm45969607490152">19</a>]. It does this by doing vocabulary pruning and using compression algorithms. Exploring these possibilities could be a good option in cases where large model sizes are a constraint.</p>

<div data-type="tip"><h6>Tip</h6>
<p>fastText is extremely fast to train and very useful for setting up strong baselines. The downside is the model size.</p>
</div>

<p>We hope this discussion gives a good overview of the usefulness of fastText for text classification. What we showed here is a default classification model without any tuning of the hyperparameters. fastText’s documentation contains more information on the different options to tune your classifier and on training custom embedding representations for a dataset you want. However, both of the embedding representations we’ve seen so far learn a representation of words and characters and collect them together to form a text representation. Let’s see how to learn the representation for a document directly using the Doc2vec approach we discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>.<a contenteditable="false" data-primary="subword embeddings" data-startref="term22" data-type="indexterm" id="idm45969607485320"/><a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="subword" data-startref="term2222" id="idm45969607483912"/><a contenteditable="false" data-primary="fastText library (Facebook)" data-startref="term23" data-type="indexterm" id="idm45969607482264"/><a contenteditable="false" data-type="indexterm" data-primary="Facebook" data-secondary="fastText library" data-startref="term222" id="idm45969607480920"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Document Embeddings"><div class="sect2" id="document_embeddings">
<h2>Document Embeddings</h2>

<p><a contenteditable="false" data-primary="document embeddings" data-secondary="text classification with" data-type="indexterm" id="term25"/><a contenteditable="false" data-primary="text classification" data-secondary="with document embeddings" data-secondary-sortas="document embeddings" data-type="indexterm" id="term26"/>In the Doc2vec<a contenteditable="false" data-primary="Doc2vec model" data-secondary="text classification with" data-type="indexterm" id="term24"/> embedding scheme, we learn a direct representation for the entire document (sentence/paragraph) rather than each word. Just as we used word and character embeddings as features for performing text classification, we can also use Doc2vec as a feature representation mechanism. Since there are no existing pre-trained models that work with the latest version of Doc2vec [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607471976-marker" href="ch04.xhtml#idm45969607471976">20</a>], let’s see how to build our own Doc2vec model and use it for text classification.</p>

<p>We’ll use a dataset called “Sentiment Analysis: Emotion in Text”<a contenteditable="false" data-primary="Sentiment Analysis: Emotion in Text dataset (Figure Eight)" data-type="indexterm" id="idm45969607469512"/> from <a contenteditable="false" data-primary="Figure Eight" data-type="indexterm" id="idm45969607468344"/><a class="orm:hideurl" href="http://figure-eight.com">figure-eight.com</a> [<a data-type="noteref" href="ch04.xhtml#footnote_4_5">9</a>], which contains 40,000 tweets labeled with 13 labels signifying different emotions. Let’s take the three most frequent labels in this dataset—neutral, worry, happiness—and build a text classifier for classifying new tweets into one of these three classes. The notebook for this subsection (<em>Ch4/Doc2Vec_Example.ipynb</em>) walks you through the steps involved in using Doc2vec for text classification and provides the dataset.</p>

<p>After loading the dataset and taking a subset of the three most frequent labels, an important step to consider here is pre-processing the data. What’s different here compared to previous examples? Why can’t we just follow the same procedure as before? There are a few things that are different about tweets compared to news articles or other such text, as we briefly discussed in <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a> when we talked about text pre-processing. First, they are very short. Second, our traditional tokenizers may not work well with tweets, splitting smileys, hashtags, Twitter<a contenteditable="false" data-primary="Twitter" data-type="indexterm" id="idm45969607462344"/> handles, etc., into multiple tokens. Such specialized needs prompted a lot of research into NLP for Twitter in the recent past, which resulted in several pre-processing options for tweets. One such solution is a <code>TweetTokenizer</code><a contenteditable="false" data-primary="TweetTokenizer" data-type="indexterm" id="idm45969607460600"/><a contenteditable="false" data-primary="tokenization" data-secondary="tweet" data-type="indexterm" id="idm45969607459496"/>, implemented in the NLTK [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607457992-marker" href="ch04.xhtml#idm45969607457992">21</a>] library in Python. We’ll discuss more on this topic in <a data-type="xref" href="ch08.xhtml#social_media">Chapter 8</a>. For now, let’s see how we can use a <code>TweetTokenizer</code> in the following code snippet:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">tweeter</code> <code class="o">=</code> <code class="n">TweetTokenizer</code><code class="p">(</code><code class="n">strip_handles</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">preserve_case</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">mystopwords</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s2">"english"</code><code class="p">))</code>

<code class="c1">#Function to pre-process and tokenize tweets</code>
<code class="k">def</code> <code class="nf">preprocess_corpus</code><code class="p">(</code><code class="n">texts</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">remove_stops_digits</code><code class="p">(</code><code class="n">tokens</code><code class="p">):</code>
    <code class="c1">#Nested function to remove stopwords and digits</code>
          <code class="k">return</code> <code class="p">[</code><code class="n">token</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokens</code> <code class="k">if</code> <code class="n">token</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">mystopwords</code> 
                  <code class="ow">and</code> <code class="ow">not</code> <code class="n">token</code><code class="o">.</code><code class="n">isdigit</code><code class="p">()]</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">remove_stops_digits</code><code class="p">(</code><code class="n">tweeter</code><code class="o">.</code><code class="n">tokenize</code><code class="p">(</code><code class="n">content</code><code class="p">))</code> <code class="k">for</code> <code class="n">content</code> <code class="ow">in</code> <code class="n">texts</code><code class="p">]</code>

<code class="n">mydata</code> <code class="o">=</code> <code class="n">preprocess_corpus</code><code class="p">(</code><code class="n">df_subset</code><code class="p">[</code><code class="s1">'content'</code><code class="p">])</code>
<code class="n">mycats</code> <code class="o">=</code> <code class="n">df_subset</code><code class="p">[</code><code class="s1">'sentiment'</code><code class="p">]</code></pre>

<p>The next step in this process is to train a Doc2vec model<a contenteditable="false" data-primary="Doc2vec model" data-secondary="training" data-type="indexterm" id="idm45969607452936"/> to learn tweet representations. Ideally, any large dataset of tweets will work for this step. However, since we don’t have such a ready-made corpus, we’ll split our dataset into train-test and use the training data for learning the Doc2vec representations. The first part of this process involves converting the data into a format readable by the Doc2vec implementation, which can be done using the <code>TaggedDocument</code> class<a contenteditable="false" data-primary="TaggedDocument class" data-type="indexterm" id="idm45969607339080"/>. It’s used to represent a document as a list of tokens, followed by a “tag,” which in its simplest form can be just the filename or ID of the document. However, Doc2vec by itself can also be used as a nearest neighbor classifier for both multiclass and multilabel classification problems using <code/>. We’ll leave this as an exploratory exercise for the reader. Let’s now see how to train a Doc2vec classifier for tweets through the code snippet below:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Prepare training data in doc2vec format:</code>
<code class="n">d2vtrain</code> <code class="o">=</code> <code class="p">[</code><code class="n">TaggedDocument</code><code class="p">((</code><code class="n">d</code><code class="p">),</code><code class="n">tags</code><code class="o">=</code><code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)])</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">d</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_data</code><code class="p">)]</code>
<code class="c1">#Train a doc2vec model to learn tweet representations. Use only training data!!</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Doc2Vec</code><code class="p">(</code><code class="n">vector_size</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.025</code><code class="p">,</code> <code class="n">min_count</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">dm</code> <code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">build_vocab</code><code class="p">(</code><code class="n">d2vtrain</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">d2vtrain</code><code class="p">,</code> <code class="n">total_examples</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">corpus_count</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">epochs</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"d2v.model"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Model Saved"</code><code class="p">)</code></pre>

<p>Training for Doc2vec involves making several choices regarding parameters, as seen in the model definition in the code snippet above. <code>vector_size</code> refers to the dimensionality of the learned embeddings; <code>alpha</code> is the learning rate; <code>min_count</code> is the minimum frequency of words that remain in vocabulary; <code>dm</code>, which stands for distributed memory, is one of the representation learners implemented in Doc2vec (the other is <code>dbow</code>, or distributed bag of words); and <code>epochs</code> are the number of training iterations. There are a few other parameters that can be customized. While there are some guidelines on choosing optimal parameters for training Doc2vec models [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607196824-marker" href="ch04.xhtml#idm45969607196824">22</a>], these are not exhaustively validated, and we don’t know if the guidelines work for tweets.</p>

<p>The best way to address this issue is to explore a range of values for the ones that matter to us (e.g., <code>dm</code> versus <code>dbow</code>, vector sizes, learning rate) and compare multiple models. How do we compare these models, as they only learn the text representation? One way to do it is to start using these learned representations in a downstream task—in this case, text classification. Doc2vec’s <code>infer_vector</code> function can be used to infer the vector representation for a given text using a pre-trained model. Since there is some amount of randomness due to the choice of hyperparameters, the inferred vectors differ each time we extract them. For this reason, to get a stable representation, we run it multiple times (called steps) and aggregate the vectors. Let’s use the learned model to infer features for our data and train a logistic regression classifier:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Infer the feature representation for training and test data using </code>
<code class="c1">#the trained model</code>
<code class="n">model</code><code class="o">=</code> <code class="n">Doc2Vec</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"d2v.model"</code><code class="p">)</code>
<code class="c1">#Infer in multiple steps to get a stable representation</code>
<code class="n">train_vectors</code> <code class="o">=</code>  <code class="p">[</code><code class="n">model</code><code class="o">.</code><code class="n">infer_vector</code><code class="p">(</code><code class="n">list_of_tokens</code><code class="p">,</code> <code class="n">steps</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
              <code class="k">for</code> <code class="n">list_of_tokens</code> <code class="ow">in</code> <code class="n">train_data</code><code class="p">]</code>
<code class="n">test_vectors</code> <code class="o">=</code> <code class="p">[</code><code class="n">model</code><code class="o">.</code><code class="n">infer_vector</code><code class="p">(</code><code class="n">list_of_tokens</code><code class="p">,</code> <code class="n">steps</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
              <code class="k">for</code> <code class="n">list_of_tokens</code> <code class="ow">in</code> <code class="n">test_data</code><code class="p">]</code>
<code class="n">myclass</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">)</code> 
<code class="c1">#because classes are not balanced</code>
<code class="n">myclass</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_vectors</code><code class="p">,</code> <code class="n">train_cats</code><code class="p">)</code>
<code class="n">preds</code> <code class="o">=</code> <code class="n">myclass</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">test_vectors</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">test_cats</code><code class="p">,</code> <code class="n">preds</code><code class="p">))</code></pre>

<p>Now, the performance of this model seems rather poor, achieving an F1 score of 0.51 on a reasonably large corpus, with only three classes. There are a couple of interpretations for this poor result. First, unlike full news articles or even well-formed sentences, tweets contain very little data per instance. Further, people write with a wide variety in spelling and syntax when they tweet. There are a lot of emoticons in different forms. Our feature representation should be able to capture such aspects. While tuning the algorithms by searching a large parameter space for the best model may help, an alternative could be to explore problem-specific feature representations, as we discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>. We’ll see how to do this for tweets in <a data-type="xref" href="ch08.xhtml#social_media">Chapter 8</a>. An important point to keep in mind when using Doc2vec<a contenteditable="false" data-primary="Doc2vec model" data-type="indexterm" id="idm45969607071448"/> is the same as for fastText: if we have to use Doc2vec for feature representation, we have to store the model that learned the representation. While it’s not typically as bulky as fastText, it’s also not as fast to train. Such trade-offs need to be considered and compared before we make a deployment decision.</p>

<p><a contenteditable="false" data-primary="neural embeddings" data-startref="term19" data-type="indexterm" id="idm45969607069592"/><a contenteditable="false" data-primary="text classification" data-secondary="with neural embeddings" data-secondary-sortas="neural embeddings" data-startref="term20" data-type="indexterm" id="idm45969607068216"/><a contenteditable="false" data-primary="Doc2vec model" data-secondary="text classification with" data-startref="term24" data-type="indexterm" id="idm45969607066296"/><a contenteditable="false" data-primary="document embeddings" data-secondary="text classification with" data-startref="term25" data-type="indexterm" id="idm45969607064680"/><a contenteditable="false" data-primary="text classification" data-secondary="with document embeddings" data-secondary-sortas="document embeddings" data-startref="term26" data-type="indexterm" id="idm45969607063064"/>So far, we’ve seen a range of feature representations and how they play a role for text classification using ML algorithms. Let’s now turn to a family of algorithms that became popular in the past few years, known as “deep learning.”</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Deep Learning for Text Classification"><div class="sect1" id="deep_learning_for_text_classification">
<h1>Deep Learning for Text Classification</h1>

<p><a contenteditable="false" data-primary="deep learning (DL)" data-secondary="for text classification" data-secondary-sortas="text classification" data-type="indexterm" id="term30"/><a contenteditable="false" data-primary="text classification" data-secondary="DL for" data-type="indexterm" id="term31"/>As we discussed in <a data-type="xref" href="ch01.xhtml#nlp_a_primer">Chapter 1</a>, deep learning is a family of machine learning algorithms where the learning happens through different kinds of multilayered neural network architectures. Over the past few years, it has shown remarkable improvements on standard machine learning tasks, such as image classification, speech recognition, and machine translation. This has resulted in widespread interest in using deep learning for various tasks, including text classification. So far, we’ve seen how to train different machine learning classifiers, using BoW and different kinds of embedding representations. Now, let’s look at how to use deep learning architectures for text classification.</p>

<p>Two of the most commonly used neural network architectures for text classification are convolutional neural networks (CNNs)<a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="idm45969607052872"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="idm45969607051800"/> and recurrent neural networks (RNNs)<a contenteditable="false" data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="idm45969607050552"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="idm45969607049384"/>. Long short-term memory (LSTM) networks<a contenteditable="false" data-primary="long short-term memory networks (LSTMs)" data-type="indexterm" id="idm45969607048136"/><a contenteditable="false" data-primary="LSTMs (long short-term memory networks)" data-type="indexterm" id="idm45969607047016"/> are a popular form of RNNs. Recent approaches also involve starting with large, pre-trained language models and fine-tuning them for the task at hand. In this section, we’ll learn how to train CNNs and LSTMs and how to tune a pre-trained language model for text classification using the IMDB sentiment classification dataset [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607045416-marker" href="ch04.xhtml#idm45969607045416">23</a>]. Note that a detailed discussion on how neural network architectures work is beyond the scope of this book. Interested readers can read the textbook by Goodfellow et al. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607043736-marker" href="ch04.xhtml#idm45969607043736">24</a>] for a general theoretical discussion and Goldberg’s book [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969607042568-marker" href="ch04.xhtml#idm45969607042568">25</a>] for NLP-specific uses of neural network architectures. Jurafsky and Martin’s book [<a data-type="noteref" href="ch04.xhtml#footnote_4_7">12</a>] also provides a short but concise overview of different neural network methods for NLP.</p>

<p>The first step toward training any ML or DL model is to define a feature representation. This step has been relatively straightforward in the approaches we’ve seen so far, with BoW or embedding vectors. However, for neural networks, we need further processing of input vectors, as we saw in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>. Let’s quickly recap the steps involved in converting training and test data into a format suitable for the neural network input layers:</p>

<ol>
	<li>
	<p>Tokenize the texts and convert them into word index vectors.</p>
	</li>
	<li>
	<p>Pad the text sequences so that all text vectors are of the same length.</p>
	</li>
	<li>
	<p>Map every word index to an embedding vector. We do that by multiplying word index vectors with the embedding matrix. The embedding matrix can either be populated using pre-trained embeddings or it can be trained for embeddings on this corpus.</p>
	</li>
	<li>
	<p>Use the output from Step 3 as the input to a neural network architecture.</p>
	</li>
</ol>

<p>Once these are done, we can proceed with the specification of neural network architectures and training classifiers with them. The Jupyter notebook associated with this section (<em>Ch4/DeepNN_Example.ipynb</em>) will walk you through the entire process from text pre-processing to neural network training and evaluation. We’ll use Keras, a Python-based DL library. The code snippet below illustrates Steps 1 and 2:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer.</code>
<code class="c1">#Tokenizer is fit on training data only, and that is used to tokenize both train </code>
<code class="c1">#and test data.</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">Tokenizer</code><code class="p">(</code><code class="n">num_words</code><code class="o">=</code><code class="n">MAX_NUM_WORDS</code><code class="p">)</code>
<code class="n">tokenizer</code><code class="o">.</code><code class="n">fit_on_texts</code><code class="p">(</code><code class="n">train_texts</code><code class="p">)</code>
<code class="n">train_sequences</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">(</code><code class="n">train_texts</code><code class="p">)</code> 
<code class="n">test_sequences</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">(</code><code class="n">test_texts</code><code class="p">)</code>
<code class="n">word_index</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">word_index</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Found </code><code class="si">%s</code><code class="s1"> unique tokens.'</code> <code class="o">%</code> <code class="nb">len</code><code class="p">(</code><code class="n">word_index</code><code class="p">))</code>
<code class="c1">#Converting this to sequences to be fed into neural network. Max seq. len is </code>
<code class="c1">#1000 as set earlier. Initial padding of 0s, until vector is of </code>
<code class="c1">#size MAX_SEQUENCE_LENGTH</code>
<code class="n">trainvalid_data</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">(</code><code class="n">train_sequences</code><code class="p">,</code> <code class="n">maxlen</code><code class="o">=</code><code class="n">MAX_SEQUENCE_LENGTH</code><code class="p">)</code>
<code class="n">test_data</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">(</code><code class="n">test_sequences</code><code class="p">,</code> <code class="n">maxlen</code><code class="o">=</code><code class="n">MAX_SEQUENCE_LENGTH</code><code class="p">)</code>
<code class="n">trainvalid_labels</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">train_labels</code><code class="p">))</code>
<code class="n">test_labels</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">test_labels</code><code class="p">))</code></pre>

<p>Step 3: If we want to use pre-trained embeddings to convert the train and test data into an embedding matrix like we did in the earlier examples with Word2vec and fastText, we have to download them and use them to convert our data into the input format for the neural networks. The following code snippet shows an example of how to do this using GloVe embeddings, which were introduced in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>. GloVe embeddings<a contenteditable="false" data-primary="GloVe" data-type="indexterm" id="idm45969606903928"/> come with multiple dimensionalities, and we chose 100 as our dimension here. The value of dimensionality<a contenteditable="false" data-primary="dimensionality" data-type="indexterm" id="idm45969606902632"/> is a hyperparameter, and we can experiment with other dimensions as well:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="ch04fn1-marker" href="ch04.xhtml#ch04fn1">i</a></sup></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">embeddings_index</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">GLOVE_DIR</code><code class="p">,</code> <code class="s1">'glove.6B.100d.txt'</code><code class="p">))</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">f</code><code class="p">:</code>
          <code class="n">values</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
          <code class="n">word</code> <code class="o">=</code> <code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
          <code class="n">coefs</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">values</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">dtype</code><code class="o">=</code><code class="s1">'float32'</code><code class="p">)</code>
          <code class="n">embeddings_index</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">=</code> <code class="n">coefs</code>

<code class="n">num_words</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">MAX_NUM_WORDS</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">word_index</code><code class="p">))</code> <code class="o">+</code> <code class="mi">1</code>
<code class="n">embedding_matrix</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">num_words</code><code class="p">,</code> <code class="n">EMBEDDING_DIM</code><code class="p">))</code>
<code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">word_index</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
    <code class="k">if</code> <code class="n">i</code> <code class="o">&gt;</code> <code class="n">MAX_NUM_WORDS</code><code class="p">:</code>
          <code class="k">continue</code>
    <code class="n">embedding_vector</code> <code class="o">=</code> <code class="n">embeddings_index</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">embedding_vector</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>
          <code class="n">embedding_matrix</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">embedding_vector</code></pre>

<p>Step 4: Now, we’re ready to train DL models for text classification! DL architectures consist of an input layer, an output layer, and several hidden layers in between the two. Depending on the architecture, different hidden layers are used. The input layer for textual input is typically an embedding layer. The output layer, especially in the context of text classification, is a softmax layer with categorical output. If we want to train the input layer instead of using pre-trained embeddings, the easiest way is to call the <code>Embedding</code> layer class in Keras, specifying the input and output dimensions. However, since we want to use pre-trained embeddings, we should create a custom embedding layer that uses the embedding matrix we just built. The following code snippet shows how to do that:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">embedding_layer</code> <code class="o">=</code> <code class="n">Embedding</code><code class="p">(</code><code class="n">num_words</code><code class="p">,</code> <code class="n">EMBEDDING_DIM</code><code class="p">,</code>
                        <code class="n">embeddings_initializer</code><code class="o">=</code><code class="n">Constant</code><code class="p">(</code><code class="n">embedding_matrix</code><code class="p">),</code>
                        <code class="n">input_length</code><code class="o">=</code><code class="n">MAX_SEQUENCE_LENGTH</code><code class="p">,</code>
                        <code class="n">trainable</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Preparing of embedding matrix is done"</code><code class="p">)</code></pre>

<p>This will serve as the input layer for any neural network we want to use (CNN or LSTM). Now that we know how to pre-process the input and define an input layer, let’s move on to specifying the rest of the neural network architecture using CNNs and LSTMs.</p>

<section data-type="sect2" data-pdf-bookmark="CNNs for Text Classification"><div class="sect2" id="cnns_for_text_classification">
<h2>CNNs for Text Classification</h2>

<p><a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="term32"/><a contenteditable="false" data-type="indexterm" data-primary="convolutional neural networks (CNNs)" id="ch04_term332"/><a contenteditable="false" data-primary="text classification" data-secondary="CNNs for" data-type="indexterm" id="term33"/>Let’s now look at how to define, train, and evaluate a CNN model for text classification. CNNs typically consist of a series of convolution and pooling layers as the hidden layers. In the context of text classification, CNNs can be thought of as learning the most useful bag-of-words/n-grams features instead of taking the entire collection of words/n-grams as features, as we did earlier in this chapter. Since our dataset has only two classes—positive and negative—the output layer has two outputs, with the softmax activation function. We’ll define a CNN with three convolution-pooling layers using the <code>Sequential</code> model class in Keras, which allows us to specify DL models as a sequential stack of layers—one after another. Once the layers and their activation functions are specified, the next task is to define other important parameters, such as the optimizer, loss function, and the evaluation metric to tune the hyperparameters of the model. Once all this is done, the next step is to train and evaluate the model. The following code snippet shows one way of specifying a CNN architecture for this task using the Python library Keras and prints the results with the IMDB dataset for this model:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="s1">'Define a 1D CNN model.'</code><code class="p">)</code>
<code class="n">cnnmodel</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">embedding_layer</code><code class="p">)</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling1D</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling1D</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">GlobalMaxPooling1D</code><code class="p">())</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">labels_index</code><code class="p">),</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'softmax'</code><code class="p">))</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'categorical_crossentropy'</code><code class="p">,</code>
                    <code class="n">optimizer</code><code class="o">=</code><code class="s1">'rmsprop'</code><code class="p">,</code>
                    <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'acc'</code><code class="p">])</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code>
          <code class="n">batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
          <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_val</code><code class="p">,</code> <code class="n">y_val</code><code class="p">))</code>
<code class="n">score</code><code class="p">,</code> <code class="n">acc</code> <code class="o">=</code> <code class="n">cnnmodel</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">test_data</code><code class="p">,</code> <code class="n">test_labels</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Test accuracy with CNN:'</code><code class="p">,</code> <code class="n">acc</code><code class="p">)</code></pre>

<p>As you can see, we made a lot of choices in specifying the model, such as activation functions, hidden layers, layer sizes, loss function, optimizer, metrics, epochs, and batch size. While there are some commonly recommended options for these, there’s no consensus on one combination that works best for all datasets and problems. A good approach while building your models is to experiment with different settings (i.e., hyperparameters). Keep in mind that all these decisions come with some <span class="keep-together">associated</span> cost. For example, in practice, we have the number of epochs as 10 or above. But that also increases the amount of time it takes to train the model. Another thing to note is that, if you want to train an embedding layer instead of using pre-trained embeddings in this model, the only thing that changes is the line <code>cnnmodel.add(embedding_layer)</code>. Instead, we can specify a new embedding layer as, for example, <code>cnnmodel.add(Embedding(Param1, Param2))</code><em>.</em> The code snippet below shows the code and model performance for the same:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="s2">"Defining and training a CNN model, training embedding layer on the fly </code>
      <code class="n">instead</code> <code class="n">of</code> <code class="n">using</code> <code class="n">pre</code><code class="o">-</code><code class="n">trained</code> <code class="n">embeddings</code><code class="s2">")</code>
<code class="n">cnnmodel</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Embedding</code><code class="p">(</code><code class="n">MAX_NUM_WORDS</code><code class="p">,</code> <code class="mi">128</code><code class="p">))</code>
<code class="err">…</code>
<code class="o">...</code>
<code class="n">cnnmodel</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code>
          <code class="n">batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
          <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_val</code><code class="p">,</code> <code class="n">y_val</code><code class="p">))</code>
<code class="n">score</code><code class="p">,</code> <code class="n">acc</code> <code class="o">=</code> <code class="n">cnnmodel</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">test_data</code><code class="p">,</code> <code class="n">test_labels</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Test accuracy with CNN:'</code><code class="p">,</code> <code class="n">acc</code><code class="p">)</code></pre>

<p>If we run this code in the notebook, we’ll notice that, in this case, training the embedding layer on our own dataset seems to result in better classification on test data. However, if the training data were substantially small, sticking to the pre-trained embeddings, or using the domain adaptation techniques we’ll discuss later in this chapter, would be a better choice.<a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-startref="term32" data-type="indexterm" id="idm45969606432680"/><a contenteditable="false" data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-startref="ch04_term332" id="idm45969606365640"/><a contenteditable="false" data-primary="text classification" data-secondary="CNNs for" data-startref="term33" data-type="indexterm" id="idm45969606364280"/> Let’s look at how to train similar models using an LSTM.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="LSTMs for Text Classification"><div class="sect2" id="lstms_for_text_classification">
<h2>LSTMs for Text Classification</h2>

<p><a contenteditable="false" data-primary="LSTMs (long short-term memory networks)" data-type="indexterm" id="term3334"/><a contenteditable="false" data-type="indexterm" data-primary="long short-term memory networks (LSTMs)" id="term333"/><a contenteditable="false" data-primary="text classification" data-secondary="LSTMs for" data-type="indexterm" id="term34"/>As we saw briefly in <a data-type="xref" href="ch01.xhtml#nlp_a_primer">Chapter 1</a>, LSTMs and other variants of RNNs in general have become the go-to way of doing neural language modeling in the past few years. This is primarily because language is sequential in nature and RNNs are specialized in working with sequential data. The current word in the sentence depends on its context—the words before and after. However, when we model text using CNNs, this crucial fact is not taken into account. RNNs work on the principle of using this context while learning the language representation or a model of language. Hence, they’re known to work well for NLP tasks. There are also CNN variants that can take such context into account, and CNNs versus RNNs is still an open area of debate. In this section, we’ll see an example of using RNNs for text classification. Now that we’ve already seen one neural network in action, it’s relatively easy to train another! Just replace the convolutional and pooling parts with an LSTM in the prior two code examples. The following code snippet shows how to train an LSTM model using the same IMDB dataset for text classification:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="s2">"Defining and training an LSTM model, training embedding layer on the fly"</code><code class="p">)</code>
<code class="n">rnnmodel</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Embedding</code><code class="p">(</code><code class="n">MAX_NUM_WORDS</code><code class="p">,</code> <code class="mi">128</code><code class="p">))</code>
<code class="n">rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">))</code>
<code class="n">rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'sigmoid'</code><code class="p">))</code>
<code class="n">rnnmodel</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'binary_crossentropy'</code><code class="p">,</code>
               <code class="n">optimizer</code><code class="o">=</code><code class="s1">'adam'</code><code class="p">,</code>
               <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Training the RNN'</code><code class="p">)</code>
<code class="n">rnnmodel</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code>
          <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
          <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
          <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_val</code><code class="p">,</code> <code class="n">y_val</code><code class="p">))</code>
<code class="n">score</code><code class="p">,</code> <code class="n">acc</code> <code class="o">=</code> <code class="n">rnnmodel</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">test_data</code><code class="p">,</code> <code class="n">test_labels</code><code class="p">,</code>
                          <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Test accuracy with RNN:'</code><code class="p">,</code> <code class="n">acc</code><code class="p">)</code></pre>

<p>Notice that this code took much longer to run than the CNN example. While LSTMs are more powerful in utilizing the sequential nature of text, they’re much more data hungry as compared to CNNs. Thus, the relative lower performance of the LSTM on a dataset need not necessarily be interpreted as a shortcoming of the model itself. It’s possible that the amount of data we have is not sufficient to utilize the full potential of an LSTM. As in the case of CNNs, several parameters and hyperparameters play important roles in model performance, and it’s always a good practice to explore multiple options and compare different models before finalizing on one.<a contenteditable="false" data-primary="LSTMs (long short-term memory networks)" data-startref="term333" data-type="indexterm" id="idm45969606350744"/><a contenteditable="false" data-type="indexterm" data-primary="LSTMs (long short-term memory networks)" data-startref="term3334" id="idm45969606204296"/><a contenteditable="false" data-primary="text classification" data-secondary="LSTMs for" data-startref="term34" data-type="indexterm" id="idm45969606202936"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Text Classification with Large, Pre-Trained Language Models"><div class="sect2" id="text_classification_with_largecomma_pre">
<h2>Text Classification with Large, Pre-Trained Language Models</h2>

<p><a contenteditable="false" data-primary="text classification" data-secondary="with large, pre-trained language models" data-secondary-sortas="large" data-type="indexterm" id="term35"/><a contenteditable="false" data-primary="pre-trained language models, large" data-type="indexterm" id="term36"/><a contenteditable="false" data-primary="language models, large, pre-trained" data-type="indexterm" id="term37"/><a contenteditable="false" data-primary="large, pre-trained language models" data-type="indexterm" id="term38"/>In the past two years, there have been great improvements in using neural network–based text representations for NLP tasks. We discussed some of these in <a data-type="xref" href="ch03.xhtml#universal_text_representations">“Universal Text Representations”</a>. These representations have been used successfully for text classification in the recent past by fine-tuning the pre-trained models to the given task and dataset. <a contenteditable="false" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="text classification with" data-type="indexterm" id="term39"/>BERT, which was mentioned in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>, is a popular model used in this way for text classification. Let’s take a look at how to use BERT for text classification using the IMDB dataset we used earlier in this section. The full code is in the relevant notebook (<em>Ch4/BERT_Sentiment_Classification_IMDB.ipynb</em>).</p>

<p>We’ll use ktrain<a contenteditable="false" data-primary="ktrain" data-type="indexterm" id="idm45969606188104"/>, a lightweight wrapper to train and use pre-trained DL models using the TensorFlow library Keras. ktrain provides a straightforward process for all steps, from obtaining the dataset and the pre-trained BERT to fine-tuning it for the classification task. Let’s see how to load the dataset first through the code snippet below:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">get_file</code><code class="p">(</code>
<code class="n">fname</code><code class="o">=</code><code class="s2">"aclImdb.tar.gz"</code><code class="p">,</code>   
<code class="n">origin</code><code class="o">=</code><code class="s2">"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</code><code class="p">,</code>
 <code class="n">extract</code><code class="o">=</code><code class="bp">True</code><code class="p">,)</code>
</pre>

<p>Once the dataset is loaded, the next step is to download the BERT model and pre-process the dataset according to BERT’s requirements. The following code snippet shows how to do this with ktrain’s functions:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">),</code> <code class="n">preproc</code> <code class="o">=</code> 
                       <code class="n">text</code><code class="o">.</code><code class="n">texts_from_folder</code><code class="p">(</code><code class="n">IMDB_DATADIR</code><code class="p">,</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>                                                                   
   <code class="n">preprocess_mode</code><code class="o">=</code><code class="s1">'bert'</code><code class="p">,</code><code class="n">train_test_names</code><code class="o">=</code><code class="p">[</code><code class="s1">'train'</code><code class="p">,</code><code class="s1">'test'</code><code class="p">],</code></pre>

<p>The next step is to load the pre-trained BERT model and fine-tune it for this dataset. Here’s the code snippet to do this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">text</code><code class="o">.</code><code class="n">text_classifier</code><code class="p">(</code><code class="s1">'bert'</code><code class="p">,</code> <code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="n">preproc</code><code class="o">=</code><code class="n">preproc</code><code class="p">)</code>
<code class="n">learner</code><code class="o">=</code><code class="n">ktrain</code><code class="o">.</code><code class="n">get_learner</code><code class="p">(</code><code class="n">model</code><code class="p">,</code><code class="n">train_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code><code class="n">y_train</code><code class="p">),</code>    
                          <code class="n">val_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">),</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">6</code><code class="p">)</code>
<code class="n">learner</code><code class="o">.</code><code class="n">fit_onecycle</code><code class="p">(</code><code class="mf">2e-5</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code></pre>

<p>These three lines of code will train a text classifier using the BERT pre-trained model. As with other examples we’ve seen so far, we would need to do parameter tuning and a lot of experimentation to pick the best-performing model. We leave that as an exercise for the reader.</p>

<p>In this section, we introduced the idea of using DL for text classification using two neural network architectures—CNN and LSTM—and showed how we can tune a state-of-the-art, pre-trained language model (BERT) for a given dataset and classification task. There are several variants to these architectures, and new models are being proposed every day by NLP researchers. We saw how to use one pre-trained language model, BERT<a contenteditable="false" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="text classification with" data-startref="term39" data-type="indexterm" id="idm45969598392952"/>. There are other such models, and this is a constantly evolving area in NLP research; the state of the art keeps changing every few months (or even weeks!). However, in our experience as industry practitioners, several NLP tasks, especially text classification, still widely use several of the non-DL approaches we described earlier in the chapter. Two primary reasons for this are a lack of the large amounts of task-specific training data that neural networks demand and issues related to computing and deployment costs.</p>

<div data-type="tip"><h6>Tip</h6>
<p>DL-based text classifiers are often nothing but condensed representations of the data they were trained on. These models are often as good as the training dataset. Selecting the right dataset becomes all the more important in such cases.</p>
</div>

<p>We’ll end this section by reiterating what we mentioned earlier when we discussed the text classification pipeline: in most industrial settings, it always makes sense to start with a simpler, easy-to-deploy approach as your MVP and go from there incrementally, taking customer needs and feasibility into account.</p>

<p class="pagebreak-before"><a contenteditable="false" data-primary="deep learning (DL)" data-secondary="for text classification" data-secondary-sortas="text classification" data-startref="term30" data-type="indexterm" id="idm45969598388344"/><a contenteditable="false" data-primary="text classification" data-secondary="DL for" data-startref="term31" data-type="indexterm" id="idm45969598386424"/>We’ve seen several approaches to building text classification models so far. Unlike heuristics-based approaches where the predictions can be justified by tracing back the rules applied on the data sample, ML models are treated as a black box while making predictions. However, in the recent past, the topic of interpretable ML started to gain prominence, and programs that can “explain” an ML model’s predictions exist now.<a contenteditable="false" data-primary="text classification" data-secondary="with large, pre-trained language models" data-secondary-sortas="large" data-startref="term35" data-type="indexterm" id="idm45969598384200"/><a contenteditable="false" data-primary="pre-trained language models, large" data-startref="term36" data-type="indexterm" id="idm45969598382312"/><a contenteditable="false" data-primary="language models, large, pre-trained" data-startref="term37" data-type="indexterm" id="idm45969598380920"/><a contenteditable="false" data-primary="large, pre-trained language models" data-startref="term38" data-type="indexterm" id="idm45969598379528"/> Let’s take a quick look at their application for text classification.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Interpreting Text Classification Models"><div class="sect1" id="interpreting_text_classification_models">
<h1>Interpreting Text Classification Models</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="interpreting models" data-type="indexterm" id="term40"/>In the previous sections, we’ve seen how to train text classifiers using multiple approaches. In all these examples, we took the classifier predictions as is, without seeking any explanations. In fact, most real-world use cases of text classification may be similar—we just consume the classifier’s output and don’t question its decisions. Take spam classification: we generally don’t look for explanations of why a certain email is classified as spam or regular email. However, there may be scenarios where such explanations are necessary.</p>

<p>Consider a scenario where we developed a classifier that identifies abusive comments on a discussion forum website. The classifier identifies comments that are objectionable/abusive and performs the job of a human moderator by either deleting them or making them invisible to users. We know that classifiers aren’t perfect and can make errors. What if the commenter questions this moderation decision and asks for an explanation? Some method to “explain” the classification decision by pointing to which feature’s presence prompted such a decision can be useful in such cases. Such a method is also useful to provide some insights into the model and how it may perform on real-world data (instead of train/test sets), which may result in better, more reliable models in the future.</p>

<p>As ML models started getting deployed in real-world applications, interest in the direction of model interpretability grew. Recent research [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_4_35-marker" href="ch04.xhtml#footnote_4_35">26</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598370520-marker" href="ch04.xhtml#idm45969598370520">27</a>] resulted in usable tools [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_4_33-marker" href="ch04.xhtml#footnote_4_33">28</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598367560-marker" href="ch04.xhtml#idm45969598367560">29</a>] for interpreting model predictions (especially for classification). Lime<a contenteditable="false" data-primary="Lime" data-type="indexterm" id="idm45969598366184"/> [<a data-type="noteref" href="ch04.xhtml#footnote_4_33">28</a>] is one such tool that attempts to interpret a black-box classification model by approximating it with a linear model locally around a given training instance. The advantage of this is that such a linear model is expressed as a weighted sum of its features and is easy to interpret for humans. For example, if there are two features, f1 and f2, for a given test instance of a binary classifier with classes A and B, a Lime linear model around this instance could be something like -0.3 × f1 + 0.4 × f2 with a prediction B. This indicates that the presence of feature f1 will negatively affect this prediction (by 0.3) and skew it toward A. [<a data-type="noteref" href="ch04.xhtml#footnote_4_35">26</a>] explains this in more detail. Let’s now look at how Lime [<a data-type="noteref" href="ch04.xhtml#footnote_4_33">28</a>] can be used to understand the predictions of a text classifier.</p>

<section data-type="sect2" data-pdf-bookmark="Explaining Classifier Predictions with Lime"><div class="sect2" id="explaining_classifier_predictions_with">
<h2>Explaining Classifier Predictions with Lime</h2>

<p>Let’s take a model we already built earlier in this chapter and see how Lime can help us interpret its predictions. The following code snippet uses the logistic regression model we built earlier using the “Economy News Article Tone and Relevance” dataset<a contenteditable="false" data-primary="Economic News Article Tone and Relevance dataset (Figure Eight)" data-type="indexterm" id="idm45969598359640"/><a contenteditable="false" data-type="indexterm" data-primary="Figure Eight" id="idm45969598358568"/>, which classifies a given news article as being relevant or non-relevant and shows how we can use Lime (the full code can be accessed in the notebook <em>Ch4/LimeDemo.ipynb</em>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">lime</code> <code class="kn">import</code> <code class="n">lime_text</code>
<code class="kn">from</code> <code class="nn">lime.lime_text</code> <code class="kn">import</code> <code class="n">LimeTextExplainer</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

<code class="n">y_pred_prob</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test_dtm</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">c</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">vect</code><code class="p">,</code> <code class="n">classifier</code><code class="p">)</code>
<code class="n">mystring</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[</code><code class="mi">221</code><code class="p">]</code> <code class="c1">#Take a string from test instance</code>
<code class="k">print</code><code class="p">(</code><code class="n">c</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">([</code><code class="n">mystring</code><code class="p">]))</code> <code class="c1">#Prediction is a "No" here, i.e., not relevant</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"no"</code><code class="p">,</code> <code class="s2">"yes"</code><code class="p">]</code> <code class="c1">#not relevant, relevant</code>
<code class="n">explainer</code> <code class="o">=</code> <code class="n">LimeTextExplainer</code><code class="p">(</code><code class="n">class_names</code><code class="o">=</code><code class="n">class_names</code><code class="p">)</code>
<code class="n">exp</code> <code class="o">=</code> <code class="n">explainer</code><code class="o">.</code><code class="n">explain_instance</code><code class="p">(</code><code class="n">mystring</code><code class="p">,</code> <code class="n">c</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">,</code> <code class="n">num_features</code><code class="o">=</code><code class="mi">6</code><code class="p">)</code>
<code class="n">exp</code><code class="o">.</code><code class="n">as_list</code><code class="p">()</code></pre>

<p>This code shows six features that played an important role in making this prediction. They’re as follows:</p>

<pre data-type="programlisting">
[('YORK', 0.23416984139912805),
 ('NEW', -0.22724581340890154),
 ('showing', -0.12532906927967377),
 ('AP', -0.08486610147834726),
 ('dropped', 0.07958281943957331),
 ('trend', 0.06567603359316518)]</pre>

<p>Thus, the output of the above code can be seen as a linear sum of these six features. This would mean that, if we remove the features “NEW” and “showing,” the prediction should move toward the opposite class, i.e., “relevant/Yes,” by 0.35 (the sum of the weights of these two features). Lime also has functions to visualize these predictions. <a data-type="xref" href="#figure_4_8_visualization_of_limeapostro">Figure 4-8</a> shows a visualization of the above explanation.</p>



<p>As shown in the figure, the presence of three words—York, trend, and dropped—skews the prediction toward Yes, whereas the other three words skew the prediction toward No. Apart from some uses we mentioned earlier, such visualizations of classifiers can also help us if we want to do some informed feature selection.</p>

<p>We hope this brief introduction gave you an idea of what to do if you have to explain a classifier’s predictions. We also have a notebook (<em>Ch4/Lime_RNN.ipynb</em>) that explains an LSTM model’s predictions using Lime, and we leave this detailed exploration of Lime as an exercise for the reader.<a contenteditable="false" data-primary="text classification" data-secondary="interpreting models" data-startref="term40" data-type="indexterm" id="idm45969598230504"/><a contenteditable="false" data-primary="Lime" data-startref="term41" data-type="indexterm" id="idm45969598228856"/></p>

<figure><div id="figure_4_8_visualization_of_limeapostro" class="figure"><img alt="Visualization of Lime’s explanation of a classifier’s prediction" src="Images/pnlp_0408.png" width="1216" height="552"/>
<h6><span class="label">Figure 4-8. </span>Visualization of Lime’s explanation of a classifier’s prediction</h6>
</div></figure>

</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Learning with No or Less Data and Adapting to New Domains"><div class="sect1" id="learning_with_no_or_less_data_and_adapt">
<h1>Learning with No or Less Data and Adapting to <span class="keep-together">New Domains</span></h1>

<p><a contenteditable="false" data-primary="learning" data-secondary="with no or less data" data-secondary-sortas="no" data-type="indexterm" id="term42"/><a contenteditable="false" data-primary="adapting to new domains" data-type="indexterm" id="term43"/>In all the examples we’ve seen so far, we had a relatively large training dataset available for the task. However, in most real-world scenarios, such datasets are not readily available. In other cases, we may have an annotated dataset available, but it might not be large enough to train a good classifier. There can also be cases where we have a large dataset of, say, customer complaints and requests for one product suite, but we’re asked to customize our classifier to another product suite for which we have a very small amount of data (i.e., we’re adapting an existing model to a new domain). In this section, let’s discuss how to build good classification systems for these scenarios where we have no or little data or have to adapt to new domain training data.</p>

<section data-type="sect2" data-pdf-bookmark="No Training Data"><div class="sect2" id="no_training_data">
<h2>No Training Data</h2>

<p><a contenteditable="false" data-primary="training data" data-secondary="no data" data-type="indexterm" id="term44"/>Let’s say we’re asked to design a classifier for segregating customer complaints for our e-commerce company. The classifier is expected to automatically route customer complaint emails into a set of categories: billing, delivery, and others. If we’re fortunate, we may discover a source of large amounts of annotated data for this task within the organization in the form of a historical database of customer requests and their categories. If such a database doesn’t exist, where should we start to build our <span class="keep-together">classifier?</span></p>

<p>The first step in such a scenario is creating an annotated dataset where customer complaints are mapped to the set of categories mentioned above. One way to approach this is to get customer service agents to manually label some of the complaints and use that as the training data for our ML model. Another approach is called “bootstrapping<a contenteditable="false" data-primary="bootstrapping" data-type="indexterm" id="idm45969598213704"/>” or “weak supervision<a contenteditable="false" data-primary="weak supervision" data-type="indexterm" id="idm45969598212472"/>.” There can be certain patterns of information in different categories of customer requests. Perhaps billing-related requests mention variants of the word “bill,” amounts in a currency, etc. Delivery-related requests talk about shipping, delays, etc. We can get started with compiling some such patterns and using their presence or absence in a customer request to label it, thereby creating a small (perhaps noisy) annotated dataset for this classification task. From here, we can build a classifier to annotate a larger collection of data. Snorkel [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598210664-marker" href="ch04.xhtml#idm45969598210664">30</a>], a recent software tool developed by Stanford University, is useful for deploying weak supervision for various learning tasks, including classification. Snorkel was used to deploy weak supervision–based text classification models at industrial scale at Google [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598207704-marker" href="ch04.xhtml#idm45969598207704">31</a>]. They showed that weak supervision could create classifiers comparable in quality to those trained on tens of thousands of hand-labeled examples! [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598206104-marker" href="ch04.xhtml#idm45969598206104">32</a>] shows an example of how to use Snorkel to generate training data for text classification using a large amount of unlabeled data.</p>

<p>In some other scenarios where large-scale collection of data is necessary and feasible, crowdsourcing<a contenteditable="false" data-primary="crowdsourcing" data-type="indexterm" id="idm45969598203928"/> can be seen as an option to label the data. Websites like Amazon Mechanical Turk<a contenteditable="false" data-primary="Amazon Mechanical Turk" data-type="indexterm" id="idm45969598202728"/> and Figure Eight<a contenteditable="false" data-primary="Figure Eight" data-type="indexterm" id="idm45969598201496"/> provide platforms to make use of human intelligence to create high-quality training data<a contenteditable="false" data-primary="training data" data-secondary="crowdsourcing" data-type="indexterm" id="idm45969598200120"/> for ML tasks. A popular example of using the wisdom of crowds to create a classification dataset<a contenteditable="false" data-primary="datasets" data-secondary="crowdsourcing for" data-type="indexterm" id="idm45969598198504"/> is the “CAPTCHA test<a contenteditable="false" data-primary="Captcha tests" data-type="indexterm" id="idm45969598197000"/>,” which Google<a contenteditable="false" data-primary="Google" data-secondary="Captcha tests" data-type="indexterm" id="idm45969598195736"/> uses to ask if a set of images contain a given object (e.g., “Select all images that contain a street sign”).<a contenteditable="false" data-primary="training data" data-secondary="no data" data-startref="term44" data-type="indexterm" id="idm45969598194072"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Less Training Data: Active Learning and Domain Adaptation"><div class="sect2" id="less_training_data_active_learning_and">
<h2>Less Training Data: Active Learning and Domain Adaptation</h2>

<p>In scenarios like the one described earlier, where we collected small amounts of data using human annotations or bootstrapping, it may sometimes turn out that the amount of data is too small to build a good classification model. It’s also possible that most of the requests we collected belonged to billing and very few belonged to the other categories, resulting in a highly imbalanced dataset. Asking the agents to spend many hours doing manual annotation is not always feasible. What should we do in such scenarios?</p>

<p>One approach to address such problems is <em>active learning</em><a contenteditable="false" data-primary="active learning" data-secondary="text classification" data-type="indexterm" id="term65"/>, which is primarily about identifying which data points are more crucial to be used as training data. It helps answer the following question: if we had 1,000 data points but could get only 100 of them labeled, which 100 would we choose? What this means is that, when it comes to training data, not all data points are equal. Some data points are more important as compared to others in determining the quality of the classifier trained. Active learning converts this into a continuous process.</p>

<p>Using active learning for training a classifier can be described as a step-by-step <span class="keep-together">process:</span></p>

<ol>
	<li>
	<p>Train the classifier with the available amount of data.</p>
	</li>
	<li>
	<p>Start using the classifier to make predictions on new data.</p>
	</li>
	<li>
	<p>For the data points where the classifier is very unsure of its predictions, send them to human annotators for their correct classification.</p>
	</li>
	<li>
	<p>Include these data points in the existing training data and retrain the model.</p>
	</li>
</ol>

<p>Repeat Steps 1 through 4 until a satisfactory model performance is reached.</p>

<p>Tools like Prodigy<a contenteditable="false" data-primary="Prodigy" data-type="indexterm" id="idm45969598180968"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598179704-marker" href="ch04.xhtml#idm45969598179704">33</a>] have active learning solutions implemented for text classification and support the efficient usage of active learning to create annotated data and text classification models quickly. The basic idea behind active learning is that the data points where the model is less confident are the data points that contribute most significantly to improving the quality of the model, and therefore only those data points get labeled.</p>

<p>Now, imagine a scenario for our customer complaint classifier where we have a lot of historical data for a range of products. However, we’re now asked to tune it to work on a set of newer products. What’s potentially challenging in this situation? Typical text classification approaches rely on the vocabulary of the training data. Hence, they’re inherently biased toward the kind of language seen in the training data. So, if the new products are very different (e.g., the model is trained on a suite of electronic products and we’re using it for complaints on cosmetic products), the pre-trained classifiers trained on some other source data are unlikely to perform well. However, it’s also not realistic to train a new model from scratch on each product or product suite, as we’ll again run into the problem of insufficient training data. Domain adaptation<a contenteditable="false" data-primary="domain adaptation" data-type="indexterm" id="term47"/> is a method to address such scenarios; this is also called <em>transfer learning</em><a contenteditable="false" data-primary="transfer learning" data-type="indexterm" id="term46"/>. Here, we “transfer” what we learned from one domain (source) with large amounts of data to another domain (target) with less labeled data but large amounts of unlabeled data. We already saw one example of how to use BERT for text classification earlier in this chapter.</p>

<p>This approach for domain adaptation in text classification can be summarized as <span class="keep-together">follows:</span></p>

<ol>
	<li>
	<p>Start with a large, pre-trained language model trained on a large dataset of the source domain (e.g., Wikipedia data).</p>
	</li>
	<li>
	<p>Fine-tune this model using the target language’s unlabeled data.</p>
	</li>
	<li>
	<p>Train a classifier on the labeled target domain data by extracting feature representations from the fine-tuned language model from Step 2.</p>
	</li>
</ol>
<a contenteditable="false" data-primary="text classification" data-secondary="domain adaptation for" data-startref="term47" data-type="indexterm" id="idm45969598168696"/>

<p>ULMFit<a contenteditable="false" data-primary="ULMFit" data-type="indexterm" id="idm45969598166600"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598165336-marker" href="ch04.xhtml#idm45969598165336">34</a>] is another popular domain adaptation<a contenteditable="false" data-primary="text classification" data-secondary="domain adaptation for" data-type="indexterm" id="term947"/> approach for text classification. In research experiments, it was shown that this approach matches the performance of training from scratch with 10 to 20 times more training examples and only 100 labeled examples in text classification tasks. When unlabeled data was used to fine-tune the pre-trained language model, it matched the performance of using 50 to 100 times more labeled examples when trained from scratch, on the same text classification tasks. Transfer learning methods are currently an active area of research in NLP. Their use for text classification has not yet shown dramatic improvements on standard datasets, nor are they the default solution for all classification scenarios in industry setups yet. But we can expect to see this approach yielding better and better results in the near future.</p>

<p><a contenteditable="false" data-primary="learning" data-secondary="with no or less data" data-secondary-sortas="no" data-startref="term42" data-type="indexterm" id="idm45969598160968"/><a contenteditable="false" data-primary="adapting to new domains" data-startref="term43" data-type="indexterm" id="idm45969598159048"/>So far, we’ve seen a range of text classification methods and discussed obtaining appropriate training data and using different feature representations for training the classifiers. We also briefly touched on how to interpret the predictions made by some text classification models. Let’s now consolidate what we’ve learned so far using a small case study of building a text classifier for a real-world scenario.<a contenteditable="false" data-primary="active learning" data-secondary="text classification" data-startref="term65" data-type="indexterm" id="idm45969598157112"/><a contenteditable="false" data-primary="transfer learning" data-startref="term46" data-type="indexterm" id="idm45969598112648"/><a contenteditable="false" data-primary="domain adaptation" data-startref="term47" data-type="indexterm" id="idm45969598111336"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Case Study: Corporate Ticketing"><div class="sect1" id="case_study_corporate_ticketing">
<h1>Case Study: Corporate Ticketing</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="case study" data-type="indexterm" id="term48"/><a contenteditable="false" data-primary="corporate ticketing (case study)" data-type="indexterm" id="term49"/><a contenteditable="false" data-primary="Spoke corporate ticketing system (case study)" data-type="indexterm" id="term51"/>Let’s consider a real-world scenario and learn how we can apply some of the concepts we’ve discussed in this section. Imagine we’re asked to build a ticketing system for our organization that will track all the tickets or issues people face in the organization and route them to either internal or external agents. <a data-type="xref" href="#a_corporate_ticketing_system">Figure 4-9</a> shows a representative screenshot for such a system; it’s a corporate ticketing system called Spoke.</p>

<figure><div id="a_corporate_ticketing_system" class="figure"><img alt="A corporate ticketing system" src="Images/pnlp_0409.png" width="1438" height="825"/>
<h6><span class="label">Figure 4-9. </span>A corporate ticketing system</h6>
</div></figure>

<p>Now let’s say our company has recently hired a medical counsel and partnered with a hospital. So our system should also be able to pinpoint any medical-related issue and route it to the relevant people and teams. But while we have some past tickets, none of them are labeled as health related. In the absence of these labels, how will we go about building such a health issue–related classification system?</p>

<p>Let’s explore a couple of options:</p>

<dl>
	<dt><a contenteditable="false" data-primary="text classification" data-secondary="with existing APIs or libraries" data-secondary-sortas="existing" data-type="indexterm" id="idm45969598097976"/><a contenteditable="false" data-primary="APIs" data-secondary="text classification with" data-type="indexterm" id="idm45969598096280"/>Use existing APIs or libraries</dt> 
		<dd><p>One option is to start with a public API or library and map its classes to what’s relevant to us. For instance, the Google APIs<a contenteditable="false" data-primary="Google APIs" data-type="indexterm" id="idm45969598094040"/> mentioned earlier in the chapter can classify content into over 700 categories. There are 82 categories associated with medical or health issues<a contenteditable="false" data-primary="healthcare" data-secondary="Google APIs" data-type="indexterm" id="idm45969598092552"/>. These include categories like /Health/Health Conditions/Pain Management, /Health/Medical Facilities &amp; Services/Doctors’ Offices, /Finance/Insurance/Health Insurance, etc.</p>

		<p>While not all categories are relevant to our organization, some could be, and we can map these accordingly. For example, let’s say our company doesn’t consider substance abuse and obesity issues as relevant for medical counsel. We can ignore /Health/Substance Abuse and /Health/Health Conditions/Obesity in this API. Similarly, whether insurance should be a part of HR or referred outside can also be handled with these categories.</p>
	</dd>
	<dt><a contenteditable="false" data-primary="text classification" data-secondary="with public datasets" data-secondary-sortas="public" data-type="indexterm" id="idm45969598089576"/><a contenteditable="false" data-primary="public datasets" data-type="indexterm" id="idm45969598087928"/><a contenteditable="false" data-primary="datasets" data-secondary="public" data-type="indexterm" id="idm45969598086824"/><a contenteditable="false" data-primary="datasets" data-secondary="text classification with" data-type="indexterm" id="idm45969598085448"/>Use public datasets</dt> 
		<dd><p>We can also adopt public datasets for our needs. For example, 20 Newsgroups<a contenteditable="false" data-primary="20 Newsgroups dataset" data-primary-sortas="twenty" data-type="indexterm" id="idm45969598083272"/> is a popular text classification dataset, and it’s also part of the sklearn library. It has a range of topics, including sci.med. We can also use it to train a basic classifier, classifying all other topics in one category and sci.med in another.</p>
	</dd>
	<dt>Utilize weak supervision<a contenteditable="false" data-primary="weak supervision" data-type="indexterm" id="idm45969598080856"/></dt> 
		<dd><p>We have a history of past tickets, but they’re not labeled. So, we can consider bootstrapping a dataset out of it using the approaches described earlier in this section. For example, consider having a rule: “If the past ticket contains words like fever, diarrhea, headache, or nausea, put them in the medical counsel category.” This rule can create a small amount of data, which we can use as a starting point for our classifier.</p>
	</dd>
	<dt><a contenteditable="false" data-primary="active learning" data-secondary="text classification" data-type="indexterm" id="idm45969598078312"/>Active learning</dt> 
		<dd><p>We can use tools like Prodigy<a contenteditable="false" data-primary="Prodigy" data-type="indexterm" id="idm45969598076152"/><a contenteditable="false" data-primary="active learning" data-secondary="with Prodigy" data-secondary-sortas="Prodigy" data-type="indexterm" id="idm45969598075048"/> to conduct data collection experiments where we ask someone working at the customer service desk to look at ticket descriptions and tag them with a preset list of categories. <a data-type="xref" href="#active_learning_with_prodigy">Figure 4-10</a> shows an example of using Prodigy for this purpose.</p>

	<figure class="no-frame"><div id="active_learning_with_prodigy" class="figure"><img alt="Active learning with Prodigy" src="Images/pnlp_0410.png" width="1429" height="995"/>
	<h6><span class="label">Figure 4-10. </span>Active learning with Prodigy</h6>
	</div></figure>
	</dd>
	<dt>Learning from implicit and explicit feedback<a contenteditable="false" data-primary="feedback" data-secondary="learning from" data-type="indexterm" id="idm45969598069320"/></dt> 
		<dd><p>Throughout the process of building, iterating, and deploying this solution, we’re getting feedback that we can use to improve our system. Explicit feedback<a contenteditable="false" data-primary="feedback" data-secondary="explicit" data-type="indexterm" id="idm45969598067192"/> could be when the medical counsel or hospital says explicitly that the ticket was not relevant. Implicit feedback<a contenteditable="false" data-primary="feedback" data-secondary="implicit" data-type="indexterm" id="idm45969598065560"/> could be extracted from other dependent variables like ticket response times and ticket response rates. All of these could be factored in to improve our model using active learning techniques.</p>
	</dd>
</dl>

<p>A sample pipeline summarizing these ideas may look like what’s shown in <a data-type="xref" href="#a_pipeline_for_building_a_classifier_wh">Figure 4-11</a>. We start with no labeled data and use either a public API or a model created with a public dataset or weak supervision as the first baseline model. Once we put this model to production, we’ll get explicit and implicit signals on where it’s working or failing. We use this information to refine our model and active learning to select the best set of instances that need to be labeled. Over time, as we collect more data, we can build more sophisticated and deeper models.</p>

<figure><div id="a_pipeline_for_building_a_classifier_wh" class="figure"><img alt="A pipeline for building a classifier when there’s no training data" src="Images/pnlp_0411.png" width="1225" height="992"/>
<h6><span class="label">Figure 4-11. </span><a contenteditable="false" data-primary="text classification" data-secondary="with no or less data" data-secondary-sortas="no" data-type="indexterm" id="idm45969598060312"/><a contenteditable="false" data-primary="training data" data-secondary="no data" data-type="indexterm" id="idm45969598058568"/>A pipeline for building a classifier when there’s no training data</h6>
</div></figure>

<p>In this section, we started looking at a practical scenario of not having enough training data for building our own text classifier for our custom problem. We discussed several possible solutions to address the issue. Hopefully, this helps you foresee and prepare for some of the scenarios related to data collection and creation in your future projects related to text<a contenteditable="false" data-primary="text classification" data-secondary="case study" data-startref="term48" data-type="indexterm" id="idm45969598056056"/><a contenteditable="false" data-primary="corporate ticketing (case study)" data-startref="term49" data-type="indexterm" id="idm45969598054408"/><a contenteditable="false" data-primary="Spoke corporate ticketing system (case study)" data-startref="term51" data-type="indexterm" id="idm45969598053016"/> classification.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Practical Advice"><div class="sect1" id="practical_advice-id00010">
<h1>Practical Advice</h1>

<p><a contenteditable="false" data-primary="text classification" data-secondary="practical advice" data-type="indexterm" id="term52"/>So far, we’ve shown a range of different methods for building text classifiers and potential issues you may run into. We’d like to end this chapter with some practical advice that summarizes our observations and experience with building text classification systems in industry. Most of these are generic enough to be applied to other topics in the book as well.</p>

<dl>
	<dt>Establish strong baselines</dt> 
		<dd><p>A common fallacy is to start with a state-of-the-art algorithm. This is especially true in the current era of deep learning, where every day, new approaches/algorithms keep coming up. However, it’s always good to start with simpler approaches and try to establish strong baselines first. This is useful for three main reasons:</p>

	<ol type="a">
		<li>
		<p>It helps us get a better understanding of the problem statement and key challenges.</p>
		</li>
		<li>
		<p>Building a quick MVP helps us get initial feedback from end users and stakeholders.</p>
		</li>
		<li>
		<p>A state-of-the-art research model may give us only a minor improvement compared to the baseline, but it might come with a huge amount of technical debt.</p>
		</li>
	</ol>
	</dd>
	<dt><a contenteditable="false" data-primary="bias" data-type="indexterm" id="idm45969598041480"/>Balance training data</dt> 
		<dd><p>While working with classification, it’s very important to have a balanced dataset where all categories have an equal representation. An imbalanced dataset can adversely impact the learning of the algorithm and result in a <em>biased</em> classifier. While we cannot always control this aspect of the training data, there are various techniques to fix class imbalance in the training data. Some of them are collecting more data, resampling (undersample from majority classes or oversample from minority classes), and weight balancing.</p>
	</dd>
	<dt>Combine models and humans in the loop</dt> 
		<dd><p>In practical scenarios, it makes sense to combine the outputs of multiple classification models with handcrafted rules from domain experts to achieve the best performance for the business. In other cases, it’s practical to defer the decision to a human evaluator if the machine is not sure of its classification decision. Finally, there could also be scenarios where the learned model has to change with time and newer data. We’ll discuss some solutions for such scenarios in <a data-type="xref" href="ch11.xhtml#the_end_to_end_nlp_process">Chapter 11</a>, which focuses on end-to-end systems.</p>
	</dd>
	<dt>Make it work, make it better</dt> 
		<dd><p>Building a classification system is not just about building a model. For most industrial settings, building a model is often just 5% to 10% of the total project. The rest consists of gathering data, building data pipelines, deployment, testing, monitoring, etc. It is always good to build a model quickly, use it to build a system, then start improvement iterations. This helps us to quickly identify major roadblocks and the parts that need the most work, and it’s often not the modeling part.</p>
	</dd>
	<dt>Use the wisdom of many</dt> 
		<dd><p>Every text classification algorithm has its own strengths and weaknesses. There is no single algorithm that always works well. One way to circumvent this is via <em>ensembling</em><a contenteditable="false" data-primary="ensembling" data-type="indexterm" id="idm45969598032488"/>: training multiple classifiers. The data is passed through every classifier, and the predictions generated are combined (e.g., majority voting) to arrive at a final class prediction. An interested reader can look at the work of Dong et al. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598031000-marker" href="ch04.xhtml#idm45969598031000">35</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969598029784-marker" href="ch04.xhtml#idm45969598029784">36</a>] for a deep dive into ensemble methods for text<a contenteditable="false" data-primary="text classification" data-startref="term1" data-type="indexterm" id="idm45969598028472"/><a contenteditable="false" data-primary="text classification" data-secondary="practical advice" data-startref="term52" data-type="indexterm" id="idm45969598027128"/> classification.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Wrapping Up"><div class="sect1" id="wrapping_up-id00067">
<h1>Wrapping Up</h1>

<p>In this chapter, we saw how to address the problem of text classification from multiple viewpoints. We discussed how to identify a classification problem, tackle the various stages in a text classification pipeline, collect data to create relevant datasets, use different feature representations, and train several classification algorithms. With this, we hope you’re now well-equipped and ready to solve text classification problems for your use case and scenario and understand how to use existing solutions, build our own classifiers using various methods, and tackle the roadblocks you may face in the process. We focused on only one aspect of building text classification systems in industry applications: building the model. Issues related to the end-to-end deployment of NLP systems will be dealt with in <a data-type="xref" href="ch11.xhtml#the_end_to_end_nlp_process">Chapter 11</a>. In the next chapter, we’ll use some of the ideas we learned here to tackle a related but different NLP problem: information extraction.</p>
</div></section>
<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ch04fn1"><sup><a href="ch04.xhtml#ch04fn1-marker">i</a></sup> There are other such pre-trained embeddings available. Our choice in this case is arbitrary.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608485080">[<a href="ch04.xhtml#idm45969608485080-marker">1</a>] United States Postal Service. <a href="https://oreil.ly/g32q4"><em>The United States Postal Service: An American History</em></a>, 57–60. ISBN: 978-0-96309-524-4. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608475224">[<a href="ch04.xhtml#idm45969608475224-marker">2</a>] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and Navaneethan Santhanam. “Noise reduction and smart ticketing for social media-based communication systems.” US Patent Application 20190026653, filed January 24, 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608474296">[<a href="ch04.xhtml#idm45969608474296-marker">3</a>] Spasojevic, Nemanja and Adithya Rao. “Identifying Actionable Messages on Social Media.” <em>2015 IEEE International Conference on Big Data</em>: 2273–2281.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608437416">[<a href="ch04.xhtml#idm45969608437416-marker">4</a>] <a href="https://oreil.ly/qBOLP">CLPSYCH: Computational Linguistics and Clinical Psychology Workshop</a>. Shared Tasks 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_43">[<a href="ch04.xhtml#footnote_43-marker">5</a>] Google Cloud. <a href="https://oreil.ly/6JR2T">“Natural Language”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608374888">[<a href="ch04.xhtml#idm45969608374888-marker">6</a>] <a href="https://oreil.ly/NlU3m">Amazon Comprehend</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608373592">[<a href="ch04.xhtml#idm45969608373592-marker">7</a>] <a href="https://oreil.ly/7qZSK">Azure Cognitive Services</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608363832">[<a href="ch04.xhtml#idm45969608363832-marker">8</a>] Iderhoff, Nicolas. <a href="https://oreil.ly/NcwbT">nlp-datasets: Alphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP)</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_4_5">[<a href="ch04.xhtml#footnote_4_5-marker">9</a>] Kaggle. <a href="https://oreil.ly/Imbhb">“Sentiment Analysis: Emotion in Text”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608357816">[<a href="ch04.xhtml#idm45969608357816-marker">10</a>] UC Irvine Machine Learning Repository. <a href="https://oreil.ly/YsY4f">A collection of repositories for machine learning</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608354776">[<a href="ch04.xhtml#idm45969608354776-marker">11</a>] Google. <a href="https://oreil.ly/GJxBp">“Dataset Search”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_4_7">[<a href="ch04.xhtml#footnote_4_7-marker">12</a>] Jurafsky, Dan and James H. Martin. <a href="https://oreil.ly/Ta16f"><em>Speech and Language Processing</em></a>, Third Edition (Draft), 2018.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608050264">[<a href="ch04.xhtml#idm45969608050264-marker">13</a>] Lemaître, Guillaume, Fernando Nogueira, and Christos K. Aridas. <a href="https://oreil.ly/GIj0o">“Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning”</a>. <em>The Journal of Machine Learning Research</em> 18.1 (2017): 559–563.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608031928">[<a href="ch04.xhtml#idm45969608031928-marker">14</a>] For a detailed mathematical description of logistic regression, refer to Chapter 5 in [<a data-type="noteref" href="ch04.xhtml#footnote_4_7">12</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607783976">[<a href="ch04.xhtml#idm45969607783976-marker">15</a>] Google. <a href="https://oreil.ly/JLX5C">Pre-trained word2vec model</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607585544">[<a href="ch04.xhtml#idm45969607585544-marker">16</a>] Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. “Enriching Word Vectors with Subword Information.” <em>Transactions of the Association for Computational Linguistics</em> 5 (2017): 135–146.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607579464">[<a href="ch04.xhtml#idm45969607579464-marker">17</a>] Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. <a href="https://oreil.ly/uJX-t">“Bag of Tricks for Efficient Text Classification”</a>. (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607576808">[<a href="ch04.xhtml#idm45969607576808-marker">18</a>] Ramesh, Sree Harsha. <a href="https://oreil.ly/MaLab">torchDatasets</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607490152">[<a href="ch04.xhtml#idm45969607490152-marker">19</a>] Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. <a href="https://oreil.ly/LEf1y">“Fasttext.zip: Compressing text classification models”</a>. (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607471976">[<a href="ch04.xhtml#idm45969607471976-marker">20</a>] For older Doc2vec versions, there are some pre-trained models; e.g., <a class="orm:hideurl" href="https://oreil.ly/kt0U0"><em class="hyperlink">https://oreil.ly/kt0U0</em></a> (last accessed June 15, 2020).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607457992">[<a href="ch04.xhtml#idm45969607457992-marker">21</a>] Natural Language Toolkit. <a href="https://www.nltk.org">“NLTK 3.5 documentation”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607196824">[<a href="ch04.xhtml#idm45969607196824-marker">22</a>] Lau, Jey Han and Timothy Baldwin. <a href="https://oreil.ly/SgtZK">“An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation”</a>. (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607045416">[<a href="ch04.xhtml#idm45969607045416-marker">23</a>] Stanford Artificial Intelligence Laboratory. <a href="https://oreil.ly/ehHdC">“Large Movie Review Dataset”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607043736">[<a href="ch04.xhtml#idm45969607043736-marker">24</a>] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep Learning</em>. Cambridge: MIT Press, 2016. ISBN: 978-0-26203-561-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969607042568">[<a href="ch04.xhtml#idm45969607042568-marker">25</a>] Goldberg, Yoav. “Neural Network Methods for Natural Language Processing.” <em>Synthesis Lectures on Human Language Technologies</em> 10.1 (2017): 1–309.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_4_35">[<a href="ch04.xhtml#footnote_4_35-marker">26</a>] Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “‘Why Should I Trust You?’ Explaining the Predictions of Any Classifier.” <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2016): 1135–1144.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598370520">[<a href="ch04.xhtml#idm45969598370520-marker">27</a>] Lundberg, Scott M. and Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” <em>Advances in Neural Information Processing Systems</em> 30 (NIPS 2017): 4765–4774.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_4_33">[<a href="ch04.xhtml#footnote_4_33-marker">28</a>] Marco Tulio Correia Ribeiro. <a href="https://oreil.ly/AadAv">Lime: Explaining the predictions of any machine learning classifier</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598367560">[<a href="ch04.xhtml#idm45969598367560-marker">29</a>] Lundberg, Scott. <a href="https://oreil.ly/Spm6i">shap: A game theoretic approach to explain the output of any machine learning model</a>, (GitHub repo).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598210664">[<a href="ch04.xhtml#idm45969598210664-marker">30</a>] Snorkel<a contenteditable="false" data-primary="Snorkel software" data-type="indexterm" id="idm45969598210104"/>. <a href="https://www.snorkel.org">“Programmatically Building and Managing Training Data”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598207704">[<a href="ch04.xhtml#idm45969598207704-marker">31</a>] Bach, Stephen H., Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen et al. <a href="https://oreil.ly/CnWxH">“Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale”</a>. (2018).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598206104">[<a href="ch04.xhtml#idm45969598206104-marker">32</a>] Snorkel. <a href="https://oreil.ly/3emjt">“Snorkel Intro Tutorial: Data Labeling”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598179704">[<a href="ch04.xhtml#idm45969598179704-marker">33</a>] <a href="https://prodi.gy">Prodigy</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598165336">[<a href="ch04.xhtml#idm45969598165336-marker">34</a>] Fast.ai. <a href="https://oreil.ly/vHgQk">“Introducing state of the art text classification with universal language models”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598031000">[<a href="ch04.xhtml#idm45969598031000-marker">35</a>] Dong, Yan-Shi and Ke-Song Han. “A comparison of several ensemble methods for text categorization.” <em>IEEE International Conference on Services Computing</em> (2004): 419–422.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969598029784">[<a href="ch04.xhtml#idm45969598029784-marker">36</a>] Caruana, Rich, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. “Ensemble Selection from Libraries of Models.” <em>Proceedings of the Twenty-First International Conference on Machine Learning</em> (2004): 18.</p></div></div></section></div>



  </body>
</html>