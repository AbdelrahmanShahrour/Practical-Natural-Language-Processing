<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />
<style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1&gt;p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1&gt;p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]&gt;div&gt;h1,#sbo-rt-content section[data-type="preface"]&gt;div&gt;h1,#sbo-rt-content section[data-type="appendix"]&gt;div&gt;h1,#sbo-rt-content section[data-type="glossary"]&gt;div&gt;h1,#sbo-rt-content section[data-type="bibliography"]&gt;div&gt;h1,#sbo-rt-content section[data-type="index"]&gt;div&gt;h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000;padding-top:.25em !important;margin-top:0 !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]&gt;div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dl{margin-bottom:1.5em !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important;line-height:1.25rem;font-style:italic}#sbo-rt-content dd{margin:10px 0 .25em 1.5em !important;line-height:1.65em !important}#sbo-rt-content dd p{padding:0 !important;margin:0 0 10px !important}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul&gt;li,#sbo-rt-content ol ul,#sbo-rt-content ol ul&gt;li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul&gt;li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul&gt;li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul&gt;li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol&gt;li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol&gt;li,#sbo-rt-content ul ol,#sbo-rt-content ul ol&gt;li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol&gt;li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol&gt;li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol&gt;li&gt;ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol&gt;li&gt;ol&gt;li&gt;ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content table li{margin:10px 0 0 .25em !important}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:top;font-size:80%}#sbo-rt-content th{vertical-align:bottom}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller;word-break:break-all}#sbo-rt-content table.border tbody&gt;tr:last-child&gt;td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:2em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content pre.break-code,#sbo-rt-content code.break-code,#sbo-rt-content .break-code pre,#sbo-rt-content .break-code code{word-break:break-all}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10,#sbo-rt-content figure.width-10 img{width:10% !important}#sbo-rt-content .width-20,#sbo-rt-content figure.width-20 img{width:20% !important}#sbo-rt-content .width-30,#sbo-rt-content figure.width-30 img{width:30% !important}#sbo-rt-content .width-40,#sbo-rt-content figure.width-40 img{width:40% !important}#sbo-rt-content .width-50,#sbo-rt-content figure.width-50 img{width:50% !important}#sbo-rt-content .width-60,#sbo-rt-content figure.width-60 img{width:60% !important}#sbo-rt-content .width-70,#sbo-rt-content figure.width-70 img{width:70% !important}#sbo-rt-content .width-80,#sbo-rt-content figure.width-80 img{width:80% !important}#sbo-rt-content .width-90,#sbo-rt-content figure.width-90 img{width:90% !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100% !important}#sbo-rt-content .sc{text-transform:none !important}#sbo-rt-content .right{float:none !important}#sbo-rt-content a.totri-footnote{padding:0 !important}#sbo-rt-content figure.width-10,#sbo-rt-content figure.width-20,#sbo-rt-content figure.width-30,#sbo-rt-content figure.width-40,#sbo-rt-content figure.width-50,#sbo-rt-content figure.width-60,#sbo-rt-content figure.width-70,#sbo-rt-content figure.width-80,#sbo-rt-content figure.width-90{width:auto !important}#sbo-rt-content p img,#sbo-rt-content pre img{width:1.25em;line-height:1em;margin:0 .15em -.2em}#sbo-rt-content figure.no-frame div.border-box{border:none}#sbo-rt-content .right{text-align:right !important}
    </style>
<style type="text/css" id="font-styles">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }</style>
<style type="text/css" id="font-family">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }</style>
<style type="text/css" id="column-width">#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }</style>

<style type="text/css">body{margin:1em;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}body{background-color:transparent!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Chatbots"><div class="chapter" id="chatbots">
<h1><span class="label">Chapter 6. </span>Chatbots</h1>
<blockquote class="right">
<p class="right"><em>One machine can do the work of fifty ordinary men.</em><br/>
<em>No machine can do the work of one extraordinary man.</em></p>
<p data-type="attribution" style="text-align:right"><em>Elbert Green Hubbard</em></p>
</blockquote>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" id="ch06_term1"/>Chatbots are interactive systems that allow users to interact in natural language. They generally<a contenteditable="false" data-type="indexterm" data-primary="Hubbard, Elbert Green" id="idm45969596549272"/> interact via text but can also use speech interfaces. Early 2016 saw the introduction of the first wave of chatbots that soon became ubiquitous. Platforms like Facebook Messenger<a contenteditable="false" data-type="indexterm" data-primary="Facebook Messenger" id="idm45969596547848"/>, Google Assistant<a contenteditable="false" data-type="indexterm" data-primary="Google Assistant" id="idm45969596546616"/>, and Amazon Alexa<a contenteditable="false" data-type="indexterm" data-primary="Amazon Alexa" id="idm45969596545352"/> are some examples of chatbots<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="examples" id="idm45969596544088"/>. There are now tools that allow developers to create custom chatbots [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596542584-marker" href="ch06.xhtml#idm45969596542584">1</a>] for their brand or service so that consumers can carry out some of their daily actions from within their messaging platforms.</p>
<p>The introduction of chatbots into society has brought us to the beginning of a new era in technology: the era of the conversational interface. It’s an interface that soon won’t require a screen or a mouse to use. There will be no need to click or swipe; just the use of voice will be enough. This interface will be completely conversational, and those conversations will be indistinguishable from the conversations we have with our friends and family. Since chatbots deal with text under the hood, it’s all about understanding the text responses coming from users and producing reasonable replies. From understanding to generation, NLP plays a significant role, which we’ll see throughout this chapter.</p>
<p>The history of chatbots and of artificial intelligence<a contenteditable="false" data-type="indexterm" data-primary="artificial intelligence (AI)" id="idm45969596539464"/> in general are pretty intertwined. In the 1950s and ’60s, computer scientists Alan Turing<a contenteditable="false" data-type="indexterm" data-primary="Turing, Alan" id="idm45969596538152"/> and Joseph Weizenbaum<a contenteditable="false" data-type="indexterm" data-primary="Weizenbaum, Joseph" id="idm45969596536888"/> contemplated the concept of computers communicating like humans do. Later, in 1966, Joseph Weizebaum built Eliza<a contenteditable="false" data-type="indexterm" data-primary="Eliza chatbot" id="idm45969596535496"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596534264-marker" href="ch06.xhtml#idm45969596534264">2</a>], the first chatterbot ever coded, using only 200 lines of code. Eliza imitated the language of a Rogerian Psychotherapist using regular expressions and rules. Humans knew they were interacting with a computer program, and yet, through the emotional responses Eliza would offer, still grew emotionally attached to the program during trials.</p>
<p>Later, in the advent of powerful signal processing tools, researchers focused on building spoken dialog tools with the goal of improving user experience. Many spoken dialog systems were built between 1980 and 2000 and started as military-based projects (by DARPA<a contenteditable="false" data-type="indexterm" data-primary="DARPA" id="idm45969596531976"/>) aimed mainly at improving automated communication with soldiers. The systems were used to provide instructions, which later translated into chatbots capable of helping users get answers to frequently asked questions for various services. The bots were still handcrafted such that responses they generated were fixed, and the bots were not good at handling the context provided in the conversation.</p>
<p>In recent years, chatbots have become more feasible and useful, both due to the ubiquity of smartphones and recent advances in ML and DL. In addition to APIs to create chatbots on popular messaging platforms like Facebook Messenger, we now have various platforms to create the AI and logic behind the chatbots. This has allowed folks and companies with limited AI background and experience to deploy their own chatbots easily.</p>
<p>This chapter aims to cover the underlying systems and theory of chatbots, along with practical, hands-on experience building chatbots using different scenarios. We’ll end with some state-of-the-art research that may bring major advances to this entire paradigm. We’ll motivate our readers by introducing popular applications of chatbots.</p>
<section data-type="sect1" data-pdf-bookmark="Applications"><div class="sect1" id="applications-id00001">
<h1>Applications</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="applications" id="ch06_term2"/>Chatbots can be used for many different tasks in many different industries, from retail, to news, and even the medical field. We’ll briefly discuss various applications of chatbots. Many of these use cases have become more mature in recent years, while some are still in their infancy. These use cases include:</p>
<dl>
<dt>Shopping and e-commerce</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="shopping" data-see="e-commerce and retail" id="idm45969596523432"/><a contenteditable="false" data-type="indexterm" data-primary="e-commerce and retail" data-secondary="chatbots" id="idm45969596522056"/>Recently, chatbots are being used for various e-commerce operations, including placing or modifying an order, payment, etc. Bots for recommending various items are also of great interest to the e-commerce industry. Industries are focused on building conversational recommendation systems to provide a more seamless user experience.</dd>
<dt>News and content discovery</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="news and content discovery" id="idm45969596519496"/><a contenteditable="false" data-type="indexterm" data-primary="content discovery" data-secondary="chatbots in" id="idm45969596518376"/>Similar to e-commerce, chatbots can be used in news and content discovery. Users may specify various nuances of their search in a conversational manner, and the bot should be able respond with relevant articles.</dd>
<dt>Customer service</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="customer support" data-secondary="chatbots in" id="idm45969596515912"/>Customer service is another area where bots are used heavily. They’re used to lodge complaints, help answer FAQs, and navigate queries in pre-defined conversational flows set by the business requirements.</dd>
<dt>Medical</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="healthcare" data-secondary="chatbots in" id="idm45969596513448"/><a contenteditable="false" data-type="indexterm" data-primary="medical care" data-see="healthcare" id="idm45969596512072"/>In health and medical applications, FAQ bots are of great use. These bots can help patients fetch relevant information quickly based on their symptoms. Recently, there has also been interest in building chatbots that elicit useful information from patients, especially older patients, regarding their health conditions by asking relevant questions.</dd>
<dt>Legal</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="legal services" data-secondary="chatbots" id="idm45969596509464"/>In legal applications, bots can also be used to serve FAQs for users. They can even be used for more complex goals, such as asking follow-up questions. For example, if a user asks for legal articles to follow up on a case, a bot might ask specific questions regarding the nature of the case to find a more appropriate match.</dd>
</dl>
<p>Here’s a more elaborate example of an FAQ bot, which is common in many service platforms, to help users by providing answers to frequently asked questions.</p>
<section data-type="sect2" data-pdf-bookmark="A Simple FAQ Bot"><div class="sect2" id="a_simple_faq_bot">
<h2>A Simple FAQ Bot</h2>
<p>A FAQ bot<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="FAQ bot" id="ch06_term4"/><a contenteditable="false" data-type="indexterm" data-primary="FAQ bots" id="ch06_term3"/> is generally a search-based system where, given a question, it looks for correct answers and provides them to the user. It’s essentially a bot that allows a user to ask questions in different ways to get a response. Such bots are quite useful for providing a conversational interface to a complex set of questions.</p>
<p>As an example, we’ll consider a subset of Amazon Machine Learning Frequently Asked Questions. A machine needs to learn to provide the correct answer given similar questions, so it’s a good idea to have some paraphrases of each question. See <a data-type="xref" href="#amazon_ml_faq_to_be_used_for_a_faq_bot">Table 6-1</a> for some input-output examples for such a chatbot.</p>
<table class="border" id="amazon_ml_faq_to_be_used_for_a_faq_bot">
<caption><span class="label">Table 6-1. </span>Amazon ML FAQ to be used for a FAQ bot [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596498728-marker" href="ch06.xhtml#idm45969596498728">3</a>]</caption>
<thead>
<tr>
<th>Questions</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>What can I do with Amazon Machine Learning?<br/>How can I use Amazon Machine Learning?<br/>What can Amazon Machine Learning do?</td>
<td>You can use Amazon Machine Learning to create a wide variety of predictive applications. For example, you can use Amazon Machine Learning to help you build applications that flag suspicious transactions, detect fraudulent orders, forecast demand, etc.</td>
</tr>
<tr>
<td>What algorithm does Amazon Machine Learning use to generate models? How does Amazon Machine Learning build models?</td>
<td>Amazon Machine Learning currently uses an industry-standard logistic regression algorithm to generate models.</td>
</tr>
<tr>
<td>Are there limits to the size of the dataset I can use for training?<br/>What is the maximum size of training dataset?</td>
<td>Amazon Machine Learning can train models on datasets up to 100 GB in size.</td>
</tr>
</tbody>
</table>
<p><a data-type="xref" href="#an_faq_bot">Figure 6-1</a> is a working version of such an FAQ bot. Later in the chapter, we’ll learn how to build such a bot for various applications step by step.</p>
<figure><div id="an_faq_bot" class="figure">
<img src="Images/pnlp_0601.png" alt="An FAQ bot" width="394" height="515"/>
<h6><span class="label">Figure 6-1. </span>An FAQ bot</h6>
</div></figure>
<p>Now, we’ll transition to the taxonomy of chatbots and explain various categories of chatbot based<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="applications" data-startref="ch06_term2" id="idm45969596485192"/><a contenteditable="false" data-type="indexterm" data-primary="FAQ bots" data-startref="ch06_term3" id="idm45969596483544"/> on their usage.</p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="A Taxonomy of Chatbots"><div class="sect1" id="a_taxonomy_of_chatbots">
<h1>A Taxonomy of Chatbots</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="taxonomy of" id="ch06_term5"/>Let’s expand on chatbots of various uses and their applicability to various domains. Chatbots can be classified in many ways, which affects how they’re built and where they’re used. A way of looking at these chatbots is how they interact with the user:</p>
<dl>
<dt>Exact answer or FAQ bot<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="FAQ bot" data-startref="ch06_term4" id="idm45969596477288"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="exact answer" id="idm45969596475608"/> with limited conversations</dt>
<dd>These chatbots are linked to a fixed set of responses and retrieve a correct response based on understanding the user’s query. For example, if we build an FAQ bot, the bot has to understand the question and retrieve a fixed, correct answer for it. Generally, one response from the user does not depend on the previous responses. Take a look at <a data-type="xref" href="#types_of_chatbots">Figure 6-2</a>. In the FAQ bot example, we see that, in the first two turns, the bot provides a fixed response to similar questions that are asked with slight variations. For a different question, it pulls out a different answer.</dd>
<dt>Flow-based bot</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="flow-based" id="idm45969596471368"/>Flow-based conversational bots are generally more complex than FAQ bots in terms of the variability of their responses. Users may gradually express their opinions or requests over the course of conversations. For example, when ordering a pizza, a user may express their requested toppings, pizza size, and other nuances gradually. The bot should understand and track this information throughout the conversation to successfully generate a response every time. In <a data-type="xref" href="#types_of_chatbots">Figure 6-2</a>, for the flow-based bot, we see that the bot asks a specific set of questions to achieve the goal of making a pizza order. This flow was pre-defined, and the bot asks relevant questions to fulfill the order. We’ll discuss such a flow-based bot in greater detail later in this chapter.</dd>
<dt>Open-ended bot</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="open-ended" id="idm45969596467384"/>Open-ended bots are intended mainly for entertainment, where the bot is supposed to converse with the user about various topics. The bot doesn’t have to maintain specific directions or flows of the conversation. In <a data-type="xref" href="#types_of_chatbots">Figure 6-2</a>, the open-ended bot carries out a conversation without any pre-existing template or fixed question-answer pairs. It transitions fluently from one topic to another to maintain the interesting conversation. This example of an open-ended bot was built by one of the authors for a popular digital assistant platform.</dd>
</dl>


<figure><div id="types_of_chatbots" class="figure">
<img src="Images/pnlp_0602.png" alt="Types of chatbots" width="1411" height="1012"/>
<h6><span class="label">Figure 6-2. </span>Types of chatbots<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="open" id="idm45969596462504"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="FAQ bot" id="idm45969596461096"/><a contenteditable="false" data-type="indexterm" data-primary="FAQ bots" id="idm45969596459720"/></h6>
</div></figure>

<p>Chatbots are classified into two broad categories: (1) goal-oriented dialogs and (2) chitchats<a contenteditable="false" data-type="indexterm" data-primary="chitchats" id="idm45969596457960"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="chitchat types" id="idm45969596456856"/>. FAQ bots and flow-based bots fall into the first category, whereas open-ended bots are mainly chitchat types. Both of these types of bots are used heavily in industry and are also in the active area of research in academia.</p>

<section data-type="sect2" data-pdf-bookmark="Goal-Oriented Dialog"><div class="sect2" id="goal_oriented_dialog">
<h2>Goal-Oriented Dialog</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="goal-oriented dialogs" id="ch06_term7"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="goal-oriented dialog" id="ch06_term6"/>The natural human purpose of having a conversation is to accomplish a goal via relevant information seeking. In the similar line of thought, it’s easy to design any chatbot or conversation agent for a specific use case where the end goal is known. Most of the chatbots we’ve discussed so far (those typically used in research or industry) are goal-oriented chatbots. The user interacting with the chatbot should have complete information about what they want to achieve after the conversation. For example, looking for a movie recommendation or booking flight reservations through chatbots or conversational agents are examples of goal-oriented dialog where the goal is to watch a movie or book a flight.</p>
<p>Now, by definition, the goal-oriented systems are domain-specific, which requires domain-specific knowledge in the system. This hampers the generalizability and scalability of the chatbot framework. Research from Facebook [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596448984-marker" href="ch06.xhtml#idm45969596448984">4</a>] recently presented an end-to-end framework for training all components from the dialogs themselves to mitigate that limitation. This research proposes an automatic manipulation of the data—for example, question-answer pairs to carry out a meaningful conversation via required API calls. This is one of the newest approaches that researchers and industry practitioners have started to follow.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Chitchats"><div class="sect2" id="chitchats">
<h2>Chitchats</h2>
<p>Apart from goal-oriented conversations, humans also engage in unstructured, open-domain conversations without any specific goals. These human-human conversations involve free-form, opinionated discussions about various  topics. Having a conversational agent that can have a chitchat with a human is challenging due to the absence of objective goals. A conversational agent must generate coherent, on-topic, and factually correct responses to make the dialog more natural.</p>
<p>The application of chitchat bots is futuristic but holds immense potential. For example, these bots could be used to elicit useful but sensitive information in the case of a medical emergency for geriatric care. The free-form conversational bot could also be used to address the long-standing issue of loneliness and depression among teenagers and elderly people. Some of the market-leader companies, such as Amazon, Apple, and Google, to name a few, are investing heavily in building such bots for worldwide customers.</p>
<p class="pagebreak-before">So far, we’ve discussed various kinds of chatbots and their usage in various industries. This will allow us to appreciate various components of chatbots based on usage and also help us implement some of the components as we need them. Now, we’ll deep-dive into the chatbot development pipeline and discuss details of various<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="taxonomy of" data-startref="ch06_term5" id="idm45969596442888"/> <span class="keep-together">components.</span></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="A Pipeline for Building Dialog Systems"><div class="sect1" id="a_pipeline_for_building_dialog_systems">
<h1>A Pipeline for Building Dialog Systems</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="pipeline for" id="ch06_term9"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="pipeline for" id="ch06_term8"/>We discussed various NLP tasks, such as classification and entity detection, throughout Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#text_classification">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.xhtml#information_extraction">5</a>. Now, we’ll utilize some of them to describe an example pipeline to build a dialog system. <a data-type="xref" href="#pipeline_for_a_dialog_system">Figure 6-3</a> depicts a complete pipeline of a dialog system with various components. We’ll discuss the utility of each component and data flow through the pipeline.</p>
<figure><div id="pipeline_for_a_dialog_system" class="figure">
<img src="Images/pnlp_0603.png" alt="Pipeline for a dialog system" width="1390" height="515"/>
<h6><span class="label">Figure 6-3. </span>Pipeline for a dialog system</h6>
</div></figure>
<dl>
<dt>Speech recognition<a contenteditable="false" data-type="indexterm" data-primary="speech recognition" id="idm45969596428488"/></dt>
<dd>Usually, the dialog system works as an interface between human and machine, so the input into the dialog system is human speech. Speech recognition algorithms transcribe speech to natural text. In industrial dialog systems, state-of-the-art [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_5-marker" href="ch06.xhtml#footnote_6_5">5</a>] speech-to-text models are used, which is beyond the scope of this book. If you’re interested in speech models, refer to [<a data-type="noteref" href="ch06.xhtml#footnote_6_5">5</a>] for an overall view.</dd>
<dt>Natural language understanding (NLU)<a contenteditable="false" data-type="indexterm" data-primary="natural language understanding (NLU)" id="idm45969596423320"/><a contenteditable="false" data-type="indexterm" data-primary="NLU" data-see="natural language understanding" id="idm45969596422152"/></dt>
<dd>After transcribing, the system tries to analyze and “understand” the transcribed text. This module encompasses various natural language understanding tasks. Examples of such tasks are sentiment detection, named entity extraction, coreference resolution, etc. This module is primarily responsible for gathering all possible information that is implicitly (sentiment) or explicitly (named entities) present in the input text.</dd>
<dt>Dialog and task manager<a contenteditable="false" data-type="indexterm" data-primary="task managers" id="idm45969596419480"/></dt>
<dd>Once we obtain information from the input, a <em>dialog manager</em><a contenteditable="false" data-type="indexterm" data-primary="dialog managers" id="idm45969596417576"/>, as shown in the figure, gathers and systematically decides which pieces of information are important or not. A dialog manager is a module that controls and guides the flow of the conversation. Imagine this as a table containing information extracted in NLU steps and stored concurrently for all utterances in the ongoing conversation. The dialog manager develops a strategy via rules or other complex mechanisms, such as reinforcement learning, to effectively utilize the information obtained from the input. Dialog managers are mostly prevalent in goal-oriented dialogs since there’s a definite objective to reach via the conversation.</dd>
<dt>Natural language generation<a contenteditable="false" data-type="indexterm" data-primary="natural language generation" id="idm45969596415272"/></dt>
<dd>Finally, as the dialog manager decides a strategy for responding, the natural language generation module generates a response in a human-readable form according to the strategy devised by the dialog manager. The response generator could be template based or a generative model learned from data. After this, a speech synthesis<a contenteditable="false" data-type="indexterm" data-primary="speech synthesis" id="idm45969596413352"/> module converts the text back to speech to the end user. For more information on speech synthesis tasks, take a look at [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596411976-marker" href="ch06.xhtml#idm45969596411976">6</a>] and [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596410520-marker" href="ch06.xhtml#idm45969596410520">7</a>].</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>

<p>Any chatbot can be built using such a pipeline. For text-based chatbots, we can remove the speech processing components. While the NLU and generation component can be complex, a dialog manager could simply be rules routing the bot to an appropriate response generator.</p>
</div>
<p>Although the pipeline in <a data-type="xref" href="#pipeline_for_a_dialog_system">Figure 6-3</a> assumes the chatbot is voice based<a contenteditable="false" data-type="indexterm" data-primary="voice-based chatbots" data-seealso="chatbots" id="idm45969596405512"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="voice-based" id="idm45969596404168"/>, a similar pipeline without the speech processing modules will work for text-based chatbots<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="text-based" id="idm45969596402552"/><a contenteditable="false" data-type="indexterm" data-primary="text-based chatbots" id="idm45969596401176"/>. But in all industrial applications, we’re moving toward eventually having more and more voice-based systems, so the pipeline discussed here is more general, and it applies to a variety of applications we described previously (including the case study in <a data-type="xref" href="ch01.xhtml#nlp_a_primer">Chapter 1</a>). Now that we’ve briefly discussed the various components of a chatbot and how a conversation flow takes place, let’s deep dive to understand these components in detail.<a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="pipeline for" data-startref="ch06_term8" id="idm45969596398520"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="pipeline for" data-startref="ch06_term9" id="idm45969596396872"/></p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Dialog Systems in Detail"><div class="sect1" id="dialog_systems_in_detail">
<h1>Dialog Systems in Detail</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="in detail" data-secondary-sortas="detail" id="ch06_term10"/>The main idea of a dialog system or chatbot is to understand a user’s query or input and to provide an appropriate response. This is different from typical question-answering systems where, given a question, there has to be an answer. In a dialog setup, users may ask their queries in “turns.” In each turn, a user reveals their interest about the topic based on what the bot may have responded with. So, in a dialog system, the most important thing is understanding nuances from the user’s input in a turn-by-turn way and storing them in context to generate responses.</p>
<p>Before we get into the details of bots and dialog systems, we’ll cover the terminology used in dialog systems<a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="terminology" id="ch06_term11"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="terminology" id="ch06_term12"/> and chatbot development more broadly.</p>
<dl>
<dt>Dialog act or intent<a contenteditable="false" data-type="indexterm" data-primary="dialog act or intent" id="idm45969596386056"/></dt>
<dd>This is the aim of a user command. In traditional systems, the intent is a primary descriptor. Often, several other things, such as sentiment, can be linked to the intent. The intent is also called a “dialog act” in some literature. In the first example in <a data-type="xref" href="#example_of_different_terminology_used_i">Figure 6-4</a>, orderPizza is the intent of the user command. Similarly, in the second example, the user wants to know about a stock, so the intent is getStockQuote. These intents are usually pre-defined based on the chatbot’s domain of operation.</dd>
<dt>Slot or entity<a contenteditable="false" data-type="indexterm" data-primary="slots or entities" id="ch06_term13"/></dt>
<dd>This is the fixed ontological construct that holds information regarding specific entities related to the intent. The information related to each slot that’s surfaced in the original utterance is “value.” The slots and value together are sometimes denoted as an “entity.” <a data-type="xref" href="#example_of_different_terminology_used_i">Figure 6-4</a> shows two examples of entities. The first example looks for specific attributes of the pizza to be ordered: “medium” and “extra cheese.” On the other hand, the second example looks for the related entities for getStockQuote: the stock name and the time period the chatbot is asked for.</dd>
<dt>Dialog state or context<a contenteditable="false" data-type="indexterm" data-primary="dialog state or context" id="idm45969596378632"/></dt>
<dd>A dialog state is an ontological construct that contains both the information about the dialog act as well as state-value pairs. Similarly, context can be viewed as a set of dialog states that also captures previous dialog states as history.</dd>
</dl>
<figure class="width-50"><div id="example_of_different_terminology_used_i" class="figure">
<img src="Images/pnlp_0604.png" alt="Example of different terminology used in chatbots" width="602" height="407"/>
<h6><span class="label">Figure 6-4. </span>Example of different terminology used in chatbots</h6>
</div></figure>
<p class="pagebreak-before">Now, let’s complete a walkthrough using a cloud API called Dialogflow [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596373832-marker" href="ch06.xhtml#idm45969596373832">8</a>] for a fictional pizza shop to enable users to converse with a chatbot to order pizza. This is a goal-oriented system where the goal is to accommodate the user’s request and order a pizza.</p>
<section data-type="sect2" data-pdf-bookmark="PizzaStop Chatbot"><div class="sect2" id="pizzastop_chatbot">
<h2>PizzaStop Chatbot</h2>
<p>Dialogflow<a contenteditable="false" data-type="indexterm" data-primary="Dialogflow (Google)" id="idm45969596370408"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="build walkthrough" id="ch06_term14"/><a contenteditable="false" data-type="indexterm" data-primary="Dialogflow (Google)" data-secondary="chatbot build with" id="ch06_term15"/><a contenteditable="false" data-type="indexterm" data-primary="Google Cloud" data-secondary="Dialogflow" id="ch06_term16"/> is a conversational agent–building platform by Google. By providing the tools to understand and generate natural language and manage the conversation, Dialogflow enables us to easily create conversational experiences. While there are many other tools available, we chose this one because it’s easy to use, mature, and is being improved constantly.</p>
<p>Imagine there’s a fictional pizza shop called PizzaStop, and we have to build a chatbot that can take an order from a customer. A pizza can have multiple toppings (like onions, tomatoes, and peppers), and it can come in different sizes. An order can also contain one or more items from the sides, appetizers, and/or beverages section of the menu. Now that we understand the requirements, let’s begin building our bot using the Dialogflow framework.</p>
<section data-type="sect3" data-pdf-bookmark="Building our Dialogflow agent"><div class="sect3" id="building_our_dialogflow_agent">
<h3>Building our Dialogflow agent</h3>
<p>Before we begin creating our agent, we need to create an account and set up a few things. For this, open the official Dialogflow website [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596360856-marker" href="ch06.xhtml#idm45969596360856">9</a>], log in with your Google account, and provide the required permissions. Navigate to V2 of the API [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596359448-marker" href="ch06.xhtml#idm45969596359448">10</a>]. Click on “try it for free” and you’ll be directed to the free tier of Google Cloud Services, then you can follow the registration process.</p>
<ol>
<li><p>First, we need to create an agent. Click the Create Agent button, then enter the name of the agent. You can provide any name, but it’s good practice to provide a name that gives an idea of what the agent is used for. For our PizzaStop project, we’ll name our agent “Pizza.” Now, set the time zone and click the Create button.</p> 

<p><a data-type="xref" href="#creating_an_agent_using_dialogflow">Figure 6-5</a> shows the UI you’ll see while creating an agent.</p><br/>

<figure><div id="creating_an_agent_using_dialogflow" class="figure">
<img src="Images/pnlp_0605.png" alt="Creating an agent using Dialogflow" width="997" height="327"/>
<h6><span class="label">Figure 6-5. </span>Creating an agent using Dialogflow<a contenteditable="false" data-type="indexterm" data-primary="Dialogflow (Google)" data-secondary="creating agents with" id="idm45969596353064"/></h6>
</div></figure></li>
<!--/ol>
This should be the same list; keeping the markup just in case (de)
<ol-->

<li><p>You’ll then be redirected to another page with options that allow you to create the bot. <a data-type="xref" href="#dialogflow_ui_after_creating_an_agent">Figure 6-6</a> shows the UI of Dialogflow, which we’ll use multiple times while creating our agent. By default, we’ll already have two intents: <em>Default Fallback Intent</em> and <em>Default Welcome Intent</em><em>.</em> Default Fallback Intent is the default response if some internal API fails and Default Welcome Intent will generate a welcome message.</p><br/>
<figure><div id="dialogflow_ui_after_creating_an_agent" class="figure">
<img src="Images/pnlp_0606.png" alt="Dialogflow UI after creating an agent" width="1362" height="661"/>
<h6><span class="label">Figure 6-6. </span>Dialogflow UI after creating an agent</h6>
</div></figure>
</li>

<li class="less_space pagebreak-before"><p>Now, we need to add the intents and entities we care about to our agent. To add an intent, hover over the Intents block and click the + button. You’ll see something similar to <a data-type="xref" href="#dialogflow_ui_after_clicking_the_quotat">Figure 6-7</a>. These intents and entities are what we defined earlier in the section.</p><br/>
<figure><div id="dialogflow_ui_after_clicking_the_quotat" class="figure">
<img src="Images/pnlp_0607.png" alt="Dialogflow UI after clicking the + button" width="1161" height="905"/>
<h6><span class="label">Figure 6-7. </span>Dialogflow UI after clicking the “+” button</h6>
</div></figure></li>

<li><p>Now, we’ll create the first intent: orderPizza. As we create a new intent, we have to provide training examples, called “training phrases,” to enable the bot to detect variations of responses that belong to the intent. We also need to provide “context”: a piece of information that can be remembered over the span of a conversation and that will be used for subsequent intent detection.</p>
<p>Examples of training phrases are “I want to order a pizza” or “medium with cheese please.” The first one denotes a simple intent of pizza ordering, whereas the second one consists of entities that are useful to remember, such as medium size and cheese topping.</p> 

<p class="pagebreak-before"><a data-type="xref" href="#adding_training_phrases_for_intents">Figure 6-8</a> shows sample training phrases added to the agent.</p><br/>
<figure><div id="adding_training_phrases_for_intents" class="figure">
<img src="Images/pnlp_0608.png" alt="Adding training phrases for intents" width="715" height="530"/>
<h6><span class="label">Figure 6-8. </span>Adding training phrases for intents</h6>
</div></figure></li>

<li><p>Since we’ve included intent, we need to add the respective entities to remember important information provided by the user. Create an entity named pizzaSize, enable “fuzzy matching” (which matches entities even if they’re only approximately the same), and provide the necessary values. Similarly, create a pizzaTopping entity, but this time, also enable “Define synonyms” (this lets us define synonyms while allowing us to match several words, defined as synonyms, to the same entity).</p> 

<p>These two together will help us detect “medium size” and “cheese toppings,” as shown in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#creating_the_pizzasize_entity">6-9</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#creating_the_pizzatopping_entity">6-10</a>.</p><br/>
<figure><div id="creating_the_pizzasize_entity" class="figure">
<img src="Images/pnlp_0609.png" alt="Creating the pizzaSize entity" width="723" height="326"/>
<h6><span class="label">Figure 6-9. </span>Creating the pizzaSize entity</h6>
</div></figure>
<figure><div id="creating_the_pizzatopping_entity" class="figure">
<img src="Images/pnlp_0610.png" alt="Creating the pizzaTopping entity" width="761" height="444"/>
<h6><span class="label">Figure 6-10. </span>Creating the pizzaTopping entity</h6>
</div></figure></li>

<li><p>Now, let’s go back to the Intents block to add additional information to the Action and Parameters section. We need both the topping and size to complete the order, so we need to check the Required box on those. One pizza can’t be multiple sizes, but one pizza can have multiple toppings. So, enable the isList option for toppings to allow it to have multiple values.</p>
<p>A user might only mention the size <em>or</em> the topping. To gather the complete information, we need to add a prompt that asks follow-up questions, such as, “What size of pizza would you like?” as a prompt for pizzaSize. This is shown in <a data-type="xref" href="#actions_and_parameters_for_orderpizza_i">Figure 6-11</a>.</p>
<figure><div id="actions_and_parameters_for_orderpizza_i" class="figure">
<img src="Images/pnlp_0611.png" alt="Actions and parameters for orderPizza intent" width="731" height="312"/>
<h6><span class="label">Figure 6-11. </span>Actions and parameters for orderPizza intent</h6>
</div></figure></li>

<li><p>We also need to provide sample responses, as shown in <a data-type="xref" href="#adding_the_appropriate_responses_our_ag">Figure 6-12</a>, that the agent will give the user. We can ask the user if they need drinks, appetizers, or sides. If we were creating something like a billing intent, we could end the conversation after it by enabling the “Set this intent as end of conversation” slider in the Responses block.</p><br/>
<figure><div id="adding_the_appropriate_responses_our_ag" class="figure">
<img src="Images/pnlp_0612.png" alt="Adding the appropriate responses our agent should use" width="737" height="356"/>
<h6><span class="label">Figure 6-12. </span>Adding the appropriate responses our agent should use</h6>
</div></figure>
</li>

<li><p>So far, we’ve added a simple intent and entities. Now we can look at a complex entity with context. Consider the statement, “I want to order 2 L of juice and 3 wings.” Our agent needs to recognize the quantity and the item ordered. This is done by adding a custom entity in Dialogflow. We’ve created an entity called compositeSide, and it can handle all of these combinations. For example, in “@sys.number-integer:number-integer @appetizer:appetizer”, the first entity deals with recognizing how many of the appetizers are ordered, and the next one deals with the type of appetizer, as shown in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#creating_the_compositeside_entity">6-13</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#example_of_a_complex_statement_with_mul">6-14</a>. As you can see, the signatures of these entities are given as regular expressions.</p><br/>
<figure><div id="creating_the_compositeside_entity" class="figure">
<img src="Images/pnlp_0613.png" alt="Creating the compositeSide entity" width="775" height="323"/>
<h6><span class="label">Figure 6-13. </span>Creating the compositeSide entity</h6>
</div></figure>
<figure><div id="example_of_a_complex_statement_with_mul" class="figure">
<img src="Images/pnlp_0614.png" alt="Example of a complex statement with multiple entities and context" width="291" height="501"/>
<h6><span class="label">Figure 6-14. </span>Example of a complex statement with multiple entities and context</h6>
</div></figure>
</li>

<li><p>We can add many more intents and entities to make our agent robust. In <a data-type="xref" href="#all_the_intents_for_this_agent">Figure 6-15</a> and <a data-type="xref" href="#all_the_entities_for_this_agent">Figure 6-16</a>, take a look at examples of other intents and entities we added to enrich and enhance the user’s pizza-buying experience.</p>
<figure><div id="all_the_intents_for_this_agent" class="figure">
<img src="Images/pnlp_0615.png" alt="All the intents for this agent" width="769" height="336"/>
<h6><span class="label">Figure 6-15. </span>All the intents for this agent</h6>
</div></figure>
<figure><div id="all_the_entities_for_this_agent" class="figure">
<img src="Images/pnlp_0616.png" alt="All the entities for this agent" width="766" height="420"/>
<h6><span class="label">Figure 6-16. </span>All the entities for this agent</h6>
</div></figure>
</li></ol>
<p>Now that we’ve gone through the steps to build a bot for PizzaStop, we’ll test our bot to see how it works in various scenarios.</p>
</div></section>
<section data-type="sect3" class="pagebreak-before" data-pdf-bookmark="Testing our agent"><div class="sect3" id="testing_our_agent">
<h3 class="less_space">Testing our agent</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="testing" id="ch06_term17"/>Now, let’s test our agent in a website setting. For this, we need to open it in “web demo” mode. Click the Integrations block and scroll down until you reach Web Demo. Click the link in the pop-up window, and that’s it! Feel free to test your agent to your heart’s content. <a data-type="xref" href="#making_a_simple_order_usin">Figure 6-17</a> shows snippets of the one we built. Testing our bot is important for validating that it’s working. We’ll analyze a few cases of varying difficulty.</p>
<figure><div id="making_a_simple_order_usin" class="figure">
<img src="Images/pnlp_0617.png" alt="Making a simple order using our agent" width="1440" height="932"/>
<h6><span class="label">Figure 6-17. </span>Making a simple order using our agent</h6>
</div></figure>
<p>We can see in  <a data-type="xref" href="#making_a_simple_order_usin">Figure 6-17</a> that our bot is able to handle simple queries to order a pizza. As we have tested the bot end to end, we can also test various components of it individually. Testing individual components helps to prototype quickly and catch edges cases before the end-to-end testing.</p>
<p>Now, let’s go through a more complex example, which will be tested with an integration of this bot with Google Assistant. In the example shown in <a data-type="xref" href="#making_a_simple_order_usin">Figure 6-17</a>, our agent identifies the intent to order a pizza and recognizes the toppings we ordered. The pizzaSize entity is not fulfilled, so it asks a question regarding the size of the pizza to fulfill the entity’s requirement. With the orderPizza intent fulfilled, the agent then proceeds to ask us about sides and appetizers. Based on the statement we provided, the agent needs to fulfill the orderSize intent and should be able to identify the quantity of juice and the appetizer. This shows that the agent is able to handle complex entities. Finally, we move on to the conversation for selecting the type of payment. Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#texting_complex_statements_with_multipl">6-18</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#testing_with_a_complex_entity_and_conte">6-19</a> show how internal state and extracted entities work in another conversation.</p>
<figure class="width-100"><div id="texting_complex_statements_with_multipl" class="figure">
<img src="Images/pnlp_0618.png" alt="Texting complex statements with multiple entities" width="1442" height="723"/>
<h6><span class="label">Figure 6-18. </span>Texting complex statements with multiple entities</h6>
</div></figure>
<figure><div id="testing_with_a_complex_entity_and_conte" class="figure">
<img src="Images/pnlp_0619.png" alt="Testing with a complex entity and context" width="290" height="483"/>
<h6><span class="label">Figure 6-19. </span>Testing with a complex entity and context</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>

<p>Dialogflow allows us to build goal-oriented chatbots. It’s important to have an extensive ontology (possible slots and intents) for our domain, as it will make our bot rich in responding to varied user queries.</p>
</div>
<p>We’ve shown how to build a fully functional chatbot using the Dialogflow API. We learned about intents and entities—the two main building blocks of understanding dialog. Now, we’ll delve deep into building custom models for intent/dialog act classification and entity/slot identification.<a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="in detail" data-secondary-sortas="detail" data-startref="ch06_term10" id="idm45969596281576"/><a contenteditable="false" data-type="indexterm" data-primary="Dialogflow (Google)" data-secondary="chatbot build with" data-startref="ch06_term15" id="idm45969596279656"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="build walkthrough" data-startref="ch06_term14" id="idm45969596278008"/><a contenteditable="false" data-type="indexterm" data-primary="Google Cloud" data-secondary="Dialogflow" data-startref="ch06_term16" id="idm45969596276360"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="testing" data-startref="ch06_term17" id="idm45969596274712"/></p>
</div></section>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Deep Dive into Components of a Dialog System"><div class="sect1" id="deep_dive_into_components_of_a_dialog_s">
<h1>Deep Dive into Components of a Dialog System</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="components" id="ch06_term20"/>So far, we’ve seen how to build a chatbot using Dialogflow and how to add various features to handle complex entities and context. Now, we want to deep-dive into the machine learning aspect of the internals of a dialog system. As we discussed while describing the pipeline for a dialog system, understanding the context (i.e., the user response) in light of conversation history is one of the most important tasks for building a dialog system.</p>
<p>Understanding context can be broken down into understanding the user’s intent and detecting corresponding entities for that particular intent. These internal components correspond to the natural language understanding component in the chatbot pipeline. To illustrate this, we’ll go through a sample of a conversation on restaurant booking and describe how to model different components for context understanding.</p>
<p><a data-type="xref" href="#conversation_about_restaurant_booking_l">Figure 6-20</a> shows an example of a user looking for a restaurant reservation. As we can see, there are labels available for each response. The labels indicate intents and entities for these responses. We want to use such annotations to train our ML models.</p> 

<figure class="width-50"><div id="conversation_about_restaurant_booking_l" class="figure">
<img src="Images/pnlp_0620.png" alt="Conversation about restaurant booking [_17]" width="564" height="444"/>
<h6><span class="label">Figure 6-20. </span>Conversation about restaurant booking [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_17-marker" href="ch06.xhtml#footnote_6_17">11</a>]</h6>
</div></figure>

<p>Before we go into the model, we’ll formally define two natural understanding tasks related to context understanding for dialogs. Since this involves understanding the nuances of language underneath, these are also attributed as <em>natural language understanding (NLU)</em><a contenteditable="false" data-type="indexterm" data-primary="natural language understanding (NLU)" id="idm45969596261640"/> tasks.</p>


<section data-type="sect2" data-pdf-bookmark="Dialog Act Classification"><div class="sect2" id="dialog_act_classification">
<h2>Dialog Act Classification</h2>
<p><em>Dialog act classification</em><a contenteditable="false" data-type="indexterm" data-primary="dialog act classification" id="ch06_term21"/> is a task to identify how the user utterance plays a role in the context of dialog. This informs what “act” the user is performing. For example, a simple example of dialog acts would be to identify a “yes/no” question. If the user asks, “Are you going to school today?”, this would be classified as a yes/no question. On the other hand, if the user asks, “What is the depth of the ocean?”, that may not be classified as a yes/no question. We’ve seen that intents or dialog acts are important for building a chatbot, even in Cloud APIs. Identifying intent helps to understand what the user is asking for and to take actions accordingly.</p>
<div data-type="tip"><h6>Tip</h6>

<p>Building dialog act classification and slot identification from scratch can be a complex and data-consuming process. Doing so makes sense when our dialog acts and slots are more open-ended in nature than a Cloud API or existing framework can solve. Having complete control of dialog internals can yield better results over time in such problems.</p>
</div>
<p>This can be reframed as a classification problem: given a dialog utterance, classify it into dialog acts or labels. In our example from <a data-type="xref" href="#conversation_about_restaurant_booking_l">Figure 6-20</a>, we define a dialog act prediction task where labels include inform, request, etc. The utterance “Where is it?” can be classified as a dialog act “request.” On the other hand, the utterance “I’m looking for a cheaper restaurant” can be classified as an “inform” dialog act. Drawing on what we learned in <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>, we can use any classifier we like to solve this task. We’ll discuss the models pertaining to this task with a complete dataset example in <a data-type="xref" href="#dialog_examples_with_code_walkthrough">“Dialog Examples with Code Walkthrough”</a>.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Identifying Slots"><div class="sect2" id="identifying_slots">
<h2>Identifying Slots</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="slots or entities" id="ch06_term22"/>Once we’ve extracted the intents, we want to move on to extracting entities. Extracting entities is also important for generating correct and appropriate responses to the user’s input. We also saw in our Dialogflow example that extracting entities along with the intents creates a full understanding of the user’s input.</p>
<p>In the example in <a data-type="xref" href="#conversation_about_restaurant_booking_l">Figure 6-20</a>—”I’m looking for a cheaper restaurant”—we want to identify “cheaper” as a price slot and take its value verbatim—i.e., the value of the slot is “cheaper.” If we know ontologies for slot-value pairs, a more normalized form can ultimately be restored, such as “cheaper” -&gt; “cheap.” We have seen similar tasks in <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>, where we learned how to extract entities from sentences. We can take a similar approach (i.e., a sequence labeling approach) here as well to extract these entities.</p>
<p>Previously, in our Dialogflow examples, we saw that slots have to be pre-defined beforehand. But here, we want to build this component on our own using an ML algorithm. Recall the algorithms discussed in the context of NER in <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>. We’ll use similar algorithms for slot detection and labeling. We’ll use an open source sequence labeling library called sklearn-crfsuite [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596242216-marker" href="ch06.xhtml#idm45969596242216">12</a>], which we introduced in <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>, for this task. We’ll discuss details of this experiment in a later section.</p>

<div data-type="tip"><h6>Tip</h6>
<p>We can choose a range of ontologies for annotating entities. Imagine we’re building a travel bot. The choice of entity for the destination can be city or airport. To make it robust, we must detect airports as an entity since one city can have multiple airports. On the other hand, in the case of a restaurant-booking bot, detecting cities as an entity is probably suitable.</p>
</div>

<p>One of the disadvantages of these methods is that they need a lot of labeled data for both intent and entity detection. Also, we need dedicated models for both of the tasks. This can make the system slow during deployment. Getting fine-grained labels for entities is also expensive. These issues limit the scalability of the pipeline for more domains.</p>
<p>Recent research [<a data-type="noteref" href="ch06.xhtml#footnote_6_17">11</a>] on spoken language understanding revealed that joint understanding and tracking is better than individual classification and sequence labeling parts. This joint model is lightweight at deployment as compared to individual models. For joint modeling, we can utilize dialog states, which is “inform(price - cheap)” in our example in <a data-type="xref" href="#conversation_about_restaurant_booking_l">Figure 6-20</a>. We can aim to rank or score each candidate pair jointly with dialog act (in combination, a dialog state) to jointly determine the state. Joint determination is more complex and requires better representation learning techniques, which are beyond the scope of this book. Interested readers can learn more about this at [<a data-type="noteref" href="ch06.xhtml#footnote_6_17">11</a>]. Now that we’ve discussed NLU components, let’s move on to response generation.<a contenteditable="false" data-type="indexterm" data-primary="slots or entities" data-startref="ch06_term22" id="idm45969596233688"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Response Generation"><div class="sect2" id="response_generation">
<h2>Response Generation</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="response generation" id="idm45969596230472"/>Once we identify the slots and intent, the final step is for a dialog system to generate an appropriate response. There are many ways to generate a response: fixed responses, using templates, and automatic generation.</p>
<dl>
<dt>Fixed responses</dt>
<dd>FAQ bots mainly use fixed responses<a contenteditable="false" data-type="indexterm" data-primary="response generation" data-secondary="fixed responses" id="idm45969596227592"/><a contenteditable="false" data-type="indexterm" data-primary="fixed responses" id="idm45969596226216"/>. Based on the intent and values for the slots, a dictionary lookup is made on a pool of responses and retrieves the best response. A simple case would be to discard the slot information and have one response per intent. For more complex retrieval, a ranking mechanism can be built that ranks the pool of responses based on the detected intent and slot-value pairs (or the dialog state).</dd>
<dt>Use of templates</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="templates-based response generation" id="idm45969596223848"/>To make responses dynamic, <a contenteditable="false" data-type="indexterm" data-primary="response generation" data-secondary="templates-based" id="idm45969596222600"/>a templates-based approach is often taken. Templates are very useful when the follow-up response is a clarifying question. Slot values can be used to come up with a follow-up question or a fact-driven answer. For example, “The House serves cheap Thai food” can be constructed using a template as &lt;restaurant name&gt; serves &lt;price-value&gt; &lt;food-value&gt; food. Once we identify slots and their values, we populate this template to finally generate an appropriate response.</dd>
<dt>Automatic generation</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="response generation" data-secondary="automatic" id="idm45969596219880"/><a contenteditable="false" data-type="indexterm" data-primary="automatic response generation" id="idm45969596218504"/>More natural and fluent generation can be learned using a data-driven approach. Upon obtaining the dialog state, a conditional generative model can be built that takes a dialog state as an input and generates the next response for the agent. These models can be graphical models or DL-based language models. Later, we’ll briefly cover end-to-end approaches for dialogs that are similar to automatic generations.</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>While automatic generation is robust, template generation has advantages over it. It might be hard to distinguish between the two, especially when the template variety is high, Template-based responses contain fewer grammatical errors and are easier to train.</p>
</div>
<p>Now that we’ve deep-dived into various components of a dialog system, let’s walk through examples of dialog act classification and slot predictions.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Dialog Examples with Code Walkthrough"><div class="sect2" id="dialog_examples_with_code_walkthrough">
<h2>Dialog Examples with Code Walkthrough</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="examples with code walkthrough" id="ch06_term23"/>Now, we’ll go through instances of various real-world dialog datasets that are publicly available and discuss their usage to model various aspects of a dialog system. Then we’ll use two of those datasets to show how to implement models for two tasks we described for context understanding: dialog act prediction or intent classification and slot identification or entity detection. We’ll explore a couple of models for each task and show via comparisons how these models can be improved gradually. All the models are inspired from the NLU tasks (classification and information extraction) we discussed in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#text_classification">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.xhtml#information_extraction">5</a>.</p>

<section data-type="sect3" class="pagebreak-before" data-pdf-bookmark="Datasets"><div class="sect3" id="datasets">
<h3 class="less_space">Datasets</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="for dialog systems" data-secondary-sortas="dialog systems" id="idm45969596205784"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="datasets for" id="idm45969596204136"/><a data-type="xref" href="#goal_oriented_datasets_from_various_dom">Table 6-2</a> is a brief summary of various datasets that are used for benchmarking algorithms for goal-oriented dialog tasks. As we’re interested in various NLU tasks in dialogs, we present four datasets for goal-oriented dialogs that act as benchmarks for dialog-based NLU tasks.</p>
<table class="border" id="goal_oriented_datasets_from_various_dom">
<caption><span class="label">Table 6-2. </span>Goal-oriented datasets<a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="goal-oriented" id="idm45969596200152"/><a contenteditable="false" data-type="indexterm" data-primary="goal-oriented dialogs" id="idm45969596198648"/> from various domains and their usage</caption>
<thead>
<tr>
<th>Dataset</th>
<th>Domain</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>ATIS<a contenteditable="false" data-type="indexterm" data-primary="ATIS (Airline Travel Information Systems) dataset" id="idm45969596194024"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596192568-marker" href="ch06.xhtml#idm45969596192568">13</a>]</td>
<td>Air Ticket Booking</td>
<td>Benchmark for intent classification and slot filling. This is a single-domain dataset, hence entities and intents are restricted to one domain.</td>
</tr>
<tr>
<td>SNIPS<a contenteditable="false" data-type="indexterm" data-primary="SNIPS dataset" id="idm45969596189208"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596187944-marker" href="ch06.xhtml#idm45969596187944">14</a>]</td>
<td>Multidomain</td>
<td>Benchmark for intent classification and slot filling. This is a multidomain dataset, hence the entities belong to multiple domains. Multiple-domain datasets are challenging to model due their variability.</td>
</tr>
<tr>
<td>DSTC<a contenteditable="false" data-type="indexterm" data-primary="DSTC dataset" id="idm45969596184328"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596183064-marker" href="ch06.xhtml#idm45969596183064">15</a>]</td>
<td>Restaurants</td>
<td>Benchmark for dialog state tracking or joint determination of intent and slots. This is similarly a single-domain dataset, but the entities are expressed more in terms of annotations and contain more metadata.</td>
</tr>
<tr>
<td>MultiWoZ<a contenteditable="false" data-type="indexterm" data-primary="MultiWoZ dataset" id="idm45969596179816"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596178552-marker" href="ch06.xhtml#idm45969596178552">16</a>]</td>
<td>Multidomain</td>
<td>Benchmark for dialog state tracking or joint determination of intent and slots that spans over multiple domains. For the similar reason of variability, modeling this dataset is more challenging than modeling single-domain ones.</td>
</tr>
</tbody>
</table>
<p>In addition to these datasets, several datasets of varying scale (i.e., number of sample conversations) are available [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969596175016-marker" href="ch06.xhtml#idm45969596175016">17</a>] for various other subtasks in a dialog pipeline. Later in this section, we’ll discuss how to gather such a dataset and apply it to a domain-specific scenario. For now, we’ll focus on goal-oriented dialogs since they have direct usage in industry and the state-of-the-art research is well established.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>

<p>Despite the existence of many open source datasets, there are only a few datasets that reflect the naturalness of human conversation. Datasets collected by online annotators like Mechanical Turkers<a contenteditable="false" data-type="indexterm" data-primary="Mechanical Turk" id="idm45969596172232"/> suffer from templatish and forced conversation, which affects the dialog quality. Also, domain-specific dialog datasets are still not available for many domains, such as healthcare, law, etc.</p>
</div>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Dialog act prediction"><div class="sect3" id="dialog_act_prediction">
<h3>Dialog act prediction<a contenteditable="false" data-type="indexterm" data-primary="dialog act prediction" id="ch06_term24"/></h3>
<p>Dialog act classification or intent detection is the task we described in the previous section as a part of the NLU component in a dialog system. This is a classification task, and we’ll follow our classification pipeline from <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a> to solve it.</p>

<section data-type="sect4" data-pdf-bookmark="Loading the dataset"><div class="sect4" id="loading_the_dataset">
<h4>Loading the dataset</h4><p>We’ll use ATIS (Airline Travel Information Systems) for the intent detection task. ATIS is a dataset that’s used heavily for spoken language understanding and performing various NLU tasks. The dataset consists of 4,478 training utterances and 893 test utterances with a total of 21 intents. We’ve chosen 17 intents, which appear in both the train and test set. Hence, our task is a 17-class classification task. An instance of the dataset looks like the following code:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">Query</code> <code class="n">text</code><code class="p">:</code> <code class="n">BOS</code> <code class="n">please</code> <code class="nb">list</code> <code class="n">the</code> <code class="n">flights</code> <code class="kn">from</code> <code class="nn">charlotte</code> <code class="nn">to</code> <code class="nn">long</code> <code class="nn">beach</code> <code class="nn">arriving</code> 
 <code class="n">after</code> <code class="n">lunch</code> <code class="n">time</code> <code class="n">EOS</code>
<code class="n">Intent</code> <code class="n">label</code><code class="p">:</code>  <code class="n">flight</code></pre>
</div></section>

<section data-type="sect4" data-pdf-bookmark="Models"><div class="sect4" id="models">
<h4>Models</h4><p>Since it’s a classification task, we’ll use one of the DL techniques we used in <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a> directly: a CNN model<a contenteditable="false" data-type="indexterm" data-primary="CNNs (convolutional neural networks)" id="idm45969596135880"/><a contenteditable="false" data-type="indexterm" data-primary="convolutional neural networks (CNNs)" id="idm45969596134776"/>. Using CNN is useful here because it captures the n-gram features via its dense representations. N-grams such as “list of flights” is indicative of a “flight” label:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">atis_cnnmodel</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">embedding_layer</code><code class="p">)</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling1D</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling1D</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">GlobalMaxPooling1D</code><code class="p">())</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_classes</code><code class="p">),</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'softmax'</code><code class="p">))</code>
<code class="n">atis_cnnmodel</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'categorical_crossentropy'</code><code class="p">,</code>
              <code class="n">optimizer</code><code class="o">=</code><code class="s1">'rmsprop'</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code> <code class="p">[</code><code class="s1">'acc'</code><code class="p">])</code></pre>
<p>We obtain an accuracy of 72% with the use of a CNN on the test, averaged over all classes. If we use an RNN model, the accuracy shoots up to 96%. We believe that RNN is able to capture the interdependency of words across the input sentence. RNN captures the importance of a word with respect to the context it’s seen before. The elaborate details of these models and the dataset code are given in <em>ch6/CNN_RNN_ATIS_intents.ipynb</em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">atis_rnnmodel</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">atis_rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Embedding</code><code class="p">(</code><code class="n">MAX_NUM_WORDS</code><code class="p">,</code> <code class="mi">128</code><code class="p">))</code>
<code class="n">atis_rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">))</code>
<code class="n">atis_rnnmodel</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_classes</code><code class="p">),</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'sigmoid'</code><code class="p">))</code>
<code class="n">atis_rnnmodel</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'binary_crossentropy'</code><code class="p">,</code>
              <code class="n">optimizer</code><code class="o">=</code><code class="s1">'adam'</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code> <code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code></pre>
<p>As we know, recent transformer pre-trained models (such as BERT<a contenteditable="false" data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" id="idm45969595967720"/>) are more powerful. So, we’ll try to use BERT to improve the obtained performance so far. BERT can capture the context better and has more parameters, so it’s more expressive and models the intricacies of the language. To use BERT, we use a BERT-style input tokenization scheme:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># For data:</code><code>
</code><code class="n">sentence</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2"> [CLS] </code><code class="s2">"</code><code> </code><code class="o">+</code><code> </code><code class="n">query</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2"> [SEP]</code><code class="s2">"</code><code>
</code><code class="n">Tokenizer</code><code> </code><code class="o">=</code><code> </code><code class="n">BertTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'</code><code class="s1">bert-base-uncased</code><code class="s1">'</code><code class="p">,</code><code> </code><code>
</code><code>                                         </code><code class="n">do_lower_case</code><code class="o">=</code><b><code class="bp">True</code></b><code class="p">)</code><code>
</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">tokenize</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code><code>
</code><code>
</code><code class="c1"># For model:</code><code>
</code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">BertForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">bert-base-uncased</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                                                     </code><code class="n">num_labels</code><code class="o">=</code><code class="n">num_classes</code><code class="p">)</code><code>
</code></pre>
<p>Since BERT is pre-trained, the representation of content is much better than any models we train from scratch, such as CNNs or RNNs. We see that BERT achieves 98.8% accuracy, beating both CNN and RNN for the dialog act prediction task. Follow the notebook <em>ch6/BERT_ATIS_intents.ipynb</em> for the complete code for model and data preparation.<a contenteditable="false" data-type="indexterm" data-primary="dialog act prediction" data-startref="ch06_term24" id="idm45969595934120"/></p>
</div></section>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Slot identification"><div class="sect3" id="slot_identification">
<h3>Slot identification</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="slots or entities" id="ch06_term25"/>Slot identification is another task we described in the previous section as a part of the NLU component in a dialog system. We described why we can pose this as a sequence labeling task. We need to find the slot values given the input, and we’ll follow our sequence labelling pipeline from <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a> to solve this task.</p>

<section data-type="sect4" data-pdf-bookmark="Loading the dataset"><div class="sect4" id="idm45969595780872">
<h4>Loading the dataset</h4><p>We’ll use SNIPS for this slot identification task. SNIPS<a contenteditable="false" data-type="indexterm" data-primary="SNIPS dataset" id="idm45969595779672"/> is a dataset curated by Snips<a contenteditable="false" data-type="indexterm" data-primary="Snips platform" id="idm45969595778440"/>, an AI voice platform for connected devices. It contains 16,000 crowdsourced queries and is a popular benchmark for slot identification tasks. We’ll load both training and test examples, and an instance of the dataset looks like the code below:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">Query</code> <code class="n">text</code><code class="p">:</code> <code class="p">[</code><code class="n">Play</code><code class="p">,</code> <code class="n">Magic</code><code class="p">,</code> <code class="n">Sam</code><code class="p">,</code> <code class="n">from</code><code class="p">,</code> <code class="n">the</code><code class="p">,</code> <code class="n">thirties</code><code class="p">]</code>  <code class="c1"># tokenized</code>
<code class="n">Slots</code><code class="p">:</code> <code class="p">[</code><code class="n">O</code><code class="p">,</code> <code class="n">artist</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">artist</code><code class="o">-</code><code class="mi">2</code><code class="p">,</code> <code class="n">O</code><code class="p">,</code> <code class="n">O</code><code class="p">,</code> <code class="n">year</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code></pre>
<p>As we discussed in <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>, we’re using the BIO scheme to annotate the slots. Here, <code>O</code> denotes “other,” and <code>artist-1</code> and <code>artist-2</code> denote the two words for artist name. The same goes for the year.</p>
</div></section>

<section data-type="sect4" data-pdf-bookmark="Models"><div class="sect4" id="idm45969595714712">
<h4>Models</h4><p><a contenteditable="false" data-type="indexterm" data-primary="modeling" data-secondary="slot identification with" id="ch06_term26"/>Since a slot identification task can be viewed as a sequence labeling task, we’ll use one of the popular techniques we used in <a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>: a CRF++ model from the sklearn package. We also use word vectors instead of creating handcrafted features to feed into a CRF. CRFs are a popular sequence labeling technique and are used heavily in information extraction.</p>
<p>We use word features that will be useful for this particular task. We see that the context for each word is important in addition to the meaning of the word itself. So, we use the previous two words and next two words for a given word as features. We also use the word embedding vectors retrieved from GloVe pre-trained embeddings (discussed in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>) as additional features. Features for each word are concatenated across words in an input. This input representation is passed to a CRF model for sequence labeling:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">sent2feats</code><code class="p">(</code><code class="n">sentence</code><code class="p">):</code>
    <code class="n">feats</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">sen_tags</code> <code class="o">=</code> <code class="n">pos_tag</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code> <code class="c1">#This format is specific to this POS tagger!</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">sentence</code><code class="p">)):</code>
        <code class="n">word</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">wordfeats</code> <code class="o">=</code> <code class="p">{}</code>
        <code class="c1">#word features: word, prev 2 words, next 2 words in the sentence.</code>
        <code class="n">wordfeats</code> <code class="p">[</code><code class="s1">'word'</code><code class="p">]</code> <code class="o">=</code> <code class="n">word</code>
        <code class="k">if</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevSecondWord"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"&lt;S&gt;"</code>
        <code class="k">elif</code> <code class="n">i</code><code class="o">==</code><code class="mi">1</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevSecondWord"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"&lt;/S&gt;"</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"prevSecondWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="o">-</code><code class="mi">2</code><code class="p">]</code>
        <code class="c1">#next two words as features</code>
        <code class="k">if</code> <code class="n">i</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code><code class="o">-</code><code class="mi">2</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">]</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextNextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"&lt;/S&gt;"</code>
        <code class="k">elif</code> <code class="n">i</code><code class="o">==</code><code class="nb">len</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code><code class="o">-</code><code class="mi">1</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"&lt;/S&gt;"</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextNextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"&lt;/S&gt;"</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">]</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s2">"nextNextWord"</code><code class="p">]</code> <code class="o">=</code> <code class="n">sentence</code> <code class="p">[</code><code class="n">i</code><code class="o">+</code><code class="mi">2</code><code class="p">]</code>

        <code class="c1">#Adding word vectors</code>
        <code class="n">vector</code> <code class="o">=</code> <code class="n">get_embeddings</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">iv</code><code class="p">,</code><code class="n">value</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">vector</code><code class="p">):</code>
            <code class="n">wordfeats</code> <code class="p">[</code><code class="s1">'v{}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">iv</code><code class="p">)]</code><code class="o">=</code><code class="n">value</code>

        <code class="n">feats</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">wordfeats</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">feats</code>

<code class="c1"># training</code>
<code class="n">crf</code> <code class="o">=</code> <code class="n">CRF</code><code class="p">(</code><code class="n">algorithm</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">c1</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">c2</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">max_iterations</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="c1"># Fit on training data</code>
<code class="n">crf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code></pre>
<p>We obtain an F1 of 85.5 with the use of a CRF++ model. More details can be found in the notebook <em>ch6/CRF_SNIPS_slots.ipynb</em>. Similar to the previous classification task, we’ll try to use BERT to improve the performance obtained so far.  BERT can capture the context better, even in the case of a sequence labeling task. We use all the hidden representations for all the words in the query to predict a label for each. Hence, at the end, we input a sequence of words into the model and obtain a sequence of labels (of the same length as the input), which can be inferred as predicted slots with the words as values:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># For data:</code><code>
</code><code class="n">sentence</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2"> [CLS] </code><code class="s2">"</code><code> </code><code class="o">+</code><code> </code><code class="n">query</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2"> [SEP]</code><code class="s2">"</code><code>
</code><code class="n">Tokenizer</code><code> </code><code class="o">=</code><code> </code><code class="n">BertTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'</code><code class="s1">bert-base-uncased</code><code class="s1">'</code><code class="p">,</code><code> </code><code>
</code><code>                                         </code><code class="n">do_lower_case</code><code class="o">=</code><b><code class="bp">True</code></b><code class="p">)</code><code>
</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">tokenize</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code><code>
</code><code>
</code><code class="c1"># For model:</code><code>
</code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">BertForTokenClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"</code><code class="s2">bert-base-uncased</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                                                  </code><code class="n">num_labels</code><code class="o">=</code><code class="n">num_tags</code><code class="p">)</code><code>
</code></pre>

<p>But, we find that BERT achieves only 73 F1. This could be due to the presence of many named entities in the input that were not well represented by the original BERT parameters. On the other hand, the features we obtained for the CRF were strong enough for this dataset to capture the necessary patterns. This is an interesting example where a simpler model beats BERT. See the complete model details in the notebook <em>ch6/BERT_SNIPS_slots.ipynb</em>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>

<p>As we’ve seen before and here as well, pre-trained models help get better performance over other DL models learned from scratch. There could be exceptions, as pre-trained models are sensitive to the size of the data. Pre-trained models may overfit on smaller datasets, and handcrafted features may generalize well in those cases.</p>
</div>
<p>So far, we’ve learned how to build various NLU components for a goal-oriented dialog using popular datasets. We’ve seen how various DL models perform relatively well in these tasks. With these, we’ll be able to run such custom models in our own dataset and explore various models to pick the best one. We also introduced four datasets that are popular benchmarks for goal-oriented dialog modeling. They can be used for prototyping newer models to verify their performance against state-of-the-art models. Now, we’ll transition to other dialog models that are generally used beyond goal-oriented settings, and we’ll discuss their advantages and disadvantages.<a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="components" data-startref="ch06_term20" id="idm45969595333624"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="examples with code walkthrough" data-startref="ch06_term23" id="idm45969595698024"/><a contenteditable="false" data-type="indexterm" data-primary="slots or entities" data-startref="ch06_term25" id="idm45969595314136"/><a contenteditable="false" data-type="indexterm" data-primary="modeling" data-secondary="slot identification with" data-startref="ch06_term26" id="idm45969595312760"/></p>
</div></section>
</div></section>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Other Dialog Pipelines"><div class="sect1" id="other_dialog_pipelines">
<h1>Other Dialog Pipelines</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="other pipelines" id="ch06_term227"/>So far, we’ve discussed the modular pipeline we introduced in <a data-type="xref" href="#pipeline_for_a_dialog_system">Figure 6-3</a>. But there are many other pipelines that can be used in various scenarios, especially in the case of an open-ended chatbot. The initial pipeline in <a data-type="xref" href="#pipeline_for_a_dialog_system">Figure 6-3</a> sometimes lacks in terms of ease of trainability due to multiple components, as each of them has to be individually trained and they need separate annotated datasets for each component. Besides that, in a modular pipeline, one needs to define the ontology explicitly and it does not capture latent patterns from the data. That is why we will briefly touch upon other existing pipelines that may be promising in future.</p>
<section data-type="sect2" data-pdf-bookmark="End-to-End Approach"><div class="sect2" id="end_to_end_approach">
<h2>End-to-End Approach</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="end-to-end approach" id="idm45969595303464"/>Sequence-to-sequence models (we’ll call it seq2seq)<a contenteditable="false" data-type="indexterm" data-primary="sequence-to-sequence models (seq2seq) " id="idm45969595301960"/><a contenteditable="false" data-type="indexterm" data-primary="modeling" data-secondary="sequence-to-sequence (seq2seq) models" id="idm45969595284440"/> have seen huge acceptance in critical NLP tasks such as neural machine translation, named entity recognition, etc. The seq2seq models generally take a sequence as input and output another sequence. In a translation task, imagine our input sentence is in one language and output is in the language we want to translate it to.</p>
<p>Similar to other tasks, we can build a chatbot using seq2seq models. Imagine that the input of the model is the user utterance: a sequence of words. As the output, it generates another sequence of words, which is the response from the bot. Seq2seq models are end-to-end trainable, so we don’t have to maintain multiple modules, and they are generally LSTM based. Recently, state-of-the-art transformers have been used for seq2seq tasks, so they can also be applied in the case of dialog.</p>
<p>Usually, we use tokenization to create word tokens and create a sequence out of a question. Seq2seq is capable of capturing the inherent order of the token in the sequence—this is important, as it ensures that we capture the right meaning of the question in order to answer it correctly. See <a data-type="xref" href="#example_of_work_done_by_google_on_seq2s">Figure 6-21</a> for some examples from a work by Google [11] on such an end-to-end model. They input the questions to the model, and the model generated the corresponding outputs.</p>
<figure><div id="example_of_work_done_by_google_on_seq2s" class="figure">
<img src="Images/pnlp_0621.png" alt="Example of work done by Google on seq2seq models [_11]" width="1330" height="726"/>
<h6><span class="label">Figure 6-21. </span>Example of work done by Google on seq2seq models [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595278168-marker" href="ch06.xhtml#idm45969595278168">18</a>]</h6>
</div></figure>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Deep Reinforcement Learning for Dialogue Generation"><div class="sect2" id="deep_reinforcement_learning_for_dialogu">
<h2>Deep Reinforcement Learning for Dialogue Generation</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning" data-secondary="for dialogue generation" data-secondary-sortas="dialogue generation" id="ch06_term29"/><a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning" data-secondary="deep" id="ch06_term28"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="deep reinforcement learning for" id="ch06_term27"/><a contenteditable="false" data-type="indexterm" data-primary="deep learning (DL)" data-secondary="for dialogue generation" data-secondary-sortas="dialogue generation" id="ch06_term226"/><a contenteditable="false" data-type="indexterm" data-primary="deep learning (DL)" data-secondary="reinforcement learning" id="ch06_term666"/>If you’re wondering how a machine would generate a diverse set of answers given any kind of question, you’re not alone. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_12-marker" href="ch06.xhtml#footnote_6_12">19</a>] studied the drawbacks of typical seq2seq models and discovered that they often kept generating the generic output, “I don’t know.” These models generated utterances without considering how to respond in order to have a good conversation. Doing so requires futuristic knowledge about the goodness of the conversation, which will ultimately help the user achieve their goal. The concept of goodness is abstract, so it’s typically defined based on the objective of the conversation. For example, with a goal-oriented dialog setup, we have a defined goal to achieve, whereas in a chitchat setup, goodness is defined by how interesting the conversation will be.</p>
<p>Here, we see the combination of two ideas: goal-oriented dialog and seq2seq-based generation. Reinforcement learning can help us here. Each time the machine utters a response is nothing but it performing a specific action. A set of such actions can be made in a way that ensures the goal is finally achieved via the conversation. In reinforcement learning based on exploration and exploitation, the machine tries to learn to generate the best response based on a futuristic reward defined by the user, which is directly related to how likely the current response is to achieve the final goal. <a data-type="xref" href="#comparison_of_deep_reinforcement_learni">Figure 6-22</a> shows how the reinforcement learning–based model performed well compared to the typical seq2seq-based model. On the right-hand side, you can see that the reinforcement learning–based model generated a more diverse response instead of collapsing into a generic default response.</p>
<figure><div id="comparison_of_deep_reinforcement_learni" class="figure">
<img src="Images/pnlp_0622.png" alt="Comparison of deep reinforcement learning and a seq2seq model [_12]" width="1212" height="568"/>
<h6><span class="label">Figure 6-22. </span>Comparison of deep reinforcement learning and a seq2seq model [<a data-type="noteref" href="ch06.xhtml#footnote_6_12">19</a>]</h6>
</div></figure>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Human-in-the-Loop"><div class="sect2" id="human_in_the_loop">
<h2>Human-in-the-Loop</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="human-in-the-loop" id="ch06_term31"/><a contenteditable="false" data-type="indexterm" data-primary="human-in-the-loop" id="ch06_term30"/>So far, we’ve talked about machines generating answers in response to questions asked, without human intervention. The machine<a contenteditable="false" data-type="indexterm" data-primary="deep learning (DL)" data-secondary="for dialogue generation" data-secondary-sortas="dialogue generation" data-startref="ch06_term226" id="idm45969595253448"/><a contenteditable="false" data-type="indexterm" data-primary="deep learning (DL)" data-secondary="reinforcement learning" data-startref="ch06_term666" id="idm45969595251528"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="deep reinforcement learning for" data-startref="ch06_term27" id="idm45969595249880"/><a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning" data-secondary="deep" data-startref="ch06_term28" id="idm45969595248264"/><a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning" data-secondary="for dialogue generation" data-secondary-sortas="dialogue generation" data-startref="ch06_term29" id="idm45969595246616"/> may improve its performance if humans intervene in its learning process and reward or penalize based on the correct or incorrect response. These rewards or penalties act as feedback for the model.</p>
<p>Answering a natural language query typically follows three steps: understand the query, perform an action, and respond to utterances. While doing this, the machine might need human intervention in various scenarios—for example, if the question is out of the chatbot’s scope, if the action it took was not correct, or if the understanding of the query was wrong. Typically, when humans intervene in a machine’s learning process, it’s termed as <em>human-in-the-loop</em>.</p>
<p>In the context of chatbots, Facebook has performed an exercise [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_13-marker" href="ch06.xhtml#footnote_6_13">20</a>] of using humans to inject partial rewards when the bot is learning in a reinforcement learning setup. As we discussed in the previous subsection, the ultimate goal of the bot is to fulfill the user’s needs. But with human-in-the-loop, while exploring various actions, the bot receives additional input from a human “teacher,” which clearly improves the quality of the response, as shown in <a data-type="xref" href="#humans_providing_additional_signals_dur">Figure 6-23</a>.</p>
<figure><div id="humans_providing_additional_signals_dur" class="figure">
<img src="Images/pnlp_0623.png" alt="Humans providing additional signals during dialog learning [_13]" width="1059" height="225"/>
<h6><span class="label">Figure 6-23. </span>Humans providing additional signals during dialog learning [<a data-type="noteref" href="ch06.xhtml#footnote_6_13">20</a>]</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>

<p>Human-in-the-loop is ultimately a more practical system to deploy than a completely automated dialog generation system. End-to-end models are efficient to train, but they may not be reliable in producing factually correct outputs. Hence, a hybrid system with the combination of end-to-end dialog generation framework and with human resources will be more reliable and robust.</p>
</div>
<p>We’ve discussed various techniques beyond goal-oriented dialog. Many of these methods are built by industry and are usable in practical settings. These end-to-end models can grow large in terms of parameters (via the use of new transformer architecture) and can therefore become infeasible to deploy in small-scale applications. But we also saw here that even LSTM models can generate reasonable outputs. Human-in-the-loop is also a feasible technique that can be adopted regardless of the computing power available.<a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="other pipelines" data-startref="ch06_term227" id="idm45969595234488"/><a contenteditable="false" data-type="indexterm" data-primary="human-in-the-loop" data-startref="ch06_term30" id="idm45969595232840"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="human-in-the-loop" data-startref="ch06_term31" id="idm45969595231464"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Rasa NLU"><div class="sect1" id="rasa_nlu">
<h1>Rasa NLU</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="natural language understanding (NLU)" data-secondary="Rasa" id="ch06_term33"/><a contenteditable="false" data-type="indexterm" data-primary="Rasa NLU" id="ch06_term32"/>So far, we’ve discussed how to build two main components of a dialog system: dialog act prediction and slot filling. Beyond these two components, there are several integration steps to tie them into a complete pipeline for dialog. Also, we can build wrapping logic around these components and create a comprehensive dialog experience for users.</p>
<p>Building such a complete dialog system requires significant engineering work. But the good news is there are frameworks available that allow us to build custom NLP models as various components of the system and that provide overhead engineering tools and supports to build a functioning bot. One example of such a framework is Rasa. Rasa offers a suite of features [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_14-marker" href="ch06.xhtml#footnote_6_14">21</a>] that can be essential in building a chatbot for industrial use. <a data-type="xref" href="#rasa_chatbot_interface_and_interactive">Figure 6-24</a> shows the Rasa chatbot interface along with its interactive learning framework, which we’ll discuss later.</p>
<figure><div id="rasa_chatbot_interface_and_interactive" class="figure">
<img src="Images/pnlp_0624.png" alt="Rasa chatbot interface and interactive learning framework [_14]" width="1440" height="727"/>
<h6><span class="label">Figure 6-24. </span>Rasa chatbot interface and interactive learning framework [<a data-type="noteref" href="ch06.xhtml#footnote_6_14">21</a>]</h6>
</div></figure>
<p>We’ll briefly touch on Rasa’s available features and discuss how they can be used to improve the user’s experience with a chatbot:<a contenteditable="false" data-type="indexterm" data-primary="Rasa" id="ch06_term34"/></p>
<dl>
<dt>Context-based conversations<a contenteditable="false" data-type="indexterm" data-primary="conversations" data-secondary="context-based" id="idm45969595215256"/><a contenteditable="false" data-type="indexterm" data-primary="context-based conversations" id="idm45969595213848"/></dt>
<dd>The Rasa framework allows users to capture and utilize the conversation context or dialog state. Internally, Rasa performs NLU and captures required slots and their values, which can be utilized in response generation.</dd>
<dt>Interactive learning<a contenteditable="false" data-type="indexterm" data-primary="learning" data-secondary="interactive" id="idm45969595211656"/><a contenteditable="false" data-type="indexterm" data-primary="interactive learning" id="idm45969595210248"/></dt>
<dd>Rasa offers an interactive interface that can be used for two purposes. One is to create more training data for the internal models by chatting with the bot. The second is to provide feedback when the models make mistakes. This feedback can be used as negative samples for the model to improve performance in challenging cases.</dd>
<dt>Data annotation</dt>
<dd><p><a contenteditable="false" data-type="indexterm" data-primary="data annotation" id="ch06_term35"/>Rasa presents a highly interactive and easy-to-use interface to annotate more data to improve the model training. Data annotation can be done from scratch or modified from examples where labels are already predicted by the existing <span class="keep-together">models.</span> See <a data-type="xref" href="#data_annotation_and_api_integrations">Figure 6-25</a> for an example of the data annotation step in Rasa. Wrapper frameworks are built on Rasa NLU, which eases the data annotation process to generate large-scale dialog datasets. Once such framework is Chatette [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595168168-marker" href="ch06.xhtml#idm45969595168168">22</a>], which is a tool that accepts templates and then spawns dialog instances using those templates at scale.</p>
<figure class="width-75"><div id="data_annotation_and_api_integrations" class="figure">
<img src="Images/pnlp_0625.png" alt="Data annotation and API integrations" width="1440" height="1937"/>
<h6><span class="label">Figure 6-25. </span>Data<a contenteditable="false" data-type="indexterm" data-primary="data annotation" data-startref="ch06_term35" id="idm45969595164936"/> annotation and API integrations</h6>
</div></figure></dd>
<dt>API integration<a contenteditable="false" data-type="indexterm" data-primary="APIs" data-secondary="integration of" id="idm45969595163080"/></dt>
<dd>Finally, the dialog service can also be integrated with other APIs as well as chat platforms like Slack, Facebook, Google Home, and Amazon Alexa. The next section includes a case study where we’ll produce recipe recommendations via conversations and integrate a faceted search API endpoint into the bot to facilitate the recommendation process.</dd>
<dt>Customize your models in Rasa<a contenteditable="false" data-type="indexterm" data-primary="modeling" data-secondary="customization" id="idm45969595160424"/></dt>
<dd><p>Apart from the framework, Rasa also allows us to customize our models by choosing from a pool of models. For example, for intent/dialog act detection, we can choose “sklearn classifier” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595158264-marker" href="ch06.xhtml#idm45969595158264">23</a>] or “mitie classifier” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595156712-marker" href="ch06.xhtml#idm45969595156712">24</a>], or we can write our own classifier and add that to the building pipeline for Rasa to use it. Various options for embeddings such as spaCy and Rasa’s own are available with the framework.</p>
<p>We can also harness the power of transformer models as we see performance improvement while building our individual components. Rasa provides BERT (and various distilled versions to improve on latency) for both classification and sequence labeling tasks [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595154376-marker" href="ch06.xhtml#idm45969595154376">25</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969595152520-marker" href="ch06.xhtml#idm45969595152520">26</a>]. Overall, this makes Rasa a very powerful tool for building a dialog system from scratch.<a contenteditable="false" data-type="indexterm" data-primary="Rasa" data-startref="ch06_term34" id="idm45969595150856"/></p></dd>
</dl>

<div data-type="tip"><h6>Tip</h6>
<p>Rasa enables us to build our chatbot in a modular way. For example, we can start with existing pre-trained models and later use custom models built on our specific datasets as needed. Similarly, we can start default API integrations and conversation channels and modify them when needed.</p>
</div>

<p>Now, let’s go over a complete case study with a real scenario discussing what steps are necessary to create a conversational system from scratch in an industrial setup, including data setup, model building, and deployment.<a contenteditable="false" data-type="indexterm" data-primary="Rasa NLU" data-startref="ch06_term32" id="idm45969595147608"/><a contenteditable="false" data-type="indexterm" data-primary="natural language understanding (NLU)" data-secondary="Rasa" data-startref="ch06_term33" id="idm45969595146232"/></p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="A Case Study: Recipe Recommendations"><div class="sect1" id="a_case_study_recipe_recommendations">
<h1>A Case Study: Recipe Recommendations</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="case study" id="ch06_term37"/>Cooks often look for specific recipes tailored to their culinary and dietary preferences. A conversational interface where cooks can find their recipe of choice by fleshing out their preferences via a conversation with the agent would be a good user experience. In this case study, we’ll discuss all the components we’ve covered in this chapter along with the frameworks required to build them. We’ll see the evolving need for data and modeling complexity of the business problem and address them via various tools we’ve learned about in this chapter.</p>
<p>Imagine we’re part of a recipe and food aggregator site. We’ve been tasked with building a chatbot. Users can talk about the kind of food they’re craving or want to cook. This is an uncharted problem, so how will we go about building this? <a data-type="xref" href="#example_of_a_recipe_suggestion_site_all">Figure 6-26</a> shows some example suggestions of recipes for various user preferences.</p>

<p>We need to convert this business problem into a technical problem with objectives and constraints. As a user will interact with the system, our goal is to create a fully defined query that can fetch a suitable recipe. The recipe can come from an API endpoint or a generative model. This query is made of a set of attributes that define the dish, such as ingredients, cuisine, calorie level, cooking time, etc. We also know that users can reveal their preferences through turns in a conversation, so we need to track their preferences and update the internal dialog state as the conversation proceeds.</p>

<figure><div id="example_of_a_recipe_suggestion_site_all" class="figure">
<img src="Images/pnlp_0626.png" alt="Example of a recipe-suggestion site: Allrecipes.com" width="960" height="996"/>
<h6><span class="label">Figure 6-26. </span>Example of a recipe-suggestion site: <a href="http://Allrecipes.com">Allrecipes.com</a></h6>
</div></figure>

<section data-type="sect2" data-pdf-bookmark="Utilizing Existing Frameworks"><div class="sect2" id="utilizing_existing_frameworks">
<h2>Utilizing Existing Frameworks</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="existing frameworks" id="ch06_term38"/>We’ll start with Dialogflow, the cloud API we described earlier in the chapter since it’s easy to build. Before we start, we need to define entities like we did before, such as ingredients, cuisine, calorie level, cooking time. We can build an ontology for the cooking domain and identify the number of slots we’d like our chatbot to support.</p>
<p>Initially, it will be good to keep an exhaustive list of these entities. Here are some examples of training instances that capture nuances in this early phase of bot <span class="keep-together">
building:</span></p>
<ul>
<li><p>I want a <u>low calorie</u> <u>dessert</u> that is vegan.</p></li>
<li><p>I have <u>peas</u>, <u>carrots</u>, and <u>chicken</u> in my kitchen. What can I make with it in <u>30 minutes</u>?</p></li>
</ul>
<p>Dialogflow<a contenteditable="false" data-type="indexterm" data-primary="Dialogflow (Google)" id="idm45969595124888"/><a contenteditable="false" data-type="indexterm" data-primary="Google Cloud" data-secondary="Dialogflow" id="idm45969595123752"/> is capable of handling the user’s preference and identifying the slots and values necessary to look for a correct recipe. Also, due to the conversational nature of the user’s interaction, the bot will maintain its dialog state or context to fully understand the user’s input. We’ll assume a database of recipes has been pre-defined and prefilled. Now, once the entities are captured via the bot, we need to feed them into an API endpoint. This endpoint will do a faceted search on the database and retrieve the best-ranked recipes.</p>
<p>As we collect more data, Dialogflow will slowly become better. But due to its lack of custom models, it can’t solve more complex conversations related to this task. Some examples where a Dialogflow-based bot will eventually fail are:</p>
<ul>
<li><p>I have a <u>chicken</u> with me, what can I cook with it besides <u>chicken lasagna</u>?</p></li>
<li><p>Give me a recipe for a <u>chocolate dessert</u> that can be made in just <u>10 mins</u> instead of the regular <u>half an hour</u>.</p></li>
</ul>
<p>These examples show a presence of more than one value for one slot, and only one of them is correct—for example, “10 mins” is correct, while “half an hour” isn’t. Matching-based methods in Dialogflow will fail in such cases. That’s why we need to build custom models so that these examples can be added as adversarial examples in their training pipeline. In a Rasa pipeline with custom models, we can add such adversarial examples in order for the model to learn to identify correct slots and their values. It’s also possible to generate such adversarial examples from the data we’ve gathered using data augmentation techniques and including them though the data annotation techniques of the Rasa framework, as shown in <a data-type="xref" href="#how_rasa_can_facilitate_complex_annotat">Figure 6-27</a>.</p>

<p>With this updated training data, new custom models will be able to pick the correct values to fully describe the user’s ask for the recipe. Once slots and values are captured, the rest of the process will be similar to how it was<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="existing frameworks" data-startref="ch06_term38" id="idm45969595114296"/> before (i.e., an API endpoint can use this information to query an appropriate recipe).</p>

<figure class="width-100"><div id="how_rasa_can_facilitate_complex_annotat" class="figure">
<img src="Images/pnlp_0627.png" alt="How Rasa can facilitate complex annotations" width="1442" height="901"/>
<h6><span class="label">Figure 6-27. </span>How Rasa<a contenteditable="false" data-type="indexterm" data-primary="Rasa" id="idm45969595110136"/> can facilitate complex annotations</h6>
</div></figure>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Open-Ended Generative Chatbots"><div class="sect2" id="open_ended_generative_chatbots">
<h2>Open-Ended Generative Chatbots</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="generative chatbots, open-ended" id="ch06_term41"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="open-ended" id="ch06_term40"/><a contenteditable="false" data-type="indexterm" data-primary="open-ended generative chatbots" id="ch06_term39"/>Our solution is good enough to be deployed on a real website where millions of users interact regularly. Now we can focus on solving more challenging tasks with the objective of improving the user experience even more. So far, we’ve been providing users with specific recipes that are stored in a datastore beforehand. What if we want to make the chatbot more open ended by generating recipes instead of searching for them from a pre-existing pool? The advantage of such systems is their ability to handle unknown attribute values and customize recipes to fit the personalized tastes of the users.</p>
<div data-type="tip"><h6>Tip</h6>

<p>Open-ended chatbots are generally harder to evaluate because many variants of a response can be correct given the context. Human evaluation seems to be most efficient, but it’s irreproducible and therefore harder to compare to other systems. A mix of automatic and human evaluation is the right way to evaluate generative dialog systems.</p>
</div>
<p>Here, we can utilize powerful seq2seq generative models that can condition their generation on the various desired attributes the user has described for the recipe preference. Researchers (including one of the authors) have shown [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_6_21-marker" href="ch06.xhtml#footnote_6_21">27</a>] that these seq2seq models are capable of generating personalized recipes based on preferences and <span class="keep-together">previous</span> recipe interactions. These models are capable of incorporating nuances and potentially generating a novel recipe that’s valid but unique to the user’s culinary taste. <a data-type="xref" href="#recipes_generated_personalized_to_usera">Figure 6-28</a> shows such an example of a newly generated recipe incorporating a user’s preference. The user’s preference can be just a list of recipes that they’ve interacted with before. For example, in this figure, the user had previously interacted with mojito, martini, and Bloody Mary. The personalized model added an extra garnishing step (highlighted in gray) to make it more personalized.</p>
<figure><div id="recipes_generated_personalized_to_usera" class="figure">
<img src="Images/pnlp_0628.png" alt="Recipes generated personalized to user’s preferences [_21]" width="1440" height="568"/>
<h6><span class="label">Figure 6-28. </span>Recipes generated personalized to user’s preferences [<a data-type="noteref" href="ch06.xhtml#footnote_6_21">27</a>]</h6>
</div></figure>
<p>Merging such generative models with other dialog components can really boost the user experience. While we’ve discussed one specific recipe-recommendation problem, similar approaches can be taken in developing similar applications. We’ve discussed necessary tools and models that can be used together to build a bot according to the business problem at hand. We started with a very simple approach using Dialogflow and gradually added more complexity to tackle dialog nuances in the way users may express their queries and choices. Finally, we went the extra mile to build an end-to-end personalized chatbot.<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="case study" data-startref="ch06_term37" id="idm45969595091736"/><a contenteditable="false" data-type="indexterm" data-primary="open-ended generative chatbots" data-startref="ch06_term39" id="idm45969595090088"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="open-ended" data-startref="ch06_term40" id="idm45969595088696"/><a contenteditable="false" data-type="indexterm" data-primary="generative chatbots, open-ended" data-startref="ch06_term41" id="idm45969595087048"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Wrapping Up"><div class="sect1" id="wrapping_up-id00071">
<h1>Wrapping Up</h1>
<p>In this chapter, we discussed chatbots and their applicability in various domains. We went through a pipeline approach and delved deep into its various components. We talked about a complete flow-based bot with a cloud-based API, then implemented ML components of NLU modules. Finally, we analyzed a business problem and provided some pathways to approach it incrementally.</p>
<p>But as far as dialog systems and chatbots are concerned, there are many challenges that are still unsolved. Hence, this is a very active area of research in the NLP community. In addition to academic research, industrial research groups are also looking for scalable solutions to existing approaches so that chatbots can be built reliably and deployed to users. Still today, many industrial chatbots fail to be robust and suffer in the issue of natural language understanding and natural language generation. We mention these challenges in order to provide a broader picture of the domain of <span class="keep-together">chatbots.</span></p>
<p><a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="for dialog systems" data-secondary-sortas="dialog systems" id="idm45969595081496"/><a contenteditable="false" data-type="indexterm" data-primary="dialog systems" data-secondary="datasets for" id="idm45969595079848"/>The major problem right now in building dialog systems is a lack of datasets that reflect natural conversations. Many times, personal data can’t be collected for privacy reasons. Other times, a lack of such conversational interfaces hinders data collection capability. Also, existing datasets, especially ones that claim to be real-world datasets, lack naturalness. These datasets are created mainly by online annotators, and most of the time, they sound scripted due to the nature of objective data collection. This problem is very different from other NLP tasks. For instance, annotating a correct class to a datapoint in a classification task or pointing out the relevant information in an information extraction task is more objective and easy to get than labels via crowd-sourced online annotators. In the case of dialog, many times the task is subjective hence the data collection process becomes complex.</p>
<p>Furthermore, the current generative models are not capable enough to generate factually correct statements, which becomes a critical problem in the case of chatbots. In the short span of a conversation, factually incorrect generation may hinder the quality of the conversation. Hence, future research and industrial efforts should be toward both gathering better representative datasets and improving both natural language understanding and generation models that can be used in a chatbot pipeline.</p>
<p>In summary, we discussed the foundations of dialog systems, starting with an overall pipeline, and developed a dialog system using Dialogflow, a cloud API; dove deep into building custom models for understanding dialog context; and finally, used all of them to solve a case study. While we anticipate that the area will continue evolving and improving, this chapter will be a good start for you to adapt to the new solutions that keep coming. Now let’s turn to a few other common NLP problem scenarios in the next chapter.<a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-startref="ch06_term1" id="idm45969595075528"/></p>
</div></section>
<div data-type="footnotes"><h5>Footnotes</h5></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596542584">[<a href="ch06.xhtml#idm45969596542584-marker">1</a>] <a href="https://parl.ai">ParlAI</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596534264">[<a href="ch06.xhtml#idm45969596534264-marker">2</a>] Wallace, Michal and George Dunlop. <a href="https://oreil.ly/O3bz8">Eliza, The Rogerian Therapist</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596498728">[<a href="ch06.xhtml#idm45969596498728-marker">3</a>] Amazon. <a href="https://oreil.ly/fkjpx">“Build a Machine Learning Model”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596448984">[<a href="ch06.xhtml#idm45969596448984-marker">4</a>] Miller, Alexander H., Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine Bordes, Devi Parikh, and Jason Weston. “ParlAI: A Dialog Research Software Platform.” <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em> (2017): 79–84.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_5">[<a href="ch06.xhtml#footnote_6_5-marker">5</a>] Pratap, Vineel, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, and Ronan Collobert. <a href="https://oreil.ly/hCiIU">“wav2letter++: The Fastest Open-source Speech Recognition System”</a>, (2018).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596411976">[<a href="ch06.xhtml#idm45969596411976-marker">6</a>] Google Cloud. <a href="https://oreil.ly/7w1pL">“Cloud Text-to-Speech”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596410520">[<a href="ch06.xhtml#idm45969596410520-marker">7</a>] van den Oord, Aäron and Dieleman, Sander. <a href="https://oreil.ly/dvApO">“WaveNet: A Generative Model for Raw Audio”</a>, <em>DeepMind (blog)</em>, September 8, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596373832">[<a href="ch06.xhtml#idm45969596373832-marker">8</a>] <a href="https://dialogflow.com">Dialogflow</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596360856">[<a href="ch06.xhtml#idm45969596360856-marker">9</a>] <a href="https://oreil.ly/V8eGg">Dialogflow login page</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596359448">[<a href="ch06.xhtml#idm45969596359448-marker">10</a>] Google Cloud. <a href="https://oreil.ly/piEK0">Dialogflow V2 API</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_17">[<a href="ch06.xhtml#footnote_6_17-marker">11</a>] Mrkšić, Nikola, Diarmuid O. Séaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. “Neural Belief Tracker: Data-Driven Dialogue State Tracking.” <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</em> 1 (2016): 1777–1788.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596242216">[<a href="ch06.xhtml#idm45969596242216-marker">12</a>] Team HG-Memex. <a href="https://oreil.ly/zbPGo">“sklearn-crfsuite: scikit-learn inspired API for CRFsuite”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596192568">[<a href="ch06.xhtml#idm45969596192568-marker">13</a>] Hemphill, Charles T., John J. Godfrey, and George R. Doddington. “The ATIS Spoken Language Systems Pilot Corpus.” <em>Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania</em>, June 24–27, 1990.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596187944">[<a href="ch06.xhtml#idm45969596187944-marker">14</a>] Coucke, Alice, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro et al. <a href="https://oreil.ly/_c5np">“Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces”</a>, (2018).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596183064">[<a href="ch06.xhtml#idm45969596183064-marker">15</a>] Williams, Jason, Antoine Raux, and Matthew Henderson. “The Dialog State Tracking Challenge Series: A Review.” <em>Dialogue &amp; Discourse</em> 7.3 (2016): 4–33.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596178552">[<a href="ch06.xhtml#idm45969596178552-marker">16</a>] Budzianowski, Paweł, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. <a href="https://oreil.ly/V9zyy">“MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”</a>, (2018).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969596175016">[<a href="ch06.xhtml#idm45969596175016-marker">17</a>] Serban, Iulian Vlad, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle Pineau. <a href="https://oreil.ly/nLrql">“A Survey of Available Corpora for Building Data-Driven Dialogue Systems”</a>, (2015).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595278168">[<a href="ch06.xhtml#idm45969595278168-marker">18</a>] Vinyals, Oriol and Quoc Le. <a href="https://oreil.ly/Gq8Sh">“A Neural Conversational Model”</a>, (2015).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_12">[<a href="ch06.xhtml#footnote_6_12-marker">19</a>] Li, Jiwei, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. <a href="https://oreil.ly/mfd3Q">“Deep Reinforcement Learning for Dialogue Generation”</a>, (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_13">[<a href="ch06.xhtml#footnote_6_13-marker">20</a>] Weston, Jason E. “Dialog-Based Language Learning.” <em>Proceedings of the 30th International Conference on Neural Information Processing Systems</em> (2016): 829–837.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_14">[<a href="ch06.xhtml#footnote_6_14-marker">21</a>] <a href="https://oreil.ly/aJSyJ">Rasa</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595168168">[<a href="ch06.xhtml#idm45969595168168-marker">22</a>] SimGus. <a href="https://oreil.ly/QQ64f">Chatette: A powerful dataset generator for Rasa NLU, inspired by Chatito</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595158264">[<a href="ch06.xhtml#idm45969595158264-marker">23</a>] scikit-learn. <a href="https://oreil.ly/WMulf">“Classifier comparison</a>.” Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595156712">[<a href="ch06.xhtml#idm45969595156712-marker">24</a>] MIT-NLP. <a href="https://oreil.ly/o-3Fr">MITIE: library and tools for information extraction</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595154376">[<a href="ch06.xhtml#idm45969595154376-marker">25</a>] Sucik, Sam. <a href="https://oreil.ly/Iw_5B">“Compressing BERT for faster prediction”</a>. <em>Rasa (blog)</em>, August 8, 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969595152520">[<a href="ch06.xhtml#idm45969595152520-marker">26</a>] Ganesh, Prakhar, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. <a href="https://oreil.ly/VSQvc">“Compressing Large-Scale Transformer-Based Models: A Case Study on BERT”</a>, (2020).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_6_21">[<a href="ch06.xhtml#footnote_6_21-marker">27</a>] Majumder, Bodhisattwa Prasad, Shuyang Li, Jianmo Ni, and Julian McAuley. <a href="https://oreil.ly/OVyBz">“Generating Personalized Recipes from Historical User Preferences”</a>, (2019).</p></div></div></section></div>



  </body>
</html>