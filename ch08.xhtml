<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />
<style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1&gt;p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1&gt;p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]&gt;div&gt;h1,#sbo-rt-content section[data-type="preface"]&gt;div&gt;h1,#sbo-rt-content section[data-type="appendix"]&gt;div&gt;h1,#sbo-rt-content section[data-type="glossary"]&gt;div&gt;h1,#sbo-rt-content section[data-type="bibliography"]&gt;div&gt;h1,#sbo-rt-content section[data-type="index"]&gt;div&gt;h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000;padding-top:.25em !important;margin-top:0 !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]&gt;div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dl{margin-bottom:1.5em !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important;line-height:1.25rem;font-style:italic}#sbo-rt-content dd{margin:10px 0 .25em 1.5em !important;line-height:1.65em !important}#sbo-rt-content dd p{padding:0 !important;margin:0 0 10px !important}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul&gt;li,#sbo-rt-content ol ul,#sbo-rt-content ol ul&gt;li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul&gt;li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul&gt;li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul&gt;li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol&gt;li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol&gt;li,#sbo-rt-content ul ol,#sbo-rt-content ul ol&gt;li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol&gt;li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol&gt;li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol&gt;li&gt;ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol&gt;li&gt;ol&gt;li&gt;ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content table li{margin:10px 0 0 .25em !important}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:top;font-size:80%}#sbo-rt-content th{vertical-align:bottom}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller;word-break:break-all}#sbo-rt-content table.border tbody&gt;tr:last-child&gt;td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:2em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content pre.break-code,#sbo-rt-content code.break-code,#sbo-rt-content .break-code pre,#sbo-rt-content .break-code code{word-break:break-all}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10,#sbo-rt-content figure.width-10 img{width:10% !important}#sbo-rt-content .width-20,#sbo-rt-content figure.width-20 img{width:20% !important}#sbo-rt-content .width-30,#sbo-rt-content figure.width-30 img{width:30% !important}#sbo-rt-content .width-40,#sbo-rt-content figure.width-40 img{width:40% !important}#sbo-rt-content .width-50,#sbo-rt-content figure.width-50 img{width:50% !important}#sbo-rt-content .width-60,#sbo-rt-content figure.width-60 img{width:60% !important}#sbo-rt-content .width-70,#sbo-rt-content figure.width-70 img{width:70% !important}#sbo-rt-content .width-80,#sbo-rt-content figure.width-80 img{width:80% !important}#sbo-rt-content .width-90,#sbo-rt-content figure.width-90 img{width:90% !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100% !important}#sbo-rt-content .sc{text-transform:none !important}#sbo-rt-content .right{float:none !important}#sbo-rt-content a.totri-footnote{padding:0 !important}#sbo-rt-content figure.width-10,#sbo-rt-content figure.width-20,#sbo-rt-content figure.width-30,#sbo-rt-content figure.width-40,#sbo-rt-content figure.width-50,#sbo-rt-content figure.width-60,#sbo-rt-content figure.width-70,#sbo-rt-content figure.width-80,#sbo-rt-content figure.width-90{width:auto !important}#sbo-rt-content p img,#sbo-rt-content pre img{width:1.25em;line-height:1em;margin:0 .15em -.2em}#sbo-rt-content figure.no-frame div.border-box{border:none}#sbo-rt-content .right{text-align:right !important}
    </style>
<style type="text/css" id="font-styles">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }</style>
<style type="text/css" id="font-family">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }</style>
<style type="text/css" id="column-width">#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }</style>

<style type="text/css">body{margin:1em;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}body{background-color:transparent!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Social Media"><div class="chapter" id="social_media">
<h1><span class="label">Chapter 8. </span>Social Media</h1>
<blockquote class="right">
<p class="right"><em>In today’s world, we don’t need to speak English</em><br/> <em>because we have social media.</em></p>
<p data-type="attribution" style="text-align:right"><em>Vir Das</em></p>
</blockquote>
<p><a contenteditable="false" data-type="indexterm" data-primary="social media" id="ch08_term1"/>Social media platforms (Twitter<a contenteditable="false" data-type="indexterm" data-primary="Twitter" id="idm45969589910168"/>, Facebook<a contenteditable="false" data-type="indexterm" data-primary="Facebook" id="idm45969589908936"/>, Instagram<a contenteditable="false" data-type="indexterm" data-primary="Instagram" id="idm45969589907672"/>, WhatsApp<a contenteditable="false" data-type="indexterm" data-primary="WhatsApp" id="idm45969589906408"/>, etc.) have revolutionized<a contenteditable="false" data-type="indexterm" data-primary="Das, Vir" id="idm45969589905144"/> the way we communicate with individuals, groups, communities, corporations, government agencies, media houses, etc. This, in turn, has changed established norms and etiquette and the day-to-day practices of how businesses and government agencies carry out things like sales, marketing, public relations, and customer support. Given the huge volume and variety of data generated daily on social media platforms, there’s a huge body of work focused on building intelligent systems to understand communication and interaction on these platforms. Since a large part of this communication happens in text, NLP has a fundamental role to play in building such systems. In this chapter, we’ll focus on how NLP is useful for analyzing social media data and how to build such systems.</p>
<p>To give an idea of the volume of data that’s generated on these platforms [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589902824-marker" href="ch08.xhtml#idm45969589902824">1</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589901320-marker" href="ch08.xhtml#idm45969589901320">2</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589899848-marker" href="ch08.xhtml#idm45969589899848">3</a>], consider the following numbers:</p>

<ul class="simplelist">
<li><p><em>Volume:</em> 152 million monthly active users on Twitter; for Facebook, it’s 2.5 billion</p></li>
<li><p><em>Velocity:</em> 6,000 tweets/second; 57,000 Facebook posts/second</p></li>
<li><p><em>Variety:</em> Topic, language, style, script</p></li>
</ul>

<p>The infographic shown in <a data-type="xref" href="#data_generated_in_one_minute_on_various">Figure 8-1</a> presents how much data is generated per minute across different platforms [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589893784-marker" href="ch08.xhtml#idm45969589893784">4</a>].</p>
<figure><div id="data_generated_in_one_minute_on_various" class="figure">
<img src="Images/pnlp_0801.png" alt="Data generated in one minute on various social platforms" width="1361" height="1447"/>
<h6><span class="label">Figure 8-1. </span><a contenteditable="false" data-type="indexterm" data-primary="social media" data-secondary="data generated in" id="idm45969589890664"/>Data generated in one minute on various social platforms</h6>
</div></figure>
<p>Given these numbers, social platforms have to be the largest generators of unstructured natural language data. It’s not possible to manually analyze even a fraction of this data. Since a lot of content is text, the only way forward is to design NLP-based intelligent systems that can work with social data and bring out insights. This is the focus of this chapter. We’ll look at some important business applications, such as topic detection, sentiment analysis, customer support, and fake news detection, to name a few. A large part of this chapter will be about how the text from social platforms is different from other sources of data and how we can design subsystems to handle these differences. Let’s begin by looking at some of the important applications that use NLP to extract insights from social media data.</p>
<section data-type="sect1" data-pdf-bookmark="Applications"><div class="sect1" id="applications-id00002">
<h1>Applications</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="applications that use" id="ch08_term2"/>There’s a wide variety of NLP applications that use data from social platforms, including sentiment detection, customer support, and opinion mining, to name a few. This section will briefly discuss some of the popular ones to give an idea of where we could begin applying these applications for our own needs:</p>
<dl>
<dt>Trending topic detection<a contenteditable="false" data-type="indexterm" data-primary="trending topic detection" id="idm45969589882744"/><a contenteditable="false" data-type="indexterm" data-primary="topic detection" data-secondary="trending topics" id="idm45969589881592"/></dt>
<dd>This deals with identifying the topics that are currently getting the most traction on social networks. Trending topics<a contenteditable="false" data-type="indexterm" data-primary="trending topics" id="idm45969589879672"/> tell us what content people are attracted to and what they think is noteworthy. This information is of immense importance to media houses, retailers, first responders, government entities, and many more. It helps them fine-tune their strategies of engaging with their users. Imagine the insights it could bring when done at the level of specific geolocations.</dd>
<dt>Opinion mining<a contenteditable="false" data-type="indexterm" data-primary="opinion mining" id="idm45969589877640"/></dt>
<dd>People often use social media to express their opinions about a product, service, or policy. Gathering this information and making sense of it is of great value to brands and organizations. It’s impossible to go through thousands of tweets and posts manually to understand the larger opinion of the masses. In such scenarios, being able to summarize thousands of social posts and extract the key insights is highly valuable.</dd>
<dt>Sentiment detection<a contenteditable="false" data-type="indexterm" data-primary="sentiment analysis" data-secondary="with social media data" data-secondary-sortas="social media data" id="idm45969589875224"/></dt>
<dd>Sentiment analysis of social media data has to be by far the most popular application of NLP on social data. Brands rely extensively on using signals from social media to better understand their users’ sentiments about their products and services and that of their competitors. They use it to better understand their customers, from using sentiment to identify the cohorts of customers they should engage with to understanding the shift in the sentiment of its customer base over a long duration of time.</dd>
<dt>Rumor/fake news detection<a contenteditable="false" data-type="indexterm" data-primary="fake news" id="idm45969589872184"/><a contenteditable="false" data-type="indexterm" data-primary="rumor/fake news detection" id="idm45969589871080"/></dt>
<dd>Given their fast and far reach, social networks are also misused to spread false news. In the past few years, there have been instances where social networks were used to sway the opinion of masses using false propaganda<a contenteditable="false" data-type="indexterm" data-primary="false propaganda" id="idm45969589869256"/><a contenteditable="false" data-type="indexterm" data-primary="propaganda, false" id="idm45969589868152"/>. There is a lot of work going on toward understanding and identifying fake news and rumors. This is part of both preventive and corrective measures to control this menace.</dd>
<dt>Adult content filtering<a contenteditable="false" data-type="indexterm" data-primary="adult content filtering" id="idm45969589866312"/><a contenteditable="false" data-type="indexterm" data-primary="filtering adult content" id="idm45969589865176"/></dt>
<dd>Social media also suffers from people using social networks to spread inappropriate content. NLP is used extensively to identify and filter out inappropriate content, such as nudity, profanity, racism, threats, etc.</dd>
<dt>Customer support<a contenteditable="false" data-type="indexterm" data-primary="customer support" data-secondary="on social channels" id="idm45969589863016"/></dt>
<dd>Owing to the widespread use of social media and its public visibility, customer support on social media has evolved into a must-have for every brand across the globe. Users reach out to brands with their complaints and concerns via social channels. NLP is used extensively to understand, categorize, filter, prioritize, and in some cases even automatically respond to the complaints.</dd>
</dl>
<p>There are many other applications that we haven’t dug into, such as geolocation detection, sarcasm detection, event and topic detection, emergency situation awareness, and rumor detection, to name a few. Our aim here is to give you a good idea of the landscape of applications that can be built using social media text data (SMTD)<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" id="idm45969589860184"/><a contenteditable="false" data-type="indexterm" data-primary="SMTD" data-see="social media text data" id="idm45969589859112"/>.</p>
<p>Now, let’s look into why building NLP applications using SMTD is not a straightforward application of the concepts we’ve learned so far in this book and why SMTD requires special treatment.<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="applications that" data-startref="ch08_term2" id="idm45969589856952"/></p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Unique Challenges"><div class="sect1" id="unique_challenges">
<h1>Unique Challenges</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="unique challenges" id="ch08_term3"/>Until now, we’ve (implicitly) assumed that the input text (most of the time, if not always) follows the basic tenets of any language, namely:</p>
<ul>
<li><p>Single language</p></li>
<li><p>Single script</p></li>
<li><p>Formal</p></li>
<li><p>Grammatically correct</p></li>
<li><p>Few or no spelling errors</p></li>
<li><p>Mostly text-like (very few non-textual elements, such as emoticons, images, <span class="keep-together">smileys, etc.)</span></p></li>
</ul>
<p>These assumptions essentially stem from the properties and characteristics of the domain(s) from which the input text data comes. Standard NLP systems assume that the language they deal with is highly structured and formal. When it comes to text data coming from social platforms, most of the above assumptions go for a toss. This is because users can be extremely terse when posting on social media; this extreme brevity is a hallmark of social posts. For example, users may write “are” as “r,” “we” as “v,” “laugh out loud” as “lol,” etc. This brevity has given rise to a new recipe for language: one that’s very informal and consists of nonstandard spellings, hashtags, emoticons, new words and acronyms, code-mixing, transliteration, etc. These characteristics make the language used on social platforms so unique that it’s altogether considered a new language—the “language of social.”</p>
<p class="pagebreak-before">Because of this, the NLP tools and techniques designed for standard text data don’t work well with SMTD. To illustrate this point better, let’s look at some sample tweets, shown in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#examples_of_new_words_being_introduced">8-2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#new_recipe_for_language_nonstandard_spe">8-3</a>. <a contenteditable="false" data-type="indexterm" data-primary="social media" data-secondary="example posts" id="idm45969589831144"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="example posts" id="idm45969589829928"/>Notice how the language used here is very different from the language used in newspapers, blog posts, emails, book chapters, etc.</p>

<figure><div id="examples_of_new_words_being_introduced" class="figure">
<img src="Images/pnlp_0802.png" alt="Examples of new words being introduced in vocabulary" width="1184" height="718"/>
<h6><span class="label">Figure 8-2. </span>Examples of new words being introduced in vocabulary</h6>
</div></figure>

<figure><div id="new_recipe_for_language_nonstandard_spe" class="figure">
<img src="Images/pnlp_0803.png" alt="New recipe for language: nonstandard spellings, emoticons, code-mixing, transliteration [_4]" width="353" height="432"/>
<h6><span class="label">Figure 8-3. </span>New recipe for language: nonstandard spellings, emoticons, code-mixing, transliteration [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_4-marker" href="ch08.xhtml#footnote_8_4">5</a>]</h6>
</div></figure>

<p>These differences pose challenges to standard NLP systems. Let’s look at the key differences in detail:</p>
<dl>
<dt>No grammar</dt>
<dd><a contenteditable="false" data-type="indexterm" data-primary="grammar" id="idm45969589821272"/>Any language is known to strictly follow the rules of grammar. However, conversations on social media don’t follow any grammar and are characterized by inconsistent (or absent) punctuation and capitalization, emoticons, incorrect or nonstandard spelling, repetition of the same character multiple times, and rampant abbreviations. This departure from standard languages makes basic pre-processing steps like tokenization, POS tagging, and identification of sentence boundaries difficult. Modules specialized to work with SMTD are required to achieve these tasks.</dd>
<dt>Nonstandard spelling<a contenteditable="false" data-type="indexterm" data-primary="spelling, nonstandard" id="idm45969589819208"/></dt>
<dd>Most languages have a single way of writing any word, so writing a word in any other way is a spelling mistake. In SMTD, words can have many spelling variations. As an example, consider the following different ways in which the English word “tomorrow” is written on social [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589817560-marker" href="ch08.xhtml#idm45969589817560">6</a>]—tmw, tomarrow, 2mrw, tommorw, 2moz, tomorro, tommarrow, tomarro, 2m, tomorrw, tmmrw, tomoz, tommorow, tmrrw, tommarow, 2maro, tmrow, tommoro, tomolo, 2mor, 2moro, 2mara, 2mw, tomaro, tomarow, tomoro, 2morr, 2mro, tmoz, tomo, 2morro, 2mar, 2marrow, tmr, tomz, tmorrow, 2mr, tmo, tmro, tommorrow, tmrw, tmrrow, 2mora, tommrow, tmoro, 2ma, 2morrow, tomrw, tomm, tmrww, 2morow, 2mrrw, tomorow. For an NLP system to work well, it needs to understand that all these words refer to the same word.</dd>
<dt>Multilingual<a contenteditable="false" data-type="indexterm" data-primary="multilingual writing" id="idm45969589815528"/></dt>
<dd><p>Take any article from a newspaper or a book, and you’ll probably find it’s written in a single language. Seldom will you see where large parts of it are written in more than one language. On social media, people often mix languages. Consider the following example from a social media website [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589813720-marker" href="ch08.xhtml#idm45969589813720">7</a>]:</p>
<ul class="simplelist">
<li><em>Yaar tu to</em>, GOD <em>hain</em>. <strong>tui</strong></li>
<li>JU te ki korchis? Hail u man!</li>
</ul>
<p>It means, “Buddy you are GOD. What are you doing in JU? Hail u man!” The text is a mix of three languages: English (normal font), Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing<a contenteditable="false" data-type="indexterm" data-primary="phonetic typing" id="idm45969589809128"/> has been used.</p></dd>
<dt>Transliteration<a contenteditable="false" data-type="indexterm" data-primary="transliteration" id="ch08_term4"/></dt>
<dd><p>Each language is written in its own script, which refers to how the characters are written. However, on social media, people often write the characters of one script using characters of another script. This is called “transliteration.” For example, consider the Hindi word “आप” (devanagari script, pronounced as “aap”). In English, it means “you” (roman script). But people often write it in roman script as “aap.” Transliteration is common in SMTD, usually due to the typing interface (keyboard) being in roman script but the language of communication being non-English.<a contenteditable="false" data-type="indexterm" data-primary="transliteration" data-startref="ch08_term4" id="idm45969589804888"/></p></dd>
<dt>Special characters<a contenteditable="false" data-type="indexterm" data-primary="special characters" id="idm45969589803128"/></dt>
<dd><p>SMTD is characterized by the presence of many non-textual entities, such as special characters, emojis, hashtags, emoticons, images and gifs, non-ASCII characters, etc. For example, look at the tweets shown in <a data-type="xref" href="#special_characters_in_social_media_data">Figure 8-4</a>. From an NLP standpoint, one needs modules in the pre-processing pipelines to handle such non-textual entities.</p>

<figure class="no-frame"><div id="special_characters_in_social_media_data" class="figure">
<img src="Images/pnlp_0804.png" alt="Special characters in social media data" width="1276" height="832"/>
<h6><span class="label">Figure 8-4. </span>Special characters in social media data<a contenteditable="false" data-type="indexterm" data-primary="social media" data-secondary="example posts" id="idm45969589797848"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="special characters" id="idm45969589796472"/></h6>
</div></figure></dd>

<dt>Ever-evolving vocabulary<a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="ever-evolving" id="ch08_term33"/><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="new words" id="ch08_term7"/><a contenteditable="false" data-type="indexterm" data-primary="out of vocabulary (OOV) problem" id="ch08_term6"/><a contenteditable="false" data-type="indexterm" data-primary="OOV (out of vocabulary) problem" id="ch08_term666"/></dt>
<dd><p>Most languages add either no or very few new words to their vocabulary every year. But when it comes to the language of social, the vocabulary increases at a very fast rate. New words pop up every single day. This means that any NLP system processing SMTD sees a lot of new words that were not part of the vocabulary of the training data. This has an adverse impact on the performance of the NLP system and is known as the out of vocabulary (OOV) problem.</p>
<p>In order to get an idea of the severity of this problem, look at the infographic shown in <a data-type="xref" href="#fraction_of_new_vocabulary_words_seen_e">Figure 8-5</a>. We did this experiment [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_14-marker" href="ch08.xhtml#footnote_8_14">8</a>] a few years ago, where we collected a large corpus of tweets and quantified the amount of “new vocabulary” seen on a month-by-month basis. The figure shows the fraction of new words seen in a month as compared to the previous month’s data. As evident from the image, when compared to the vocabulary of the previous month, there are 10–15% new words every month.</p>

<figure class="no-frame"><div id="fraction_of_new_vocabulary_words_seen_e" class="figure">
<img src="Images/pnlp_0805.png" alt="Fraction of new vocabulary words seen every month [_14]" width="1427" height="1882"/>
<h6><span class="label">Figure 8-5. </span>Fraction<a contenteditable="false" data-type="indexterm" data-primary="out of vocabulary (OOV) problem" data-startref="ch08_term6" id="idm45969589780184"/><a contenteditable="false" data-type="indexterm" data-primary="OOV (out of vocabulary) problem" data-startref="ch08_term666" id="idm45969589778760"/><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="new words" data-startref="ch08_term7" id="idm45969589777368"/><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="ever-evolving" data-startref="ch08_term33" id="idm45969589775720"/> of new vocabulary words seen every month [<a data-type="noteref" href="ch08.xhtml#footnote_8_14">8</a>]</h6>
</div></figure></dd>


<dt>Length of text<a contenteditable="false" data-type="indexterm" data-primary="length of text" id="idm45969589772600"/></dt>
<dd><p>The average length of text on social media platforms is much smaller compared to other channels of communication like blogs, product reviews, emails, etc. The reason is that shorter text can be typed quickly while preserving understandability. This was driven primarily by Twitter<a contenteditable="false" data-type="indexterm" data-primary="Twitter" id="idm45969589770648"/>’s 140-character restriction. For example, “This is an example for texting language” might be written as “dis is n eg 4 txtin lang.” Both mean the same, but the former is 39 characters long while the latter has only 24 characters. As Twitter’s popularity and adoption grew, being terse on social platforms became the norm. This way of condensed writing has become so popular that now it can be seen in every informal communication, such as messages and chats.</p></dd>
<dt>Noisy data<a contenteditable="false" data-type="indexterm" data-primary="noise" data-secondary="on social media" data-secondary-sortas="social media" id="idm45969589768584"/></dt>
<dd><p>Social media posts are full of spam, ads, promoted content, and all manner of other unsolicited, irrelevant, or distracting content. Thus, we cannot take raw data from social platforms and consume it as is. Filtering out noisy data is a vital step. For example, imagine we’re collecting data for an NLP task (say, sarcasm detection) from a Twitter handle or Facebook page, either by scraping or using the Twitter API. It’s important to run a check that no spam, ads, or irrelevant content has come into our dataset.</p></dd>
</dl>
<p>In short, text data from social media is highly informal compared to text data from blogs, books, etc., and this lack of formality can manifest in the various ways described above. All of these can have adverse impacts on the performance of NLP systems that don’t have built-in ways to handle them. <a data-type="xref" href="#spectrum_of_formalism_in_text_data_depe">Figure 8-6</a> [<a data-type="noteref" href="ch08.xhtml#footnote_8_4">5</a>] shows the spectrum of formalism<a contenteditable="false" data-type="indexterm" data-primary="formalism" id="idm45969589763400"/><a contenteditable="false" data-type="indexterm" data-primary="text data" id="idm45969589762328"/> in text data and where different sources of text data appear on it.</p>
<figure><div id="spectrum_of_formalism_in_text_data_depe" class="figure">
<img src="Images/pnlp_0806.png" alt="Spectrum of formalism in text data depending on data sources [_4]" width="1357" height="363"/>
<h6><span class="label">Figure 8-6. </span>Spectrum of formalism in text data depending on data sources [<a data-type="noteref" href="ch08.xhtml#footnote_8_4">5</a>]</h6>
</div></figure>
<p>Owing to the characteristics and peculiarities that stem from the informal nature of the language of social, standard NLP tools and techniques face difficulties when applied to SMTD. NLP for SMTD relies on either converting the text from social to standard text (normalization) or building systems that are specifically designed to tackle SMTD. We’ll see how to go about doing this while building various applications in the next section.</p>
<div data-type="tip"><h6>Tip</h6>

<p>It’s important to identify, understand, and address the language peculiarities found in SMTD. Building submodules that can handle these peculiarities often goes a long way toward improving the performance of models working with SMTD.</p>
</div>
<p>Now, let’s focus on building business applications using SMTD.<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="unique challenges" data-startref="ch08_term3" id="idm45969589755336"/></p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="NLP for Social Data"><div class="sect1" id="nlp_for_social_data">
<h1>NLP for Social Data</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="Natural Language Processing (NLP)" data-secondary="for social data" data-secondary-sortas="social data" id="ch08_term9"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="NLP for" id="ch08_term8"/>We’ll now take a deep dive into applying NLP to SMTD to build some interesting applications that we can apply to a variety of problems. We may need to know how customers are responding to a particular announcement or product we’ve released, or be able to identify user demographics, for example. We’ll start with simple applications like word clouds and ramp up to more complex ones, like understanding sentiment in posts on social media platforms like Twitter.</p>
<section data-type="sect2" data-pdf-bookmark="Word Cloud"><div class="sect2" id="word_cloud">
<h2>Word Cloud</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="word clouds" id="ch08_term10"/>A word cloud is a pictorial way of capturing the most significant words in a given document or corpus. It’s nothing but an image composed of words (in different sizes) from the text under consideration, where the size of the word is proportional to its importance (frequency) in the text corpus. It’s a quick way to understand the <em>key terms</em><a contenteditable="false" data-type="indexterm" data-primary="key terms" id="idm45969589743560"/> in a corpus. If we run a word cloud algorithm on this book, we’re likely to get a word cloud similar to one shown in <a data-type="xref" href="#word_cloud_for_chapter_4_of_this_book">Figure 8-7</a>.</p>
<figure><div id="word_cloud_for_chapter_4_of_this_book" class="figure">
<img src="Images/pnlp_0807.png" alt="Word cloud for of this book" width="710" height="730"/>
<h6><span class="label">Figure 8-7. </span>Word cloud for <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a> of this book</h6>
</div></figure>
<p>Words like NLP, natural language processing, and linguistics occur many times compared to other words in the book, so they show up prominently in the corresponding word cloud. So, how do we create word clouds from a collection of tweets? What’s the NLP pipeline for this?</p>
<p>Here’s a step-by-step process for building a word cloud:</p>
<ol>
<li><p>Tokenize a given corpus or document</p></li>
<li><p>Remove stop words</p></li>
<li><p>Sort the remaining words in descending order of frequency</p></li>
<li><p>Take the top <em>k</em> words and plot them “aesthetically”</p></li>
</ol>
<p>The following code snippet illustrates how to implement this pipeline in practice (the complete code can be found in <em>Ch8/wordcloud.ipynb</em>). For this, we’ll use a library called <code>wordcloud</code> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589732152-marker" href="ch08.xhtml#idm45969589732152">9</a>] that has a built-in function for generating word clouds:</p>
<pre data-code-language="python" data-type="programlisting">    <code class="kn">from</code> <code class="nn">wordcloud</code> <code class="kn">import</code> <code class="n">WordCloud</code>
<code class="n">document_file_path</code> <code class="o">=</code> <code class="err">‘</code><code class="o">./</code><code class="n">twitter_data</code><code class="o">.</code><code class="n">txt</code><code class="err">’</code>
<code class="n">text_from_file</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">document_file_path</code><code class="p">)</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>

<code class="n">stop_words</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">nltk</code><code class="o">.</code><code class="n">corpus</code><code class="o">.</code><code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s1">'english'</code><code class="p">))</code>

<code class="n">word_tokens</code> <code class="o">=</code> <code class="n">twokenize</code><code class="p">(</code><code class="n">text_from_file</code><code class="p">)</code>
<code class="n">filtered_sentence</code> <code class="o">=</code> <code class="p">[</code><code class="n">w</code> <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">word_tokens</code> <code class="k">if</code> <code class="ow">not</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">stop_words</code><code class="p">]</code>
<code class="n">wl_space_split</code> <code class="o">=</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">filtered_sentence</code><code class="p">)</code>
<code class="n">my_wordcloud</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">()</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="n">wl_space_split</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">my_wordcloud</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<p>Depending<a contenteditable="false" data-type="indexterm" data-primary="word clouds" data-startref="ch08_term10" id="idm45969589729208"/> on the styling, we can generate word clouds in various shapes to suit our application [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589641928-marker" href="ch08.xhtml#idm45969589641928">10</a>], as shown in <a data-type="xref" href="#the_same_word_cloud_in_various_shapes">Figure 8-8</a>.</p>
<figure><div id="the_same_word_cloud_in_various_shapes" class="figure">
<img src="Images/pnlp_0808.png" alt="The same word cloud in various shapes" width="1440" height="508"/>
<h6><span class="label">Figure 8-8. </span>The same word cloud in various shapes</h6>
</div></figure>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Tokenizer for SMTD"><div class="sect2" id="tokenizer_for_smtd">
<h2>Tokenizer for SMTD</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="tokenization" data-secondary="for SMTD" data-secondary-sortas="SMTD" id="idm45969589635896"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="tokenizers for" id="idm45969589634024"/>One of the key steps in the above process is to correctly tokenize the text data. For this, we used twokenize [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_15-marker" href="ch08.xhtml#footnote_8_15">11</a>] to get tokens from the text corpus. This is a specialized function for getting tokens from tweets’ text data. This function is part of a set of NLP tools specially designed to work with SMTD [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_16-marker" href="ch08.xhtml#footnote_8_16">12</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589628792-marker" href="ch08.xhtml#idm45969589628792">13</a>]. Now, we might ask: why do we need a specialized tokenizer, and why not use the standard tokenizer available in NLTK? We discussed this briefly in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#text_representation">3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#text_classification">4</a>, but it’s worth spending time on again. The answer lies in the fact that the tokenizer available in NLTK<a contenteditable="false" data-type="indexterm" data-primary="Natural Language Tool Kit (NLTK)" data-secondary="tokenizer" id="idm45969589624600"/> is designed to work with standard English language. Specifically in the English language, two words are separated by space. This might not necessarily be true for English used on Twitter.</p>
<p>This suggests that a tokenizer that uses space as a way to identify word boundaries might not do well on SMTD. Let’s understand this with an example. Consider the following tweet: “Hey @NLPer! This is a #NLProc tweet :-D”. The ideal way to tokenize this text is as follows: ['Hey', '@NLPer', '!', ‘This', ‘is', ‘a', '#NLProc', ‘tweet', ':-D']. Using a tokenizer designed for the English language, like nltk.tokenize.word_tokenize, we’ll get the following tokens: ['Hey', '@', ‘NLPer', '!', ‘This', ‘is', ‘a', '#', ‘NLProc', ‘tweet', ':', '-D']. Clearly, the set of tokens given by the tokenizer in NLTK is not correct. It’s important to use a tokenizer that gives correct tokens. twokenize is specifically designed to deal with SMTD.</p>
<p>Once we have the correct set of tokens, frequency counting is straightforward. There are a number of specialized tokenizers<a contenteditable="false" data-type="indexterm" data-primary="tokenization" data-secondary="specialized tokenizers" id="idm45969589620968"/> available to work with SMTD. Some of the popular ones are nltk.tokenize.TweetTokenizer<a contenteditable="false" data-type="indexterm" data-primary="nltk.tokenize.TweetTokenizer" id="idm45969589619464"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589618264-marker" href="ch08.xhtml#idm45969589618264">14</a>], Twikenizer<a contenteditable="false" data-type="indexterm" data-primary="Twikenizer" id="idm45969589616856"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589615624-marker" href="ch08.xhtml#idm45969589615624">15</a>], Twokenizer<a contenteditable="false" data-type="indexterm" data-primary="Twokenizer" id="idm45969589614120"/> by ARK at CMU [<a data-type="noteref" href="ch08.xhtml#footnote_8_16">12</a>], and twokenize<a contenteditable="false" data-type="indexterm" data-primary="twokenize" id="idm45969589612024"/> [<a data-type="noteref" href="ch08.xhtml#footnote_8_15">11</a>]. For a given input tweet, each of them can give slightly different output. Use the one that gives the best output for your corpus and use case.<a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="specialized tokenizers for data from" id="idm45969589609768"/></p>
<p>Now, we’ll move on to the next application, where we’ll try to extract topics that are trending.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Trending Topics"><div class="sect2" id="trending_topics">
<h2>Trending Topics</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="trending topics" id="ch08_term12"/><a contenteditable="false" data-type="indexterm" data-primary="topic detection" data-secondary="trending topics" id="ch08_term11"/>Just a couple of years ago, keeping yourself updated with the latest topics was pretty straightforward—pick up the day’s newspaper, read through the headlines, and you’re done. Social media has changed this. Given the volume of traffic, what is trending can (and often does) change within a few hours. Keeping track of what’s trending by the hour may not be so important for an individual, but for a business entity, it can be very important.</p>
<p>How can we keep track of trending topics? In the lingo of social media, any conversation around a topic is often associated with a hashtag. Thus, finding trending topics is all about finding the most popular hashtags in a given time window. In <a data-type="xref" href="#snapshot_of_trending_topics_on_twitter">Figure 8-9</a>, we show a snapshot of trending topics in the area of New York.</p>
<figure><div id="snapshot_of_trending_topics_on_twitter" class="figure">
<img src="Images/pnlp_0809.png" alt="Snapshot of trending topics on Twitter [_36]" width="281" height="537"/>
<h6><span class="label">Figure 8-9. </span>Snapshot of trending topics on Twitter [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589599096-marker" href="ch08.xhtml#idm45969589599096">16</a>]</h6>
</div></figure>
<p>So how do we implement a system that can collect trending topics? One of the simplest ways to do this is using a Python API called Tweepy<a contenteditable="false" data-type="indexterm" data-primary="Tweepy" id="ch08_term13"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589595400-marker" href="ch08.xhtml#idm45969589595400">17</a>]. Tweepy gives a simple function, <code>trends_available</code>, to fetch the trending topics. It takes the geolocation (WOEID, or Where On Earth Identifier) as an input and returns the trending topics in that geolocation. The function <code>trends_available</code> returns the top-10 trending topics for a given WOEID, on the condition that the trending information for the given WOEID is available. The response of this function call is an array of objects that are “trending.” In response, each object encodes the following information: name of the topic that’s trending, the corresponding query parameters that can be used to search for the topic using Twitter search, and the URL to Twitter search. Below is a code snippet that demonstrates how we can use Tweepy to fetch trending topics (full code at <em>Ch8/TrendingTopics.ipynb</em>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">tweepy</code><code class="o">,</code> <code class="nn">json</code>

<code class="n">CONSUMER_KEY</code> <code class="o">=</code> <code class="s1">'key'</code>
<code class="n">CONSUMER_SECRET</code> <code class="o">=</code> <code class="s1">'secret'</code>
<code class="n">ACCESS_KEY</code> <code class="o">=</code> <code class="s1">'key'</code>
<code class="n">ACCESS_SECRET</code> <code class="o">=</code> <code class="s1">'secret'</code>
<code class="n">auth</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">OAuthHandler</code><code class="p">(</code><code class="n">CONSUMER_KEY</code><code class="p">,</code> <code class="n">CONSUMER_SECRET</code><code class="p">)</code>
<code class="n">auth</code><code class="o">.</code><code class="n">set_access_token</code><code class="p">(</code><code class="n">ACCESS_KEY</code><code class="p">,</code> <code class="n">ACCESS_SECRET</code><code class="p">)</code>
<code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">)</code>

<code class="c1"># Where On Earth ID for the entire world is 1.</code>
<code class="c1"># See https://dev.twitter.com/docs/api/1.1/get/trends/place and</code>
<code class="c1"># http://developer.yahoo.com/geo/geoplanet/</code>

<code class="n">WORLD_WOE_ID</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">CANADA_WOE_ID</code> <code class="o">=</code> <code class="mi">23424775</code> <code class="c1"># WOEID for Canada</code>

<code class="n">world_trends</code> <code class="o">=</code> <code class="n">api</code><code class="o">.</code><code class="n">t</code>
<code class="n">trends_place</code><code class="p">(</code><code class="n">_id</code><code class="o">=</code><code class="n">WORLD_WOE_ID</code><code class="p">)</code>
<code class="n">canada_trends</code> <code class="o">=</code> <code class="n">api</code><code class="o">.</code><code class="n">trends_place</code><code class="p">(</code><code class="n">_id</code><code class="o">=</code><code class="n">CANADA_WOE_ID</code> <code class="p">)</code>
<code class="n">world_trends_set</code> <code class="o">=</code> <code class="nb">set</code><code class="p">([</code><code class="n">trend</code><code class="p">[</code><code class="s1">'name'</code><code class="p">]</code> <code class="k">for</code> <code class="n">trend</code> <code class="ow">in</code> <code class="n">world_trends</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s1">'trends'</code><code class="p">]])</code>

<code class="n">canada_trends_set</code> <code class="o">=</code> <code class="nb">set</code><code class="p">([</code><code class="n">trend</code><code class="p">[</code><code class="s1">'name'</code><code class="p">]</code> <code class="k">for</code> <code class="n">trend</code> <code class="n">incanada_trends</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s1">'trends'</code><code class="p">]])</code>

<code class="c1"># This gives the top trending hashtags for both world and Canada.</code>
<code class="n">common_trends</code> <code class="o">=</code> <code class="n">world_trends_set</code><code class="o">.</code><code class="n">intersection</code><code class="p">(</code><code class="n">us_trends_set</code><code class="p">)</code>

<code class="n">trend_queries</code> <code class="o">=</code> <code class="p">[</code><code class="n">trend</code><code class="p">[</code><code class="s1">'query'</code><code class="p">]</code> <code class="k">for</code> <code class="n">trend</code> <code class="ow">in</code> <code class="n">results</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s1">'trends'</code><code class="p">]]</code>

<code class="k">for</code> <code class="n">trend_query</code> <code class="ow">in</code> <code class="n">trend_queries</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">q</code><code class="o">=</code><code class="n">trend_query</code><code class="p">))</code>

<code class="c1"># this will return the tweets for each of the trending topic</code></pre>
<p>This small snippet of code will give us the live top trends for a given location. The only problem is that Tweepy is a free API, so it has rate limits.<a contenteditable="false" data-type="indexterm" data-primary="Tweepy" data-startref="ch08_term13" id="idm45969589589160"/> Twitter<a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="rate limits" id="idm45969589587816"/> imposes rate limits on how many requests an application can make to any given API resource within a given time window—you can’t make thousands of requests. Twitter’s rate limits are well documented. In case you need to make calls beyond the rate limits, look at Gnip<a contenteditable="false" data-type="indexterm" data-primary="Gnip" id="idm45969589355352"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589354120-marker" href="ch08.xhtml#idm45969589354120">18</a>], a paid data hosepipe from Twitter.</p>
<p>Now, let’s see how to implement another popular NLP application: sentiment analysis with social media data.<a contenteditable="false" data-type="indexterm" data-primary="trending topics" data-startref="ch08_term12" id="idm45969589352168"/><a contenteditable="false" data-type="indexterm" data-primary="topic detection" data-secondary="trending topics" data-startref="ch08_term11" id="idm45969589350824"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Understanding Twitter Sentiment"><div class="sect2" id="understanding_twitter_sentiment">
<h2>Understanding Twitter Sentiment</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="sentiment analysis with" id="ch08_term16"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="sentiment analysis with" id="ch08_term15"/><a contenteditable="false" data-type="indexterm" data-primary="sentiment analysis" data-secondary="with social media data" data-secondary-sortas="social media data" id="ch08_term14"/>When it comes to NLP and social media, one of the most popular applications has to be sentiment analysis. For businesses and brands across the globe, it’s crucial to listen to what people are saying about them and their products and services. It’s even more important to know whether people’s opinion is positive or negative and if this sentiment polarity is changing over time. In the pre-social era, this was done using customer surveys, including door-to-door visits. In today’s world, social media is a great way to understand people’s sentiment about a brand. Even more important is how this sentiment changes over time. <a data-type="xref" href="#tracking_change_in_sentiment_over_time">Figure 8-10</a> shows how sentiment changes over time for a given organization. Visualizations like these provide great insights to marketing teams and organizations—dissecting their audience’s reactions to their campaigns and events helps them plan strategically for future campaigns and content.</p>
<figure><div id="tracking_change_in_sentiment_over_time" class="figure">
<img src="Images/pnlp_0810.png" alt="Tracking change in sentiment over time [_37]" width="1321" height="1050"/>
<h6><span class="label">Figure 8-10. </span><a contenteditable="false" data-type="indexterm" data-primary="sentiment analysis" data-secondary="tracking changes over time" id="idm45969589338408"/>Tracking change in sentiment over time [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589336888-marker" href="ch08.xhtml#idm45969589336888">19</a>]</h6>
</div></figure>
<p>In this section, we’ll focus on building sentiment analysis for Twitter data using a dataset from the public domain. There are many datasets<a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="public" id="idm45969589334040"/><a contenteditable="false" data-type="indexterm" data-primary="public datasets" id="idm45969589332664"/> available on the internet—for example, the University of Michigan Sentiment Analysis competition<a contenteditable="false" data-type="indexterm" data-primary="University of Michigan Sentiment Analysis competition" id="idm45969589331320"/> on Kaggle<a contenteditable="false" data-type="indexterm" data-primary="Kaggle" id="idm45969589329992"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589328728-marker" href="ch08.xhtml#idm45969589328728">20</a>] and Twitter Sentiment Corpus<a contenteditable="false" data-type="indexterm" data-primary="Twitter Sentiment Corpus (Sanders)" id="idm45969589327240"/> by Niek Sanders<a contenteditable="false" data-type="indexterm" data-primary="Sanders, Niek" id="idm45969589326008"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589324744-marker" href="ch08.xhtml#idm45969589324744">21</a>].</p>
<p>How is sentiment analysis for Twitter different from the sentiment analysis models we built in <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>? The key difference lies in the dataset. In <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>, we used the IMDB dataset<a contenteditable="false" data-type="indexterm" data-primary="IMDB dataset" id="idm45969589320888"/>, which consists of sentences that are well formed and have a structure to them. On the other hand, the data in the Twitter sentiment corpus consists of tweets written informally. This leads to the various issues we discussed in <a data-type="xref" href="#unique_challenges">“Unique Challenges”</a>. These issues, in turn, impact the performance of the model. A great experiment is to run the sentiment analysis pipeline from <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a> on our Twitter corpus and take a deep dive into the kind of mistakes the model makes. We leave this as an exercise for the reader.</p>
<p class="pagebreak-before">We’ll move forward by building a system for sentiment analysis and setting up a baseline. For this, we’ll use TextBlob<a contenteditable="false" data-type="indexterm" data-primary="TextBlob toolkit" id="idm45969589316504"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_22-marker" href="ch08.xhtml#footnote_8_22">22</a>], which is a Python-based NLP toolkit built on top of NLTK and Pattern. It comes with an array of modules for text processing, text mining, and text analysis. All it takes is five lines of code to get a basic sentiment <span class="keep-together">classifier:</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">textblob</code> <code class="kn">import</code> <code class="n">TextBlob</code>

<code class="k">for</code> <code class="n">tweet_text</code> <code class="ow">in</code> <code class="n">tweets_text_collection</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">tweet_text</code><code class="p">)</code>
    <code class="n">analysis</code> <code class="o">=</code> <code class="n">TextBlob</code><code class="p">(</code><code class="n">tweet_text</code><code class="p">)</code>
    <code class="k">print</code><code class="p">(</code><code class="n">analysis</code><code class="o">.</code><code class="n">sentiment</code><code class="p">)</code></pre>
<p>This will give us polarity and subjectivity values of each of the tweets in the corpus. Polarity<a contenteditable="false" data-type="indexterm" data-primary="polarity" id="idm45969589300136"/> is a value between [–1.0, 1.0] and tells how positive or negative the text is. Subjectivity<a contenteditable="false" data-type="indexterm" data-primary="subjectivity" id="idm45969589232104"/> is within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.</p>
<p>It uses a simple idea: tokenize the tweet and compute polarity and subjectivity for each of the tokens. Then combine the polarity and subjectivity numbers to arrive at a single value for the whole sentence. We leave it to the reader to get into the finer details. This simple sentiment classifier might not work well, primarily because of the tokenizer used by TextBlob. Our data comes from social media, so it will most likely not follow formal English. Thus, after tokenization, many of the tokens may not be standard words found in the English dictionary, so we won’t have the polarity and subjectivity for all such tokens.</p>
<p>Say we’ve been asked to improve our classifier. We can try various techniques and algorithms we learned in <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>. However, we might not see a great improvement in performance because of the noise present in the data (discussed in <a data-type="xref" href="#unique_challenges">“Unique Challenges”</a>). Thus, the key to improving the system lies in better cleaning and pre-processing of the text data. This is crucial for SMTD. Below, we’ll discuss some important parts of pre-processing for SMTD. For the rest of the pipeline, we can follow the pipeline discussed in <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Pre-processing<a contenteditable="false" data-type="indexterm" data-primary="sentiment analysis" data-secondary="with social media data" data-secondary-sortas="social media data" data-startref="ch08_term14" id="idm45969589225080"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="sentiment analysis with" data-startref="ch08_term15" id="idm45969589223128"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="sentiment analysis with" data-startref="ch08_term16" id="idm45969589221512"/> and data cleaning are crucial when working with SMTD. This step is likely to provide the most gains in model performance.</p>
</div>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Pre-Processing SMTD"><div class="sect2" id="preprocessing_smtd">
<h2>Pre-Processing SMTD</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="pre-processing" data-secondary="SMTD" id="ch08_term21"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="pre-processing" id="ch08_term20"/>Most NLP systems that work with SMTD have a rich pre-processing pipeline that includes many steps. In this section, we’ll cover some of the steps that come up often in dealing with SMTD.</p>
<section data-type="sect3" data-pdf-bookmark="Removing markup elements"><div class="sect3" id="removing_markup_elements">
<h3>Removing markup elements</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="markup elements: removing" id="idm45969589286296"/>It’s not surprising to see markup elements (HTML, XML, XHTML, etc.) in SMTD, and it’s important to remove them. A great way to achieve this is to use a library called Beautiful Soup<a contenteditable="false" data-type="indexterm" data-primary="Beautiful Soup library" id="idm45969589284792"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589283560-marker" href="ch08.xhtml#idm45969589283560">23</a>]:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>

<code class="n">markup</code> <code class="o">=</code> <code class="s1">'&lt;a href="http://nlp.com/"&gt;</code><code class="se">\n</code><code class="s1">I love &lt;i&gt;nlp&lt;/i&gt;</code><code class="se">\n</code><code class="s1">&lt;/a&gt;'</code>
<code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">markup</code><code class="p">)</code>
<code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code></pre>
<p>This gives the output <code>\nI love  nlp\n</code>.</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Handling non-text data"><div class="sect3" id="handling_non_text_data">
<h3>Handling non-text data</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="non-text data" id="idm45969589183064"/>SMTD is often full of symbols, special characters, etc., and they’re often in encodings such as Latin and Unicode. In order to understand them, it’s important to convert the symbols present in the data to simple and easier-to-understand characters. This is often done by converting to a standard encoding format like UTF-8. In the example below, we see how the entire text is converted into a machine-readable form:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">text</code> <code class="o">=</code> <code class="s1">'I love Pizza 🍕!  Shall we book a cab 🚕 to gizza?'</code>
<code class="n">Text</code> <code class="o">=</code> <code class="n">text</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"utf-8"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">Text</code><code class="p">)</code>

<code class="s-Affix">b</code><code class="s1">'I love Pizza </code><code class="se">\xf0\x9f\x8d\x95</code><code class="s1">!  </code>
<code class="n">Shall</code> <code class="n">we</code> <code class="n">book</code> <code class="n">a</code> <code class="n">cab</code> \<code class="n">xf0</code>\<code class="n">x9f</code>\<code class="n">x9a</code>\<code class="n">x95</code> <code class="n">to</code> <code class="n">get</code> <code class="n">pizza</code><code class="err">?</code><code class="s1">'</code></pre>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Handling apostrophes"><div class="sect3" id="handling_apostrophes">
<h3>Handling apostrophes</h3>
<p>Another hallmark of SMTD is the use of the apostrophe; it’s quite common to see scenarios like ‘s, ‘re, ‘r, etc. The way to handle this is to expand apostrophes. This requires a dictionary that can map apostrophes to full forms:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">Apostrophes_expansion</code> <code class="o">=</code> <code class="p">{</code>
<code class="err">“</code><code class="s1">'s" : " is",</code>
<code class="s2">"'re"</code> <code class="p">:</code> <code class="s2">" are"</code><code class="p">,</code>
<code class="s2">"'r"</code> <code class="p">:</code> <code class="s2">" are"</code><code class="p">,</code> <code class="o">...</code><code class="p">}</code> <code class="c1">## Given such a dictionary</code>
<code class="n">words</code> <code class="o">=</code> <code class="n">twokenize</code><code class="p">(</code><code class="n">tweet_text</code><code class="p">)</code>

<code class="n">processed_tweet_text</code> <code class="o">=</code> <code class="p">[</code><code class="n">Apostrophes_expansion</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="k">if</code> <code class="n">word</code> 
                       <code class="ow">in</code> <code class="n">Apostrophes_expansion</code> <code class="k">else</code> <code class="n">word</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">]</code>

<code class="n">processed_tweet_text</code> <code class="o">=</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">processed_tweet_text</code><code class="p">)</code></pre>
<p>To the best of our knowledge, such a mapping between apostrophes and their expansion is not available anywhere off the shelf, so it needs to be created manually.</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Handling emojis"><div class="sect3" id="handling_emojis">
<h3>Handling emojis</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="emojis" id="ch08_term22"/>Emojis are at the very core of communication over social channels. One small image can completely describe one or more human emotions. However, they pose a huge challenge for machines. How can we design subsystems that can understand the meaning of an emoji? A naive thing to do during pre-processing would be to remove all emojis. This could result in significant loss of meaning.</p>
<p>A good way to achieve this is to replace the emoji with corresponding text explaining the emoji. For example, replace “” with “<code>fire</code>”. To do so, we need a mapping between emojis and their corresponding elaboration in text. Demoji<a contenteditable="false" data-type="indexterm" data-primary="Demoji" id="idm45969589047096"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969589045864-marker" href="ch08.xhtml#idm45969589045864">24</a>] is a Python package that does exactly this. It has a function, <code>findall()</code>, that gives a list of all emojis in the text along with their corresponding meanings.</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">tweet</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">#startspreadingthenews yankees win great start by </code><img src="Images/1f385-1f3fd_santa.png" width="154" height="154"/><code class="s2"> going 5strong</code><code>
</code><code class="n">innings</code><code> </code><code class="k">with</code><code> </code><code class="mi">5</code><code class="n">k</code><code class="err">’</code><code class="n">s</code><img src="Images/1f525_fire.png" width="146" height="154"/><code> </code><img src="Images/1f402_ox.png" width="154" height="116"/><code> </code><code class="n">solo</code><code> </code><code class="n">homerun</code><code> </code><img src="Images/1f30b_volcano.png" width="154" height="154"/><img src="Images/1f30b_volcano.png" width="154" height="154"/><code> </code><code class="k">with</code><code> </code><code class="mi">2</code><code> </code><code class="n">solo</code><code> </code><code class="n">homeruns</code><code> </code><code>
</code><code class="ow">and</code><img src="Images/1f479_ogre.png" width="154" height="150"/><code> </code><code class="mi">3</code><code class="n">run</code><code> </code><code class="n">homerun</code><code class="err">…</code><code> </code><img src="Images/1f921_clown.png" width="154" height="154"/><code> </code><img src="Images/1f6a3-1f3fb_row.png" width="154" height="131"/><code> </code><img src="Images/1f468-1f3fe-200d-2696-fe0f_man_judge.png" width="154" height="140"/><code> </code><code class="k">with</code><code> </code><code class="n">rbi</code><code class="err">’</code><code class="n">s</code><code> </code><code class="err">…</code><code> </code><img src="Images/1f525_fire.png" width="146" height="154"/><img src="Images/1f525_fire.png" width="146" height="154"/><code> </code><img src="Images/1f1f2-1f1fd_mexico.png" width="154" height="113"/><code> </code><code class="ow">and</code><code> </code><img src="Images/1f1f3-1f1ee_nicaragua.png" width="154" height="113"/><code> </code><code>
</code><code class="n">to</code><code> </code><code class="n">close</code><code> </code><code class="n">the</code><code> </code><code class="n">game</code><img src="Images/1f525_fire.png" width="146" height="154"/><img src="Images/1f525_fire.png" width="146" height="154"/><code class="err">!</code><code class="err">!</code><code class="err">!</code><code class="err">…</code><code class="o">.</code><code class="n">WHAT</code><code> </code><code class="n">A</code><code> </code><code class="n">GAME</code><code class="err">!</code><code class="err">!</code><code> </code><code class="s2">"</code><code>
</code><code>
</code><code class="n">demoji</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">tweet</code><code class="p">)</code><code>
</code><code>
</code><code class="p">{</code><code>
</code><code>    </code><code class="s2">"</code><img id="c528" src="Images/1f525_fire.png" width="146" height="154"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">fire</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c522" src="Images/1f30b_volcano.png" width="154" height="154"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">volcano</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c526" src="Images/1f468-1f3fe-200d-2696-fe0f_man_judge.png" width="154" height="140"/><code class="s2">️</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">man judge: medium skin tone</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c524" src="Images/1f385-1f3fd_santa.png" width="154" height="154"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">Santa Claus: medium-dark skin tone</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c520" src="Images/1f1f2-1f1fd_mexico.png" width="154" height="113"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">flag: Mexico</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c527" src="Images/1f479_ogre.png" width="154" height="150"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">ogre</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c531" src="Images/1f921_clown.png" width="154" height="154"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">clown face</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c521" src="Images/1f1f3-1f1ee_nicaragua.png" width="154" height="113"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">flag: Nicaragua</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c530" src="Images/1f6a3-1f3fb_row.png" width="154" height="131"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">person rowing boat: medium-light skin tone</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><img id="c525" src="Images/1f402_ox.png" width="154" height="116"/><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">ox</code><code class="s2">"</code><code class="p">,</code><code>
</code><code class="p">}</code></pre>

<p>We can use the output of <code>findall()</code> to replace all emojis in a text with their corresponding meaning in words.</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Split-joined words"><div class="sect3" id="split_joined_words">
<h3>Split-joined words</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="split-joined words" id="idm45969588807784"/><a contenteditable="false" data-type="indexterm" data-primary="split-joined words" id="idm45969588806568"/>Another peculiarity of SMTD is that users sometimes combine multiple words into a single word, where the word disambiguation is done by using capital letters, for example GoodMorning, RainyDay, PlayingInTheCold, etc. This is simple to handle. The following code snippet does the job for us:</p>
<pre data-code-language="python" data-type="programlisting">    <code class="n">processed_tweet_text</code> <code class="o">=</code> <code class="err">“</code> <code class="err">“</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="err">‘</code><code class="p">[</code><code class="n">A</code><code class="o">-</code><code class="n">Z</code><code class="p">][</code><code class="o">^</code><code class="n">A</code><code class="o">-</code><code class="n">Z</code><code class="p">]</code><code class="o">*</code><code class="err">’</code><code class="p">,</code> <code class="n">tweet_text</code><code class="p">))</code></pre>
<p>For GoodMorning, this will return “Good Morning.”</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Removal of URLs"><div class="sect3" id="removal_of_urls">
<h3>Removal of URLs</h3>
<p><a contenteditable="false" data-type="indexterm" data-primary="URLs: removal of" id="ch08_term23"/>Another common feature of SMTD is the use of URLs. Depending on the application, we might want to remove the URL all together. The code snippet replaces all URLs with a constant; in this case, <code>constant_url</code>. While in simpler cases, we could use a regex, such as <code>http\S+</code>, in most cases, we’ll have to write a custom regex like the one shown in the following snippet. This code is complex because some social posts contain tiny URLs instead of full URLs:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">process_URLs</code><code class="p">(</code><code class="n">tweet_text</code><code class="p">):</code>
    <code class="sd">'''</code>
<code class="sd">    replace all URLs in the tweet text</code>
<code class="sd">    '''</code>
    <code class="n">UrlStart1</code> <code class="o">=</code> <code class="n">regex_or</code><code class="p">(</code><code class="s1">'https?://'</code><code class="p">,</code> <code class="s-Affix">r</code><code class="s1">'www\.'</code><code class="p">)</code>
    <code class="n">CommonTLDs</code>  <code class="o">=</code> <code class="n">regex_or</code><code class="p">(</code> <code class="s1">'com'</code><code class="p">,</code><code class="s1">'co</code><code class="se">\\</code><code class="s1">.uk'</code><code class="p">,</code><code class="s1">'org'</code><code class="p">,</code><code class="s1">'net'</code><code class="p">,</code><code class="s1">'info'</code><code class="p">,</code><code class="s1">'ca'</code><code class="p">,</code><code class="s1">'biz'</code><code class="p">,</code>
                          <code class="s1">'info'</code><code class="p">,</code><code class="s1">'edu'</code><code class="p">,</code><code class="s1">'in'</code><code class="p">,</code><code class="s1">'au'</code><code class="p">)</code>
    <code class="n">UrlStart2</code> <code class="o">=</code> <code class="s-Affix">r</code><code class="s1">'[a-z0-9\.-]+?'</code> <code class="o">+</code> <code class="s-Affix">r</code><code class="s1">'\.'</code> <code class="o">+</code> <code class="n">CommonTLDs</code> <code class="o">+</code> 
                <code class="n">pos_lookahead</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'[/ \W\b]'</code><code class="p">)</code>
    <code class="c1"># * not + for case of:  "go to bla.com." -- don't want period</code>
    <code class="n">UrlBody</code> <code class="o">=</code> <code class="s-Affix">r</code><code class="s1">'[^ \t\r\n&lt;&gt;]*?'</code>
    <code class="n">UrlExtraCrapBeforeEnd</code> <code class="o">=</code> <code class="s1">'</code><code class="si">%s</code><code class="s1">+?'</code> <code class="o">%</code> <code class="n">regex_or</code><code class="p">(</code><code class="n">PunctChars</code><code class="p">,</code> <code class="n">Entity</code><code class="p">)</code>
    <code class="n">UrlEnd</code> <code class="o">=</code> <code class="n">regex_or</code><code class="p">(</code> <code class="s-Affix">r</code><code class="s1">'\.\.+'</code><code class="p">,</code> <code class="s-Affix">r</code><code class="s1">'[&lt;&gt;]'</code><code class="p">,</code> <code class="s-Affix">r</code><code class="s1">'\s'</code><code class="p">,</code> <code class="s1">'$'</code><code class="p">)</code>
    <code class="n">Url</code> <code class="o">=</code>      <code class="p">(</code><code class="n">optional</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\b'</code><code class="p">)</code> <code class="o">+</code>
          <code class="n">regex_or</code><code class="p">(</code><code class="n">UrlStart1</code><code class="p">,</code> <code class="n">UrlStart2</code><code class="p">)</code> <code class="o">+</code>
          <code class="n">UrlBody</code> <code class="o">+</code>
    <code class="n">pos_lookahead</code><code class="p">(</code> <code class="n">optional</code><code class="p">(</code><code class="n">UrlExtraCrapBeforeEnd</code><code class="p">)</code> <code class="o">+</code> <code class="n">UrlEnd</code><code class="p">))</code>

    <code class="n">Url_RE</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s2">"(</code><code class="si">%s</code><code class="s2">)"</code> <code class="o">%</code> <code class="n">Url</code><code class="p">,</code> <code class="n">re</code><code class="o">.</code><code class="n">U</code><code class="o">|</code><code class="n">re</code><code class="o">.</code><code class="n">I</code><code class="p">)</code>
    <code class="n">tweet_text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="n">Url_RE</code><code class="p">,</code> <code class="s2">" constant_url "</code><code class="p">,</code> <code class="n">tweet_text</code><code class="p">)</code>

    <code class="c1"># fix to handle unicodes in URL</code>
    <code class="n">URL_regex2</code> <code class="o">=</code> <code class="s-Affix">r</code><code class="s1">'\b(htt)[p\:\/]*([</code><code class="se">\\</code><code class="s1">x</code><code class="se">\\</code><code class="s1">u][a-z0-9]*)*'</code>
    <code class="n">tweet_text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="n">URL_regex2</code><code class="p">,</code> <code class="s2">" constant_url "</code><code class="p">,</code> <code class="n">tweet_text</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">tweet_text</code></pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Nonstandard spellings"><div class="sect3" id="nonstandard_spellings">
<h3>Nonstandard spellings</h3>
<p>On social<a contenteditable="false" data-type="indexterm" data-primary="spelling, nonstandard" id="ch08_term24"/> media, people often write words that are technically spelling mistakes. For example, people often write one or more characters multiple times, as in “yessss” or “ssssh” (instead of “yes” or “ssh”). This repetition of characters is very common in SMTD. Below is a simple way to fix this. We use the fact that, in the English language, there are hardly any words that have the same character three times consecutively. So we trim accordingly:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">prune_multple_consecutive_same_char</code><code class="p">(</code><code class="n">tweet_text</code><code class="p">):</code>
    <code class="sd">'''</code>
<code class="sd">    yesssssssss  is converted to yes</code>
<code class="sd">    ssssssssssh is converted to ssh</code>
<code class="sd">    '''</code>
          <code class="n">tweet_text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'(.)\1+'</code><code class="p">,</code> <code class="s-Affix">r</code><code class="s1">'\1\1'</code><code class="p">,</code> <code class="n">tweet_text</code><code class="p">)</code>
          <code class="k">return</code> <code class="n">tweet_text</code></pre>
          
<p>This gives the output <code>yess ssh</code>.</p>

<p>Another idea is to use spelling-correction libraries<a contenteditable="false" data-type="indexterm" data-primary="spelling-correction libraries" id="idm45969588516968"/>. Most of them use some form of distance metric, such as edit distance or Levenshtein distance. TextBlob<a contenteditable="false" data-type="indexterm" data-primary="TextBlob toolkit" id="idm45969588506376"/> itself has some spelling-correction capabilities:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">textblob</code> <code class="kn">import</code> <code class="n">TextBlob</code>

<code class="n">data</code> <code class="o">=</code> <code class="s2">"His sellection is bery antresting"</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">TextBlob</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code class="n">correct</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="n">output</code><code class="p">)</code>
</pre>

<p>This gives the output: <code>His selection is very interesting</code>.</p>

<p>We hope<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="pre-processing" data-startref="ch08_term20" id="idm45969588494072"/><a contenteditable="false" data-type="indexterm" data-primary="pre-processing" data-secondary="SMTD" data-startref="ch08_term21" id="idm45969588452824"/><a contenteditable="false" data-type="indexterm" data-primary="spelling, nonstandard" data-startref="ch08_term24" id="idm45969588451176"/> this gives you a good idea of why, when it comes to SMTD, pre-processing is so important, and of how it can be accomplished. This is by no means an exhaustive list of pre-processing steps. Now, we’ll focus on the next step in our NLP pipeline (from back in <a data-type="xref" href="ch02.xhtml#figure_2_1_generic_nlp_pipeline">Figure 2-1</a>): feature engineering.</p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Text Representation for SMTD"><div class="sect2" id="text_representation_for_smtd">
<h2>Text Representation for SMTD</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="text representation" data-secondary="for SMTD" data-secondary-sortas="SMTD" id="ch08_term31"/><a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="text representation for" id="ch08_term30"/>Previously, we saw how to make a simple sentiment classifier for tweets using TextBlob [<a data-type="noteref" href="ch08.xhtml#footnote_8_22">22</a>].  Now, let’s try to build a more sophisticated classifier. Let’s say we’ve implemented all the pre-processing steps we discussed in the previous section. Now what? Now we need to break the text into tokens and then represent them mathematically. For tokenization, we use twokenize<a contenteditable="false" data-type="indexterm" data-primary="twokenize" id="idm45969588441528"/> [<a data-type="noteref" href="ch08.xhtml#footnote_8_15">11</a>], which is a specialized tokenizer designed to work with Twitter data. How do we represent the tokens we get? We can try various techniques we learned in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>.</p>
<p>In our experience, basic vectorization approaches like BoW<a contenteditable="false" data-type="indexterm" data-primary="BoW (bag of words)" id="idm45969588438040"/><a contenteditable="false" data-type="indexterm" data-primary="bag of words (BoW)" id="idm45969588436936"/> and TF-IDF<a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term Frequency–Inverse Document Frequency)" id="idm45969588435704"/> do not work well with SMTD, primarily due to noise and variation in text data (e.g., the variations of “tomorrow” we discussed earlier in this chapter). The noise and variations lead to extremely sparse vectors. This leaves us with the option of using embeddings. As we saw in <a data-type="xref" href="ch03.xhtml#text_representation">Chapter 3</a>, training our own embeddings is very expensive. So, we can begin by using pre-trained embeddings. In <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>, we saw how to use Google’s pre-trained word embeddings to build a sentiment classifier. Now, if we run the same code on our dataset collected from social media platforms, we may not get impressive numbers like we got there. One of the reasons may be that the vocabulary of our dataset is dramatically different from the vocabulary of the Word2vec model. To verify this, we just tokenize our text corpus and build a set on all tokens, then compare it with the vocabulary of Word2vec. The following code snippet does this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">combined</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">train_test_X</code><code class="p">)</code>

<code class="c1"># This is one way to create vocab set from our dataset.</code>
<code class="n">flat_list</code> <code class="o">=</code> <code class="n">chain</code><code class="p">(</code><code class="o">*</code><code class="n">combined</code><code class="p">)</code>
<code class="n">dataset_vocab</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">flat_list</code><code class="p">)</code>
<code class="nb">len</code><code class="p">(</code><code class="n">dataset_vocab</code><code class="p">)</code>
<code class="n">w2v_vocab</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">w2v_model</code><code class="o">.</code><code class="n">vocab</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>

<code class="k">print</code><code class="p">(</code><code class="n">dataset_vocab</code> <code class="o">-</code> <code class="n">w2v_vocab</code><code class="p">)</code></pre>
<p>Here, <code>train_test_X</code> is the combined set of reviews from training and test chunks of our corpus. Now, you may ask why this wasn’t true when we worked with the IMDB movie review dataset. The reason is that Google’s Word2vec<a contenteditable="false" data-type="indexterm" data-primary="Word2vec model (Google)" id="idm45969588302904"/><a contenteditable="false" data-type="indexterm" data-primary="Google" data-secondary="Word2vec model" id="idm45969588301896"/> is trained on Wikipedia and news articles. The language and vocabulary used in these articles is similar to the language and vocabulary used in the IMDB movie review dataset. This is unlikely to be true with our dataset from social media. So, it’s highly likely that, for our dataset from social media, the set difference will be pretty high.</p>
<p>So, how do we fix this? There are a few ways:</p>
<ol>
  <li><p>Use pre-trained embeddings<a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="pre-trained" id="idm45969588298648"/> from social data, such as the ones from Stanford’s NLP<a contenteditable="false" data-type="indexterm" data-primary="Stanford Natural Language Processing Group" data-secondary="GloVe" id="idm45969588297144"/><a contenteditable="false" data-type="indexterm" data-primary="GloVe" id="idm45969588295672"/> group [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588294440-marker" href="ch08.xhtml#idm45969588294440">25</a>]. They trained word embeddings on two billion tweets [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588292968-marker" href="ch08.xhtml#idm45969588292968">26</a>].</p></li>
  <li><p>Use a better tokenizer<a contenteditable="false" data-type="indexterm" data-primary="tokenization" data-secondary="for SMTD" data-secondary-sortas="SMTD" id="idm45969588291016"/>. We highly recommend the twokenize<a contenteditable="false" data-type="indexterm" data-primary="twokenize" id="idm45969588289208"/> tokenizer from Allen Ritter<a contenteditable="false" data-type="indexterm" data-primary="Ritter, Allen" id="idm45969588287928"/>’s work [<a data-type="noteref" href="ch08.xhtml#footnote_8_15">11</a>].</p></li>
  <li><p>Train your own embeddings<a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="word-based" id="idm45969588285240"/>. This option should be the last resort and done only if you have lots and lots of data (at least 1 to 1.5 million tweets). Even after training your own embeddings, you may not get any considerable bump in performance metrics.</p></li>
</ol>

<div data-type="tip"><h6>Tip</h6><p>In our experience, if you’re going for word-based embeddings, (1) and (2) can give you the best return on investment for your efforts.</p></div>

<p>Even if you get a considerable boost in the performance metrics, as the time gap between training data and production data keeps increasing, the performance can keep going down. This is because as the time gap increases, the overlap between the vocabulary of the training data and production data keeps reducing. One of the main reasons for this is the fact that the vocabulary of social media is always evolving—new words and acronyms are created and used all the time. You might think that new words get added only once in a while, but, surprisingly, this is far from true. <a data-type="xref" href="#plots_depicting_how_fast_the_vocabulary">Figure 8-11</a> shows how fast the vocabulary on social media can evolve [<a data-type="noteref" href="ch08.xhtml#footnote_8_14">8</a>]. The plot on the left shows the percentage of unseen tokens on a month-by-month basis. This analysis was done using approximately 2 million tweets over a span of 27 months. The plot in the middle shows the same statistics as a bar plot of total versus new tokens on a monthly basis. The plot on the right is a cumulative bar chart. On average, approximately 20% of the vocabulary for any month are new words.</p>
<figure><div id="plots_depicting_how_fast_the_vocabulary" class="figure">
<img src="Images/pnlp_0811.png" alt="Plots depicting how fast the vocabulary of social media can evolve [_14]" width="960" height="543"/>
<h6><span class="label">Figure 8-11. </span><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="new words" id="idm45969588277416"/><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="ever-evolving" id="idm45969588276040"/>Plots depicting how fast the vocabulary of social media can evolve [<a data-type="noteref" href="ch08.xhtml#footnote_8_14">8</a>]</h6>
</div></figure>
<p>What does this mean for us? No matter how good our word embeddings<a contenteditable="false" data-type="indexterm" data-primary="word embeddings" data-secondary="for SMTD" data-secondary-sortas="SMTD" id="idm45969588273160"/> are, because of the ever-evolving vocabulary of social media, within a couple of months, our embeddings will become obsolete (i.e., a large portion of our vocabulary won’t be present in our word embeddings). This means that when we query the embedding model with a word to fetch its embedding, it will return null since the query word was not present in the training data when the embeddings were trained. This is analogous to saying that all such words were completely ignored. This, in turn, will dramatically reduce the accuracy of our sentiment classifier with time, because with time, more and more words will end up getting ignored.</p>
<div data-type="tip"><h6>Tip</h6>

<p>Word embeddings are not the best way to represent SMTD, especially when you want to use them for more than four to six months.</p>
</div>
<p>Researchers working in this area identified this problem pretty early and tried various ways to circumvent it. One of the better ways to deal with this persistent OOV<a contenteditable="false" data-type="indexterm" data-primary="OOV (out of vocabulary) problem" id="idm45969588268488"/><a contenteditable="false" data-type="indexterm" data-primary="out of vocabulary (OOV) problem" id="idm45969588267400"/> problem with SMTD is to use character n-gram embeddings<a contenteditable="false" data-type="indexterm" data-primary="character n-gram embeddings" id="idm45969588266152"/><a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="character n-gram" id="idm45969588265032"/>. We discussed this idea when we covered fastText in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#text_representation">3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#text_classification">4</a>. Each character n-gram in the corpus has an embedding for it. Now, if the word is present in the vocabulary of the embeddings, then we use the word embedding directly. If not—i.e., the word is OOV—we break the word into character n-grams and combine all these embeddings to come up with the embedding for the word. fastText has pre-trained character n-gram <span class="keep-together">embeddings</span> but they’re not not Twitter or SMTD<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="text representation for" data-startref="ch08_term30" id="idm45969588231944"/><a contenteditable="false" data-type="indexterm" data-primary="text representation" data-secondary="for SMTD" data-secondary-sortas="SMTD" data-startref="ch08_term31" id="idm45969588230360"/> specific. Researchers have also tried character embeddings<a contenteditable="false" data-type="indexterm" data-primary="character embeddings" id="idm45969588228312"/><a contenteditable="false" data-type="indexterm" data-primary="embedding" data-secondary="character" id="idm45969588227128"/>. An interested reader can look into various works along these lines [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588225624-marker" href="ch08.xhtml#idm45969588225624">27</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588224168-marker" href="ch08.xhtml#idm45969588224168">28</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588222680-marker" href="ch08.xhtml#idm45969588222680">29</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588221240-marker" href="ch08.xhtml#idm45969588221240">30</a>].</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Customer Support on Social Channels"><div class="sect2" id="customer_support_on_social_channels">
<h2>Customer Support on Social Channels</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="customer support" data-secondary="on social channels" id="ch08_term34"/>From its inception to present day, social media has evolved as a channel of communication. It started primarily with the objective of helping people across the globe get connected and express themselves. But the widespread adoption of social media has forced brands and organizations to take another look at their communication strategies. A great example of this is brands providing customer support on social platforms like Twitter and Facebook<a contenteditable="false" data-type="indexterm" data-primary="Facebook" data-secondary="customer support on" id="idm45969588215432"/>. Brands never intended to do this to begin with.</p>
<p>Early in this decade, as the adoption of social platforms grew, brands started to create and own properties and assets like Twitter handles and Facebook pages primarily to reach out to their customers and users and run branding and marketing campaigns. However, over time brands saw that users and customers were reaching out to them with complaints and grievances. As the volume of the complaints and issues grew, this prompted brands to create dedicated handles and pages to handle support traffic. <a data-type="xref" href="#example_of_brandsapostrophe_support_pag">Figure 8-12</a> shows the support pages of Apple<a contenteditable="false" data-type="indexterm" data-primary="Apple" id="idm45969588211832"/> and Bank of America. Twitter and Facebook have launched various features to support brands [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588210520-marker" href="ch08.xhtml#idm45969588210520">31</a>], and most customer relationship management (CRM)<a contenteditable="false" data-type="indexterm" data-primary="customer relationship management (CRM)" id="idm45969588206904"/><a contenteditable="false" data-type="indexterm" data-primary="CRM (customer relationship management) " id="idm45969588205688"/> tools support customer service on social channels. A brand can connect their social channels to the CRM tool and use the tool to respond to inbound messages.</p>
<figure><div id="example_of_brandsapostrophe_support_pag" class="figure">
<img src="Images/pnlp_0812.png" alt="Example of brands’ support pages on Twitter [_40]" width="1394" height="560"/>
<h6><span class="label">Figure 8-12. </span>Example of brands’ support pages on Twitter<a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="customer support on" id="idm45969588202312"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588200808-marker" href="ch08.xhtml#idm45969588200808">32</a>]</h6>
</div></figure>
<p>Owing to the public nature of conversations, brands are obligated to respond quickly. However, brands’ support pages receive a lot of traffic. Some of this is genuine questions, grievances, and requests. These are popularly known as “actionable conversations,”<a contenteditable="false" data-type="indexterm" data-primary="conversations" data-secondary="actionable" id="idm45969588196152"/> as customer support teams should act on them quickly. On the other hand, a large portion of traffic is simply noise: promos, coupons, offers, opinions, troll <span class="keep-together">messages,</span> etc. This is popularly called “noise.”<a contenteditable="false" data-type="indexterm" data-primary="noise" id="idm45969588193752"/> Customer support teams cannot respond to noise and want to steer clear of all such messages. Ideally, they want only actionable messages to be converted into tickets in their CRM tools. <a data-type="xref" href="#example_of_actionable_versus_noisy_mess">Figure 8-13</a> shows examples of both actionable messages and noise.</p>
<figure><div id="example_of_actionable_versus_noisy_mess" class="figure">
<img src="Images/pnlp_0813.png" alt="Example of actionable versus noisy messages [_14]" width="1280" height="899"/>
<h6><span class="label">Figure 8-13. </span>Example of actionable versus noisy messages<a contenteditable="false" data-type="indexterm" data-primary="noise" data-secondary="examples" id="idm45969588189224"/> [<a data-type="noteref" href="ch08.xhtml#footnote_8_14">8</a>]</h6>
</div></figure>
<p>Imagine we work at a CRM product organization and are asked to build a model to segregate actionable messages from noise. How can we go about it? The problem of identifying noise versus actionable messages is analogous to the spam classification problem or sentiment classification problem. We can build a model that can look at the inbound messages. The pipeline will be very similar:</p>
<ol>
<li><p>Collect a labeled dataset</p></li>
<li><p>Clean it</p></li>
<li><p>Pre-process it</p></li>
<li><p>Tokenize it</p></li>
<li><p>Represent it</p></li>
<li><p>Train a model</p></li>
<li><p>Test model</p></li>
<li><p>Put it in production</p></li>
</ol>

<p>We’ve already discussed various aspects of this pipeline in this chapter. Much like sentiment analysis on SMTD, the key here, too, is the pre-processing step. With this, we’re now ready to move to the last topic of this chapter: identifying controversial content on social platforms.<a contenteditable="false" data-type="indexterm" data-primary="social media text data (SMTD)" data-secondary="NLP for" data-startref="ch08_term8" id="idm45969588180104"/><a contenteditable="false" data-type="indexterm" data-primary="Natural Language Processing (NLP)" data-secondary="for social data" data-secondary-sortas="social data" data-startref="ch08_term9" id="idm45969588178456"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Memes and Fake News"><div class="sect1" id="memes_and_fake_news">
<h1>Memes and Fake News</h1>
<p><a contenteditable="false" data-type="indexterm" data-primary="fake news" id="idm45969588175144"/>Users on social platforms are known to share various kinds of information and thoughts in various ways. These platforms were initially designed to be self-regulating. However, over time, users have evolved to behave beyond community norms; this is known as “trolling.”<a contenteditable="false" data-type="indexterm" data-primary="trolling" id="idm45969588173624"/> A large portion of posts on social platforms are full of controversial content such as trolls, memes, internet slang, and fake news. Some of it might be advocacy of propaganda, or it could be just for fun. In any case, this content needs to be monitored and filtered out. In this section, we’ll discuss how to study the trends of such content and the role NLP<a contenteditable="false" data-type="indexterm" data-primary="customer support" data-secondary="on social channels" data-startref="ch08_term34" id="idm45969588172008"/> has to play in it.</p>
<section data-type="sect2" data-pdf-bookmark="Identifying Memes"><div class="sect2" id="identifying_memes">
<h2>Identifying Memes</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="memes" id="ch08_term36"/>Memes are one of the most interesting elements that have been curated by social media users to communicate messages with fun or satire. These memes get reused with minimal changes in form, such as the image of “grumpy cat” (<a data-type="xref" href="#examples_of_memes_left_square_bracket_4">Figure 8-14</a>), which has been used in many scenarios with different text associated with it. This resembles the original concept of “genes” as coined by Richard Dawkins<a contenteditable="false" data-type="indexterm" data-primary="Dawkins, Richard" id="idm45969588165224"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588164024-marker" href="ch08.xhtml#idm45969588164024">33</a>]. Lada Adamic<a contenteditable="false" data-type="indexterm" data-primary="Adamic, Lada" id="idm45969588162168"/> from Facebook<a contenteditable="false" data-type="indexterm" data-primary="Facebook" data-secondary="memes on" id="idm45969588160936"/> studied information flow via these memes in the Facebook network [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_28-marker" href="ch08.xhtml#footnote_8_28">34</a>]; she says,  “…memes propagating via a manual copy and paste mechanism can be exact, or they might contain a “mutation,” an accidental or intentional modification.” <a data-type="xref" href="#examples_of_memes_left_square_bracket_4">Figure 8-14</a> shows examples of two popular memes that you might have come across.</p>
<figure><div id="examples_of_memes_left_square_bracket_4" class="figure">
<img src="Images/pnlp_0814.png" alt="Examples of memes [_42]" width="1440" height="601"/>
<h6><span class="label">Figure 8-14. </span>Examples of memes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588154824-marker" href="ch08.xhtml#idm45969588154824">35</a>]</h6>
</div></figure>
<p>Before we cover some key methods for understanding the trends in memes, let’s discuss why it’s important to understand these trends. Misuse of trolling memes in a live feed of a professional network platform like LinkedIn<a contenteditable="false" data-type="indexterm" data-primary="LinkedIn" id="idm45969588152760"/> may not be desirable. This is similar to groups on Facebook or Google that intend to spread awareness or information related to official processes or group events (e.g., a Facebook page for a fundraiser event or a Google group for helping students applying to their graduate school). Identifying content that could be a meme that’s heckling others or is otherwise offensive or violating other group or platform rules is important. There are two primary ways in which a meme could be identified:</p>
<dl>
<dt>Content-based</dt>
<dd>Content-based meme identification uses content to match with other memes of similar patterns that have already been identified. For example, in a community, it has been identified that “This is Bill. Be like Bill” (<a data-type="xref" href="#examples_of_memes_left_square_bracket_4">Figure 8-14</a>) has emerged as a meme. To identify if a new post belongs to the same template, we can extract the text and use a similarity metric like Jaccard distance to identify problematic content. In this way, it’s possible to identify memes of this pattern: “This is PersonX. Be like PersonX.” In our running example, even a regular expression would be able to identify such templates from a new post.</dd>
<dt>Behavior-based</dt>
<dd>Behavior-based meme identification is done mainly using the activity on the post. Studies have shown that the sharing behavior of a meme changes drastically from its inception to later hours. Usually, viral content can be identified by analyzing the number of shares, comments, likes for a particular post. In general, these numbers often go beyond the average metrics for other non-meme posts. This is more in the realm of anomaly detection. An interested reader can read the survey of such methods studied extensively on the Facebook network [<a data-type="noteref" href="ch08.xhtml#footnote_8_28">34</a>].</dd>
</dl>
<p>Now that we’ve discussed the basic definition of memes in the context of social media and briefly touched on how to identify or measure their effects, we’ll now move to another important and pressing issue in social media: fake<a contenteditable="false" data-type="indexterm" data-primary="memes" data-startref="ch08_term36" id="idm45969588145112"/> news.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Fake News"><div class="sect2" id="fake_news">
<h2>Fake News</h2>
<p><a contenteditable="false" data-type="indexterm" data-primary="fake news" id="ch08_term39"/>In the last few years, fake news on social platforms has become a huge issue. The number of incidents related to fake news has risen significantly along with the rise in users on social platforms. This consists of users both creating fake content and constantly sharing it on social networks to make it viral. In this section, we’ll take a look at how we can detect fake news using the NLP techniques we’ve learned so far.</p>
<p>Let’s look at an example of such fake news: “Lottery Winner Arrested for Dumping $200,000 of Manure on Ex-Boss’ Lawn.” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588139320-marker" href="ch08.xhtml#idm45969588139320">36</a>] This got over 2.3 million Facebook shares in 2018 [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588137544-marker" href="ch08.xhtml#idm45969588137544">37</a>].</p>
<p>Various media houses and content moderators are actively working on detecting and weeding out such fake news. There are some principled approaches that can be used to tackle this menace:</p>
<ol>
<li>
<p><em>Fact verification<a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="fact-verification with" id="ch08_term41"/><a contenteditable="false" data-type="indexterm" data-primary="fact-verification" id="ch08_term40"/> using external data sources</em>: Fact verification deals with validating various facts in a news article. It can be treated as a language understanding task where, given a sentence and a set of facts, a system needs to find out if the set of facts supports the claim or not.</p>
<p>Consider we have access to external data sources, such as Wikipedia<a contenteditable="false" data-type="indexterm" data-primary="Wikipedia" id="idm45969588129448"/>, where we assume the facts have been entered correctly. Now, given a piece of news text, such as, “Einstein was born in 2000,” we should be able to verify it using data sources consisting of facts. Note that, at the beginning, we don’t know which piece of information could be wrong, so this cannot be solved trivially just by pattern matching.</p>
<p>Amazon Research<a contenteditable="false" data-type="indexterm" data-primary="Amazon Research" id="idm45969588127352"/> at Cambridge created a curated dataset to deal with such cases of misinformation<a contenteditable="false" data-type="indexterm" data-primary="misinformation" data-seealso="fake news" id="idm45969588126088"/> present in natural text [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588124584-marker" href="ch08.xhtml#idm45969588124584">38</a>]. The dataset consists of examples that look like:</p>
<pre data-type="programlisting">{
    "id": 78526,
    "label": "REFUTES",
    "claim": "Lorelai Gilmore's father is named Robert.",
    "attack": "Entity replacement",
    "evidence": [
        [
            [&lt;annotation_id&gt;, &lt;evidence_id&gt;, "Lorelai_Gilmore", 3]
        ]
    ]
}</pre>
<p>As you might be able to see, we can develop a model that takes the <code>{claim, evidence}</code> as an input and produces the label <code>REFUTES</code>. This is more of a classification task with three labels: <code>AGREES</code>, <code>REFUTES</code>, and <code>NONE</code>. The evidence set contains the Wikipedia URL of the related entities of the sentence, and 3 denotes the sentence that has the correct fact in the corresponding Wikipedia article.</p>
<p>A similar dataset could be built by individual media houses to extract knowledge from existing articles related to their domains. For example, a sports news company might build a set primarily containing facts related to sports.</p>
<p>We can use BoW-based<a contenteditable="false" data-type="indexterm" data-primary="bag of words (BoW)" id="idm45969588117720"/><a contenteditable="false" data-type="indexterm" data-primary="BoW (bag of words)" id="idm45969588116616"/> methods to represent both the claim and the evidence and pass them as a pair through a logistic regression to obtain a classification label. More advanced techniques include using DL methods<a contenteditable="false" data-type="indexterm" data-primary="deep learning (DL)" data-secondary="fact-verification with" id="idm45969588115144"/> such as LSTM<a contenteditable="false" data-type="indexterm" data-primary="long short-term memory networks (LSTMs)" id="idm45969588113640"/><a contenteditable="false" data-type="indexterm" data-primary="LSTMs (long short-term memory networks)" id="idm45969588112536"/> or pre-trained BERT<a contenteditable="false" data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-secondary="fact-verification with" id="idm45969588111432"/> to obtain encodings of these inputs. We can then concatenate these embeddings and pass it to a neural network to classify the claim. An interested reader can look at [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588109688-marker" href="ch08.xhtml#idm45969588109688">39</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588108296-marker" href="ch08.xhtml#idm45969588108296">40</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588107016-marker" href="ch08.xhtml#idm45969588107016">41</a>].</p>
</li>
<li><p><a contenteditable="false" data-type="indexterm" data-primary="news classification" id="idm45969588105304"/><em>Classifying fake versus real</em>: A simple setup for this problem would be to build a parallel data corpus with instances of fake and real news excerpts and classify them as real or fake. While the setup is simple, it could be very hard for a machine to solve this task reasonably well due to the fact that people may use various linguistic nuances to confuse the machine in flagging fake content.</p>
<p>Researchers from Harvard<a contenteditable="false" data-type="indexterm" data-primary="Harvard University" id="idm45969588102664"/> recently developed a system [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_33-marker" href="ch08.xhtml#footnote_8_33">42</a>] to identify which text is written by humans and which text is generated by machines (and therefore could be fake). This system uses statistical methods to understand the facts and uses the fact that, when generating text, machines tend to use generic and common words. This is different from humans, who tend to use words that are more specific and adhere to an individual’s writing style. Their methods show that there could often be a clear distinction in the statistical properties of word usage that can be used to distinguish fake text from real text. We encourage readers to look into the work of Sebastian Gehrmann<a contenteditable="false" data-type="indexterm" data-primary="Gehrmann, Sebastian" id="idm45969588099080"/> et al. [<a data-type="noteref" href="ch08.xhtml#footnote_8_33">42</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969588097048-marker" href="ch08.xhtml#idm45969588097048">43</a>] for a complete understanding of the method.</p>
<p>A similar technique was used by the AllenNLP<a contenteditable="false" data-type="indexterm" data-primary="AllenNLP" data-secondary="Grover" id="idm45969588095032"/><a contenteditable="false" data-type="indexterm" data-primary="Grover (AllenNLP)" id="idm45969588093656"/> team to develop a tool called Grover [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_8_32-marker" href="ch08.xhtml#footnote_8_32">44</a>], which uses an ML model to generate text that looks human-written. They exploit the nuances present in the text generated to understand the quirks and attributes, which can then be used to build a system that helps in detecting potentially fake, machine-generated articles. We encourage you to play with the demo [<a data-type="noteref" href="ch08.xhtml#footnote_8_32">44</a>] that’s been open sourced by the team to understand its mechanism.</p>
</li>
</ol>
<p>We discussed two critical issues in social media—memes and fake news—and provided a quick survey of how to detect them. We also discussed how we can pose these problems as a simple natural language understanding task (such as classification) and what a potential dataset to solve that task might look like. This section should give you a good starting point to build systems that can identify malicious or fake content present in social<a contenteditable="false" data-type="indexterm" data-primary="fake news" data-startref="ch08_term39" id="idm45969588088264"/> media.</p>

</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Wrapping Up"><div class="sect1" id="wrapping_up-id00082">
<h1>Wrapping Up</h1>
<p>In this chapter, we started with an overview of the various applications of NLP in social media and discussed some of the unique challenges social media text data poses to traditional NLP methods. We then took a detailed look at different NLP applications, such as building word clouds, detecting trending topics on Twitter, understanding tweet sentiment, customer support on social media, and detecting memes and fake news. We also saw a range of text processing issues we might encounter while developing these tools and how to solve them. We hope this gave you a good understanding of how to apply NLP techniques on SMTD and solve an NLP problem dealing with social media text data you may encounter in your workplace. Let’s now move on to the next chapter, where we’ll address another vertical where NLP has proven to be very useful: e-commerce.<a contenteditable="false" data-type="indexterm" data-primary="social media" data-startref="ch08_term1" id="idm45969588084168"/></p>
</div></section>
<div data-type="footnotes"><h5>Footnotes</h5></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589902824">[<a href="ch08.xhtml#idm45969589902824-marker">1</a>] Twitter. <a href="https://oreil.ly/RasvL">Quarterly results: 2019 Fourth quarter</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589901320">[<a href="ch08.xhtml#idm45969589901320-marker">2</a>] Internet Live Stats. <a href="https://oreil.ly/Tx2U7">“Twitter Usage Statistics”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589899848">[<a href="ch08.xhtml#idm45969589899848-marker">3</a>] Zephoria Digital Marketing. <a href="https://oreil.ly/f3LTg">“The Top 20 Valuable Facebook Statistics–Updated May 2020”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589893784">[<a href="ch08.xhtml#idm45969589893784-marker">4</a>] Lewis, Lori. <a href="https://oreil.ly/YVU3C">“This Is What Happens In An Internet Minute”</a>. March 5, 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_4">[<a href="ch08.xhtml#footnote_8_4-marker">5</a>] Choudhury, Monojit. <a href="https://oreil.ly/CbIUH">“CS60017 - Social Computing, Indian Institute of Technology Kharagpur, Lecture 1: NLP for Social Media: What, Why and How?”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589817560">[<a href="ch08.xhtml#idm45969589817560-marker">6</a>] Ritter, Alan, Sam Clark, and Oren Etzioni. “Named Entity Recognition in Tweets: An Experimental Study.” <em>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</em> (2011): 1524–1534.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589813720">[<a href="ch08.xhtml#idm45969589813720-marker">7</a>] Barman, Utsab, Amitava Das, Joachim Wagner, and Jennifer Foster. “Code Mixing: A Challenge for Language Identification in the Language of Social Media.” <em>Proceedings of the First Workshop on Computational Approaches to Code Switching</em> (2014): 13–23.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_14">[<a href="ch08.xhtml#footnote_8_14-marker">8</a>] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and <a href="https://oreil.ly/P7c_a">Navaneethan Santhanam</a>. <a href="https://oreil.ly/39r6_">“Continuous Learning Systems: Building ML systems that learn from their mistakes”</a>. <em>Open Data Science Conference</em> (2019).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589732152">[<a href="ch08.xhtml#idm45969589732152-marker">9</a>] Mueller, Andreas. <a href="https://oreil.ly/7whtP">word_cloud: A little word cloud generator in Python</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589641928">[<a href="ch08.xhtml#idm45969589641928-marker">10</a>] Mueller, Andreas. <a href="https://oreil.ly/SyhSL">“Gallery of Examples”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_15">[<a href="ch08.xhtml#footnote_8_15-marker">11</a>] Ritter, Allen. <a href="https://oreil.ly/z8wWs">“Twokenize”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_16">[<a href="ch08.xhtml#footnote_8_16-marker">12</a>] Ritter, Allen. <a href="https://oreil.ly/QdtZq">“OSU Twitter NLP Tools”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589628792">[<a href="ch08.xhtml#idm45969589628792-marker">13</a>] Noah’s ARK lab. <a href="https://oreil.ly/xlhX-">“Tweet NLP”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589618264">[<a href="ch08.xhtml#idm45969589618264-marker">14</a>] Natural Language Toolkit. <a href="https://oreil.ly/g3P5x">TweetTokenizer</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589615624">[<a href="ch08.xhtml#idm45969589615624-marker">15</a>] Routar de Sousa, J. Guilherme. <a href="https://oreil.ly/TNRdM">Twikenizer</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589599096">[<a href="ch08.xhtml#idm45969589599096-marker">16</a>] <a href="https://oreil.ly/Fxn6S">Twitter’s Trending Topics</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589595400">[<a href="ch08.xhtml#idm45969589595400-marker">17</a>] Tweepy, <a href="http://www.tweepy.org">an easy-to-use Python library for accessing the Twitter API</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589354120">[<a href="ch08.xhtml#idm45969589354120-marker">18</a>] Twitter. <a href="https://oreil.ly/5-ojY">Enterprise Data: Unleash the Power of Twitter Data</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589336888">[<a href="ch08.xhtml#idm45969589336888-marker">19</a>] Wexler, Steve. <a href="https://oreil.ly/gQw6H">“How to Visualize Sentiment and Inclination”</a>. <em>Tableau (blog)</em>, January 14, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589328728">[<a href="ch08.xhtml#idm45969589328728-marker">20</a>] Kaggle. <a href="https://oreil.ly/7CoBZ">UMICH SI650—Sentiment Classification</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589324744">[<a href="ch08.xhtml#idm45969589324744-marker">21</a>] <a href="https://oreil.ly/ZlnRf">Sanders Twitter sentiment corpus</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_22">[<a href="ch08.xhtml#footnote_8_22-marker">22</a>] Loria, Steven. <a href="https://oreil.ly/18zLK">“TextBlob: Simple, Pythonic, text processing––Sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and more”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589283560">[<a href="ch08.xhtml#idm45969589283560-marker">23</a>] <a href="https://oreil.ly/4DpmK">Beautiful Soup</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969589045864">[<a href="ch08.xhtml#idm45969589045864-marker">24</a>] Solomon, Brad. <a href="https://oreil.ly/IJ643">Demoji</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588294440">[<a href="ch08.xhtml#idm45969588294440-marker">25</a>] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. <a href="https://oreil.ly/MMche">“GloVe: Global Vectors for Word Representation”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588292968">[<a href="ch08.xhtml#idm45969588292968-marker">26</a>] The Stanford Natural Language Procesisng Group. <a href="https://oreil.ly/WKYcd">“Pre-trained GloVe embeddings from Tweets”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588225624">[<a href="ch08.xhtml#idm45969588225624-marker">27</a>] Dhingra, Bhuwan, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William W. Cohen. <a href="https://oreil.ly/mQymq">“Tweet2Vec: Character-Based Distributed Representations for Social Media”</a>. (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588224168">[<a href="ch08.xhtml#idm45969588224168-marker">28</a>] Yang, Zhilin, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, and Ruslan Salakhutdinov. <a href="https://oreil.ly/0EQm1">“Words or Characters? Fine-grained Gating for Reading Comprehension”</a>. (2016).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588222680">[<a href="ch08.xhtml#idm45969588222680-marker">29</a>] Kuru, Onur, Ozan Arkan Can, and Deniz Yuret. “CharNER: Character-Level Named Entity Recognition.” <em>Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em> (2016): 911–921.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588221240">[<a href="ch08.xhtml#idm45969588221240-marker">30</a>] Godin, Fredric. <a href="https://oreil.ly/QuySb">“Twitter word embeddings”</a> and "<a href="https://oreil.ly/9cM4I">TwitterEmbeddings”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588210520">[<a href="ch08.xhtml#idm45969588210520-marker">31</a>] Lull, Travis. <a href="https://oreil.ly/Jsa6v">“Announcing new customer support features for businesses”</a>. <em>Twitter (blog)</em>, September 15, 2016; Facebook Help Center. <a href="https://oreil.ly/23UNN">“How does my Facebook Page get the ‘Very responsive to messages’ badge?”</a>; Facebook Help Center. <a href="https://oreil.ly/KRmGH">“How are response rate and response time defined for my Page?”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588200808">[<a href="ch08.xhtml#idm45969588200808-marker">32</a>] Apple’s and Bank of America’s support handles on Twitter: <a class="orm:hideurl" href="https://oreil.ly/kto4n"><em class="hyperlink">https://twitter.com/AppleSupport</em></a> and <a class="orm:hideurl" href="https://twitter.com/BofA_Help"><em class="hyperlink">https://twitter.com/BofA_Help</em></a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588164024">[<a href="ch08.xhtml#idm45969588164024-marker">33</a>] Rogers, Kara. <a href="https://oreil.ly/4J7g7">“Meme: Cultural Concept”</a>. <em>Encyclopedia Britannica</em>. Last modified March 5, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_28">[<a href="ch08.xhtml#footnote_8_28-marker">34</a>] Adamic, Lada A., Thomas M. Lento, Eytan Adar, and Pauline C. Ng. “Information Evolution in Social Networks.” <em>Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</em> (2016): 473–482.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588154824">[<a href="ch08.xhtml#idm45969588154824-marker">35</a>] <a href="https://oreil.ly/zpWRu">Popsugar Tech</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588139320">[<a href="ch08.xhtml#idm45969588139320-marker">36</a>] <a href="https://oreil.ly/mOsAp">“Lottery Winner Arrested for Dumping $200,000 of Manure on Ex-Boss’ Lawn”</a>. <em>World News Daily Report</em>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588137544">[<a href="ch08.xhtml#idm45969588137544-marker">37</a>] Silverman, Craig. <a href="https://oreil.ly/1SX-j">“Publishers Are Switching Domain Names to Try and Stay Ahead of Facebook’s Algorithm Changes”</a>. <em>BuzzFeed News</em>, March 1, 2018.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588124584">[<a href="ch08.xhtml#idm45969588124584-marker">38</a>] Thorne, James, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. <a href="https://oreil.ly/PCI0H">“FEVER: a large-scale dataset for Fact Extraction and VERification”</a>, (2018).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588109688">[<a href="ch08.xhtml#idm45969588109688-marker">39</a>] Hassan, Naeemul, Bill Adair, James T. Hamilton, Chengkai Li, Mark Tremayne, Jun Yang, and Cong Yu. “The Quest to Automate Fact Checking.” <em>Proceedings of the 2015 Computation+ Journalism Symposium</em> (2015).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588108296">[<a href="ch08.xhtml#idm45969588108296-marker">40</a>] Graves, Lucas. “Understanding the Promise and Limits of Automated Fact-Checking.” <em>Reuters Institute</em>, February 28, 2018.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588107016">[<a href="ch08.xhtml#idm45969588107016-marker">41</a>] Karadzhov, Georgi, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and Ivan Koychev. “Fully Automated Fact Checking Using External Sources.” <em>Proceedings of the International Conference Recent Advances in Natural Language Processing</em> (2017).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_33">[<a href="ch08.xhtml#footnote_8_33-marker">42</a>] Strobelt, Hendrik and Sebastian Gehrmann. <a href="http://gltr.io">“Catching a Unicorn with GLTR: A Tool to Detect Automatically Generated Text”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969588097048">[<a href="ch08.xhtml#idm45969588097048-marker">43</a>] Gehrmann, Sebastian, Hendrik Strobelt, and Alexander M. Rush. <a href="https://oreil.ly/vb1z7">“GLTR: Statistical Detection and Visualization of Generated Text”</a>, (2019).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_8_32">[<a href="ch08.xhtml#footnote_8_32-marker">44</a>] Allen Institute for AI. <a href="https://oreil.ly/0Ssr-">“Grover: A State-of-the-Art Defense against Neural Fake News”</a>. Last accessed June 15, 2020.</p></div></div></section></div>



  </body>
</html>