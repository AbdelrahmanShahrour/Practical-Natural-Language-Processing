<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />
<style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1&gt;p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1&gt;p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]&gt;div&gt;h1,#sbo-rt-content section[data-type="preface"]&gt;div&gt;h1,#sbo-rt-content section[data-type="appendix"]&gt;div&gt;h1,#sbo-rt-content section[data-type="glossary"]&gt;div&gt;h1,#sbo-rt-content section[data-type="bibliography"]&gt;div&gt;h1,#sbo-rt-content section[data-type="index"]&gt;div&gt;h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000;padding-top:.25em !important;margin-top:0 !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]&gt;div&gt;h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]&gt;div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dl{margin-bottom:1.5em !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important;line-height:1.25rem;font-style:italic}#sbo-rt-content dd{margin:10px 0 .25em 1.5em !important;line-height:1.65em !important}#sbo-rt-content dd p{padding:0 !important;margin:0 0 10px !important}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul&gt;li,#sbo-rt-content ol ul,#sbo-rt-content ol ul&gt;li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul&gt;li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul&gt;li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul&gt;li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol&gt;li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol&gt;li,#sbo-rt-content ul ol,#sbo-rt-content ul ol&gt;li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol&gt;li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol&gt;li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol&gt;li&gt;ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol&gt;li&gt;ol&gt;li&gt;ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content table li{margin:10px 0 0 .25em !important}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:top;font-size:80%}#sbo-rt-content th{vertical-align:bottom}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller;word-break:break-all}#sbo-rt-content table.border tbody&gt;tr:last-child&gt;td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:2em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content pre.break-code,#sbo-rt-content code.break-code,#sbo-rt-content .break-code pre,#sbo-rt-content .break-code code{word-break:break-all}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10,#sbo-rt-content figure.width-10 img{width:10% !important}#sbo-rt-content .width-20,#sbo-rt-content figure.width-20 img{width:20% !important}#sbo-rt-content .width-30,#sbo-rt-content figure.width-30 img{width:30% !important}#sbo-rt-content .width-40,#sbo-rt-content figure.width-40 img{width:40% !important}#sbo-rt-content .width-50,#sbo-rt-content figure.width-50 img{width:50% !important}#sbo-rt-content .width-60,#sbo-rt-content figure.width-60 img{width:60% !important}#sbo-rt-content .width-70,#sbo-rt-content figure.width-70 img{width:70% !important}#sbo-rt-content .width-80,#sbo-rt-content figure.width-80 img{width:80% !important}#sbo-rt-content .width-90,#sbo-rt-content figure.width-90 img{width:90% !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100% !important}#sbo-rt-content .sc{text-transform:none !important}#sbo-rt-content .right{float:none !important}#sbo-rt-content a.totri-footnote{padding:0 !important}#sbo-rt-content figure.width-10,#sbo-rt-content figure.width-20,#sbo-rt-content figure.width-30,#sbo-rt-content figure.width-40,#sbo-rt-content figure.width-50,#sbo-rt-content figure.width-60,#sbo-rt-content figure.width-70,#sbo-rt-content figure.width-80,#sbo-rt-content figure.width-90{width:auto !important}#sbo-rt-content p img,#sbo-rt-content pre img{width:1.25em;line-height:1em;margin:0 .15em -.2em}#sbo-rt-content figure.no-frame div.border-box{border:none}#sbo-rt-content .right{text-align:right !important}
    </style>
<style type="text/css" id="font-styles">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }</style>
<style type="text/css" id="font-family">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }</style>
<style type="text/css" id="column-width">#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }</style>

<style type="text/css">body{margin:1em;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}body{background-color:transparent!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Text Representation"><div class="chapter" id="text_representation">
<h1><span class="label">Chapter 3. </span>Text Representation</h1>

<blockquote class="right">
<p class="right"><em>In language processing,</em><br/> <em>the vectors x are derived from textual data,</em><br/> <span class="keep-together"><em>in order to reflect various linguistic properties of the text.</em></span></p>
<p data-type="attribution" style="text-align:right"><em>Yoav Goldberg</em></p>
</blockquote>

<p><a contenteditable="false" data-primary="feature extraction" data-type="indexterm" id="idm45969609812776"/>Feature extraction<a contenteditable="false" data-primary="Goldberg, Yoav" data-type="indexterm" id="idm45969609811544"/> is an important step for any machine learning problem. No matter how good a modeling algorithm you use, if you feed in poor features, you will get poor results. In computer science, this is often called “garbage in, garbage out.” In the previous two chapters, we saw an overview of NLP, the different tasks and challenges involved, and what a typical NLP pipeline looks like. In this chapter, we’ll address the question: how do we go about doing feature engineering for text data? In other words, how do we transform a given text into numerical form so that it can be fed into NLP and ML algorithms? In NLP parlance, this conversion of raw text to a suitable numerical form is called <em>text representation<a contenteditable="false" data-primary="text representation" data-type="indexterm" id="ch04_term1"/></em>. In this chapter, we’ll take a look at the different methods for text representation, or representing text as a numeric vector. With respect to the larger picture for any NLP problem, the scope of this chapter is depicted by the dotted box in <a data-type="xref" href="#scope_of_this_chapter_within_the_nlp_pi">Figure 3-1</a>.</p>

<figure><div id="scope_of_this_chapter_within_the_nlp_pi" class="figure"><img alt="Scope of this chapter within the NLP pipeline" src="Images/pnlp_0301.png" width="1075" height="425"/>
<h6><span class="label">Figure 3-1. </span>Scope of this chapter within the NLP pipeline</h6>
</div></figure>

<p>Feature representation is a common step in any ML project, whether the data is text, images, videos, or speech. However, feature representation<a contenteditable="false" data-primary="feature representation" data-type="indexterm" id="idm45969609803864"/> for text is often much more involved as compared to other formats of data. To understand this, let’s look at a few examples of how other data formats can be represented numerically. First, consider the case of images. Say we want to build a classifier that can distinguish images of cats from images of dogs. Now, in order to train an ML model to accomplish this task, we need to feed it (labeled) images. How do we feed images<a contenteditable="false" data-primary="image representation" data-type="indexterm" id="idm45969609802184"/> to an ML model? The way an image is stored in a computer is in the form of a matrix of pixels where each <code>cell[i,j]</code> in the matrix represents pixel i,j of the image. The real value stored at <code>cell[i,j]</code> represents the intensity of the corresponding pixel in the image, as shown in <a data-type="xref" href="#how_we_see_an_image_versus_how_computer">Figure 3-2</a>. This matrix representation accurately represents the complete image. Video is similar: a video is just a collection of frames where each frame is an image. Hence, any video<a contenteditable="false" data-primary="video representation" data-type="indexterm" id="idm45969609798744"/> can be represented as a sequential collection of matrices, one per frame, in the same order.</p>

<figure><div id="how_we_see_an_image_versus_how_computer" class="figure"><img alt="How we see an image versus how computers see it [_1]" src="Images/pnlp_0302.png" width="1440" height="601"/>
<h6><span class="label">Figure 3-2. </span>How we see an image versus how computers see it [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609795816-marker" href="ch03.xhtml#idm45969609795816">1</a>]</h6>
</div></figure>

<p>Now consider speech<a contenteditable="false" data-primary="speech representation" data-type="indexterm" id="idm45969609793800"/>—it’s transmitted as a wave. To represent it mathematically, we sample the wave and record its amplitude (height), as shown in <a data-type="xref" href="#sampling_a_speech_wave">Figure 3-3</a>.</p>

<figure><div id="sampling_a_speech_wave" class="figure"><img alt="Sampling a speech wave" src="Images/pnlp_0303.png" width="1390" height="408"/>
<h6><span class="label">Figure 3-3. </span>Sampling a speech wave</h6>
</div></figure>

<p>This gives us a numerical array representing the amplitude of a sound wave at fixed time intervals, as shown in <a data-type="xref" href="#speech_signal_represented_by_a_numerica">Figure 3-4</a>.</p>

<figure><div id="speech_signal_represented_by_a_numerica" class="figure"><img alt="Speech signal represented by a numerical vector" src="Images/pnlp_0304.png" width="1403" height="311"/>
<h6><span class="label">Figure 3-4. </span>Speech signal represented by a numerical vector</h6>
</div></figure>

<p>From this discussion, it’s clear that mathematically representing images, video, and speech is straightforward. What about text? It turns out that representing text is not straightforward, hence a whole chapter focusing on various schemes to address this question. We’re given a piece of text, and we’re asked to find a scheme to represent it mathematically. In literature, this is called <em>text representation</em>. Text representation has been an active area of research in the past decades, especially the last one. In this chapter, we’ll start with simple approaches and go all the way to state-of-the-art techniques for representing text. These approaches are classified into four categories:</p>

<ul>
	<li>
	<p>Basic vectorization approaches</p>
	</li>
	<li>
	<p>Distributed representations</p>
	</li>
	<li>
	<p>Universal language representation</p>
	</li>
	<li>
	<p>Handcrafted features</p>
	</li>
</ul>

<p>The rest of this chapter describes these categories one by one, covering various algorithms in each. Before we delve deeper into various schemes, consider the following scenario: we’re given a labeled text corpus and asked to build a sentiment analysis model. To correctly predict the sentiment of a sentence, the model needs to understand the meaning of the sentence. In order to correctly extract the meaning of the sentence, the most crucial data points are:</p>

<ol>
	<li>
	<p>Break the sentence into lexical units such as lexemes, words, and phrases</p>
	</li>
	<li>
	<p>Derive the meaning for each of the lexical units</p>
	</li>
	<li>
	<p>Understand the syntactic (grammatical) structure of the sentence</p>
	</li>
	<li>
	<p>Understand the context in which the sentence appears</p>
	</li>
</ol>

<p>The <em>semantics<a contenteditable="false" data-primary="semantics" data-type="indexterm" id="idm45969609774792"/></em> (meaning) of the sentence arises from the combination of the above points. Thus, any <em>good</em> text representation scheme must facilitate the extraction of those data points in the best possible way to reflect the linguistic properties of the text. Without this, a text representation scheme isn’t of much use.</p>

<div data-type="tip"><h6>Tip</h6>
<p>Often in NLP, feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a top-notch algorithm to an ordinary text representation.</p>
</div>

<p>Let’s take a look at a key concept that carries throughout this entire chapter: the vector space model.</p>

<section data-type="sect1" data-pdf-bookmark="Vector Space Models"><div class="sect1" id="vector_space_models">
<h1>Vector Space Models</h1>

<p>It should be clear from<a contenteditable="false" data-primary="vector space models (VSMs)" data-type="indexterm" id="ch04_term2"/><a contenteditable="false" data-primary="VSMs (vector space models)" data-type="indexterm" id="ch04_term3"/> the introduction that, in order for ML algorithms to work with text data, the text data must be converted into some mathematical form. Throughout this chapter, we’ll represent text units (characters, phonemes, words, phrases, sentences, paragraphs, and documents) with vectors of numbers. This is known as the <em>vector space model</em> (VSM).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="ch03fn1-marker" href="ch03.xhtml#ch03fn1">i</a></sup> It’s a simple algebraic model used extensively for representing any text blob. VSM is fundamental to many information-retrieval operations, from scoring documents on a query to document classification and document clustering [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609761560-marker" href="ch03.xhtml#idm45969609761560">2</a>]. It’s a mathematical model that represents text units as <a href="https://oreil.ly/mtyKw">vectors</a>. In the simplest form, these are vectors of identifiers, such as index numbers in a corpus vocabulary. In this setting, the most common way to calculate similarity between two text blobs is using cosine similarity: the cosine of the angle between their <span class="keep-together">corresponding</span> vectors. The cosine of 0° is 1 and the cosine of 180° is –1, with the cosine monotonically decreasing from 0° to 180°. Given two vectors, A and B, each with <em>n</em> components, the similarity between them is computed as follows:</p>

<div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>similarity</mtext><mo>=</mo><mi>cos</mi><mfenced><mi>θ</mi></mfenced><mo>=</mo><mfrac><mrow><mi mathvariant="bold">A</mi><mo>·</mo><mi mathvariant="bold">B</mi></mrow><mrow><msub><mfenced close="||" open="||"><mi mathvariant="bold">A</mi></mfenced><mn>2</mn></msub><msub><mfenced close="||" open="||"><mi mathvariant="bold">B</mi></mfenced><mn>2</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><msub><mi>A</mi><mi>i</mi></msub><msub><mi>B</mi><mi>i</mi></msub></mrow><mrow><msqrt><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><msubsup><mo largeop="true">A</mo><mi>i</mi><mn>2</mn></msubsup></msqrt><msqrt><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><msubsup><mo largeop="true">B</mo><mi>i</mi><mn>2</mn></msubsup></msqrt></mrow></mfrac></math></div>

<p>where <em>A<sub>i</sub></em> and <em>B<sub>i</sub></em> are the <em>i<sup>th</sup></em> <a href="https://oreil.ly/7tNTM">components</a> of vectors A and B, respectively. Sometimes, people also use Euclidean distance between vectors to capture similarity.</p>

<p>All the text representation schemes we’ll study in this chapter fall within the scope of vector space models. What differentiates one scheme from another is how well the resulting vector captures the linguistic properties of the text it represents. With this, we’re ready to discuss various text representation schemes.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Basic Vectorization Approaches"><div class="sect1" id="basic_vectorization_approaches">
<h1>Basic Vectorization Approaches</h1>

<p><a contenteditable="false" data-primary="text representation" data-secondary="basic vectorization approaches" data-type="indexterm" id="ch04_term4"/>Let’s start with a basic idea of text representation: map each word in the vocabulary (V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector. How do we operationalize this idea? To understand this better, let’s take a toy corpus (shown in <a data-type="xref" href="#our_toy_corpus">Table 3-1</a>) with only four documents—D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, D<sub>4</sub> —as an example.</p>

<table class="border" id="our_toy_corpus">
	<caption><span class="label">Table 3-1. </span>Our toy corpus</caption>
		<tr>
			<td>D1</td>
			<td>Dog bites man.</td>
		</tr>
		<tr>
			<td>D2</td>
			<td>Man bites dog.</td>
		</tr>
		<tr>
			<td>D3</td>
			<td>Dog eats meat.</td>
		</tr>
		<tr>
			<td>D4</td>
			<td>Man eats food.</td>
		</tr>
</table>

<p>Lowercasing text and ignoring punctuation, the vocabulary of this corpus is comprised of six words: [dog, bites, man, eats, meat, food]. We can organize the vocabulary in any order. In this example, we simply take the order in which the words appear in the corpus. Every document in this corpus can now be represented with a vector of size six. We’ll discuss multiple ways in which we can do this. We’ll assume that the text is already pre-processed (lowercased, punctuation removed, etc.) and tokenized (text string split into tokens), following the pre-processing step in the NLP pipeline described in <a data-type="xref" href="ch02.xhtml#nlp_pipeline">Chapter 2</a>. We’ll start with one-hot encoding.</p>

<section data-type="sect2" data-pdf-bookmark="One-Hot Encoding"><div class="sect2" id="one_hot_encoding">
<h2>One-Hot Encoding</h2>

<p>In one-hot encoding<a contenteditable="false" data-primary="one-hot encoding" data-type="indexterm" id="ch04_term5"/>, each word <em>w</em> in the corpus vocabulary is given a unique integer ID <em>w</em><sub>id</sub> that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s barring the index, where index = <em>w</em><sub>id</sub>. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.</p>

<p>Let’s understand this via our toy corpus. We first map each of the six words to unique IDs: dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="ch03fn2-marker" href="ch03.xhtml#ch03fn2">ii</a></sup> Let’s consider the document D1: “dog bites man”. As per the scheme, each word is a six-dimensional vector. Dog is represented as [1 0 0 0 0 0], as the word “dog” is mapped to ID 1. Bites is represented as [0 1 0 0 0 0], and so on and so forth. Thus, D1 is represented as [ [1 0 0 0 0 0] [0 1 0 0 0 0] [0 0 1 0 0 0]]. D4 is represented as [ [ 0 0 1 0 0] [0 0 0 0 1 0] [0 0 0 0 0 1]]. Other documents in the corpus can be represented similarly.</p>

<p>Let’s look at a simple way to implement this in Python from first principles. The notebook <em>Ch3/OneHotEncoding.ipynb</em> demonstrates an example of this. The code that follows is borrowed from the notebook and implements one-hot encoding. In real-world projects, we mostly use scikit-learn’s implementation of one-hot encoding, which is much more optimized. We’ve provided the same in the notebook.</p> 

<p>Since we assume that the text is tokenized, we can just split the text on white space in this example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">get_onehot_vector</code><code class="p">(</code><code class="n">somestring</code><code class="p">):</code>
  <code class="n">onehot_encoded</code> <code class="o">=</code> <code class="p">[]</code>
  <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">somestring</code><code class="o">.</code><code class="n">split</code><code class="p">():</code>
             <code class="n">temp</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">*</code><code class="nb">len</code><code class="p">(</code><code class="n">vocab</code><code class="p">)</code>
             <code class="k">if</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">vocab</code><code class="p">:</code>
                        <code class="n">temp</code><code class="p">[</code><code class="n">vocab</code><code class="p">[</code><code class="n">word</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
             <code class="n">onehot_encoded</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">temp</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">onehot_encoded</code>

<code class="n">get_onehot_vector</code><code class="p">(</code><code class="n">processed_docs</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
</pre>

<pre data-type="programlisting">
Output: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]</pre>

<p>Now that we understand the scheme, let’s discuss some of its pros and cons. On the positive side, one-hot encoding is intuitive to understand and straightforward to implement. However, it suffers from a few shortcomings:</p>

<ul>
	<li>
	<p>The size of a one-hot vector is directly proportional to size of the vocabulary, and most real-world corpora have large vocabularies. This results in a sparse representation where most of the entries in the vectors are zeroes, making it computationally inefficient to store, compute with, and learn from (sparsity leads to overfitting).</p>
	</li>
	<li>
	<p>This representation does not give a fixed-length representation for text, i.e., if a text has 10 words, you get a longer representation for it as compared to a text with 5 words. For most learning algorithms, we need the feature vectors to be of the same length.</p>
	</li>
	<li>
	<p>It treats words as atomic units and has no notion of (dis)similarity between words. For example, consider three words: run, ran, and apple. Run and ran have similar meanings as opposed to run and apple. But if we take their respective vectors and compute Euclidean distance between them, they’re all equally apart (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msqrt><mn>2</mn></msqrt></math>). Thus, semantically, they’re very poor at capturing the meaning of the word in relation to other words.</p>
	</li>
	<li>
<p>Say we train a model using our toy corpus. At runtime, we get a sentence: “man eats fruits.” The training data didn’t include “fruit”  and there’s no way to represent it in our model. This is known as the <em>out of vocabulary (OOV)</em> problem. A <span class="keep-together">one-hot</span> encoding scheme cannot handle this. The only way is to retrain the model: start by expanding the vocabulary, give an ID<a contenteditable="false" data-primary="out of vocabulary (OOV) problem" data-type="indexterm" id="idm45969609643640"/><a contenteditable="false" data-primary="OOV (out of vocabulary) problem" data-type="indexterm" id="idm45969609642520"/> to the new word, etc.</p>
</li>
</ul>

<div data-type="tip"><h6>Tip</h6>
<p>These days, one-hot encoding scheme is seldom used.</p>
</div>

<p>Some of these shortcomings can be addressed by the bag-of-words approach described next<a contenteditable="false" data-primary="one-hot encoding" data-startref="ch04_term5" data-type="indexterm" id="idm45969609639432"/>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Bag of Words"><div class="sect2" id="bag_of_words">
<h2>Bag of Words</h2>

<p>Bag of words (BoW)<a contenteditable="false" data-primary="BoW (bag of words)" data-type="indexterm" id="ch04_term7"/><a contenteditable="false" data-primary="bag of words (BoW)" data-type="indexterm" id="ch04_term6"/> is a classical text representation technique that has been used commonly in NLP, especially in text classification problems (see <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>). The key idea behind it is as follows: represent the text under consideration as a <em>bag</em> (collection) <em>of words</em> while ignoring the order and context. The basic intuition behind it is that it assumes that the text belonging to a given class in the dataset is characterized by a unique set of words. If two text pieces have nearly the same words, then they belong to the same bag (class). Thus, by analyzing the words present in a piece of text, one can identify the class (bag) it belongs to.</p>

<p>Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and <span class="keep-together">|V|.</span> Each document in the corpus is then converted into a vector of |V| dimensions where in the <em>i<sup>th</sup></em> component of the vector, <em>i</em> = <em>w</em><sub>id</sub>, is simply the number of times the word <em>w</em> occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.</p>

<p>Thus, for our toy corpus (<a data-type="xref" href="#our_toy_corpus">Table 3-1</a>), where the word IDs are dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0 0 0]. This is because the first three words in the vocabulary appeared exactly once in D1, and the last three did not appear at all. D4 becomes [0 0 1 0 1 1]. The notebook <em>Ch3/Bag_of_Words.ipynb</em> demonstrates how we can implement BoW text representation. The following code shows the key parts:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>
<code class="n">count_vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code>

<code class="c1">#Build a BOW representation for the corpus</code>
<code class="n">bow_rep</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">processed_docs</code><code class="p">)</code>

<code class="c1">#Look at the vocabulary mapping</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Our vocabulary: "</code><code class="p">,</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">vocabulary_</code><code class="p">)</code>

<code class="c1">#See the BOW rep for first 2 documents</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"BoW representation for 'dog bites man': "</code><code class="p">,</code> <code class="n">bow_rep</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"BoW representation for 'man bites dog: "</code><code class="p">,</code><code class="n">bow_rep</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code>

<code class="c1">#Get the representation using this vocabulary, for a new text</code>
<code class="n">temp</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s2">"dog and dog are friends"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Bow representation for 'dog and dog are friends':"</code><code class="p">,</code> 

<code class="n">temp</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code></pre>

<p>If we run this code, we’ll notice that the BoW representation for a sentence like “dog and dog are friends” has a value of 2 for the dimension of the word “dog,” indicating its frequency in the text. Sometimes, we don’t care about the frequency of occurrence of words in text and we only want to represent whether a word exists in the text or not. Researchers have shown that such a representation without considering frequency is useful for sentiment analysis (see Chapter 4 in [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609621832-marker" href="ch03.xhtml#idm45969609621832">3</a>]). In such cases, we just initialize <code>CountVectorizer</code> with the <code>binary=True</code> option, as shown in the following code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">count_vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">binary</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">bow_rep_bin</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">processed_docs</code><code class="p">)</code>
<code class="n">temp</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s2">"dog and dog are friends"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Bow representation for 'dog and dog are friends':"</code><code class="p">,</code> <code class="n">temp</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code></pre>

<p>This results in a different representation for the same sentence. <code>CountVectorizer</code> supports both word as well as character n-grams.</p>

<p>Let’s look at some of the advantages of this encoding:</p>

<ul>
	<li>
	<p>Like one-hot encoding, BoW is fairly simple to understand and implement.</p>
	</li>
	<li>
	<p>With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words. The distance between D<sub>1</sub> and D<sub>2</sub> is 0 as compared to the distance between D<sub>1</sub> and D<sub>4</sub>, which is 2. Thus, the vector space resulting from the BoW scheme captures the semantic similarity of documents. So if two documents have similar vocabulary, they’ll be closer to each other in the vector space and vice versa.</p>
	</li>
	<li>
	<p>We have a fixed-length encoding for any sentence of arbitrary length.</p>
	</li>
</ul>

<p>However, it has its share of disadvantages, too:</p>

<ul>
	<li>
	<p>The size of the vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to <em>n</em> number of the most frequent words.</p>
	</li>
	<li>
	<p>It does not capture the similarity between different words that mean the same thing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of all three documents will be equally apart.</p>
	</li>
	<li>
	<p>This representation does not have any way to handle out of vocabulary words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).</p>
	</li>
	<li>
	<p>As the name indicates, it is a “bag” of words—word order information is lost in this representation. Both D1 and D2 will have the same representation in this scheme.</p>
	</li>
</ul>

<p>However, despite these shortcomings, due to its simplicity and ease of implementation, BoW is a commonly used text representation scheme, especially for text classification among other NLP<a contenteditable="false" data-primary="bag of words (BoW)" data-startref="ch04_term6" data-type="indexterm" id="idm45969609489016"/><a contenteditable="false" data-primary="BoW (bag of words)" data-startref="ch04_term7" data-type="indexterm" id="idm45969609487640"/> problems.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Bag of N-Grams"><div class="sect2" id="bag_of_n_grams">
<h2>Bag of N-Grams</h2>

<p><a contenteditable="false" data-primary="bag of n-grams (BoN)" data-type="indexterm" id="ch04_term8"/><a contenteditable="false" data-primary="BoN (bag of n-grams)" data-type="indexterm" id="ch04_term9"/>All the representation schemes we’ve seen so far treat words as independent units. There is no notion of phrases or word ordering. The <em>bag-of-n-grams</em> (BoN) approach tries to remedy this. It does so by breaking text into chunks of <em>n</em> contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an <em>n-gram<a contenteditable="false" data-primary="n-grams" data-type="indexterm" id="idm45969609480312"/></em>. The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.</p>

<p>To elaborate, let’s consider our example corpus. Let’s construct a 2-gram (a.k.a. bigram) model for it. The set of all bigrams in the corpus is as follows: {dog bites, bites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. Then, BoN representation consists of an eight-dimensional vector for each document. The bigram representation for the first two documents is as follows: D<sub>1</sub> : [1,1,0,0,0,0,0,0], D<sub>2</sub> : [0,0,1,1,0,0,0,0]. The other two documents follow similarly. Note that the BoW scheme is a special case of the BoN scheme, with <em>n</em>=1. <em>n</em>=2 is called a “bigram model,” and <em>n</em>=3 is called a “trigram model.” Further, note that, by increasing the value of <em>n</em>, we can incorporate larger context; however, this further increases the sparsity. In NLP parlance, the BoN scheme is also called “n-gram feature selection.”</p>

<p>The following code (<em>Ch3/Bag_of_N_Grams.ipynb</em>) shows an example of a BoN representation considering 1–3 n-gram word features to represent the corpus that we’ve used so far. Here, we use unigram, bigram, and trigram vectors by setting <code>ngram_range = (1,3)</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#n-gram vectorization example with count vectorizer and uni, bi, trigrams</code>
<code class="n">count_vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">3</code><code class="p">))</code>

<code class="c1">#Build a BOW representation for the corpus</code>
<code class="n">bow_rep</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">processed_docs</code><code class="p">)</code>

<code class="c1">#Look at the vocabulary mapping</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Our vocabulary: "</code><code class="p">,</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">vocabulary_</code><code class="p">)</code>

<code class="c1">#Get the representation using this vocabulary, for a new text</code>
<code class="n">temp</code> <code class="o">=</code> <code class="n">count_vect</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s2">"dog and dog are friends"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Bow representation for 'dog and dog are friends':"</code><code class="p">,</code> <code class="n">temp</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code>
</pre>

<p>Here are the main pros and cons of BoN<a contenteditable="false" data-primary="bag of n-grams (BoN)" data-startref="ch04_term8" data-type="indexterm" id="idm45969609366232"/><a contenteditable="false" data-primary="BoN (bag of n-grams)" data-startref="ch04_term9" data-type="indexterm" id="idm45969609346072"/>:</p>

<ul>
	<li>
	<p>It captures some context and word-order information in the form of n-grams.</p>
	</li>
	<li>
	<p>Thus, resulting vector space is able to capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.</p>
	</li>
	<li>
	<p>As <em>n</em> increases, dimensionality (and therefore sparsity) only increases rapidly.</p>
	</li>
	<li>
	<p>It still provides no way to address the OOV problem.</p>
	</li>
</ul>
</div></section>

<section data-type="sect2" data-pdf-bookmark="TF-IDF"><div class="sect2" id="tf_idf">
<h2>TF-IDF</h2>

<p><a contenteditable="false" data-primary="TF-IDF (Term Frequency–Inverse Document Frequency)" data-type="indexterm" id="ch04_term10"/>In all the three approaches we’ve seen so far, all the words in the text are treated as equally important—there’s no notion of some words in the document being more important than<a contenteditable="false" data-primary="Term Frequency–Inverse Document Frequency" data-see="TF-IDF" data-type="indexterm" id="idm45969609335848"/> others. TF-IDF, or <em>term frequency–inverse document frequency</em>, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It’s a commonly used representation scheme for information-retrieval systems, for extracting relevant documents from a corpus for a given text query.</p>

<p>The intuition behind TF-IDF is as follows: if a word <em>w</em> appears many times in a document <em>d</em><sub>i</sub> but does not occur much in the rest of the documents <em>d</em><sub>j</sub> in the corpus, then the word <em>w</em> must be of great importance to the document <em>d</em><sub>i</sub>. The importance of <em>w</em> should increase in proportion to its frequency in <em>d</em><sub>i</sub>, but at the same time, its importance should decrease in proportion to the word’s frequency in other documents <em>d</em><sub>j</sub> in the corpus. Mathematically, this is captured using two quantities: TF and IDF. The two are then combined to arrive at the <em>TF-IDF score</em>.</p>

<p><em>TF (term frequency)<a contenteditable="false" data-primary="TF (term frequency)" data-type="indexterm" id="idm45969609326760"/></em> measures how often a term or word occurs in a given document. Since different documents in the corpus may be of different lengths, a term may occur more often in a longer document as compared to a shorter document. To normalize these counts, we divide the number of occurrences by the length of the document. <em>TF</em> of a term <em>t</em> in a document <em>d</em> is defined as:</p>

<div data-type="equation">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
<mi>TF</mi>
<mo>(</mo>
<mi>t</mi>
<mo>,</mo>
<mo> </mo>
<mi>d</mi>
<mo>)</mo>
<mo>=</mo>
<mfrac>
<mrow>
<mo>(</mo>
<mtext>Number of occurrences of term </mtext>
<mi>t</mi>
<mtext> in document </mtext>
<mi>d</mi>
<mo>)</mo>
</mrow>
<mrow>
<mo>(</mo>
<mtext>Total number of terms in the document </mtext>
<mi>d</mi>
<mo>)</mo>
</mrow>
</mfrac>
</math>
</div>

<p><em>IDF (inverse document frequency)<a contenteditable="false" data-primary="IDF (inverse document frequency)" data-type="indexterm" id="idm45969609312184"/></em> measures the importance of the term across a corpus. In computing TF, all terms are given equal importance (weightage). However, it’s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term <em>t</em> is calculated as follows:</p>

<div data-type="equation">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
<mi>IDF</mi>
<mo>(</mo>
<mi>t</mi>
<mo>)</mo>
<mo>=</mo>
<msub><mi>log</mi><mi>e</mi></msub>
<mfrac>
<mrow>
<mo>(</mo>
<mtext>Total number of documents in the corpus</mtext>
<mo>)</mo>
</mrow>
<mrow>
<mo>(</mo>
<mtext>Number of documents with term </mtext>
<mi>t</mi> 
<mtext> in them </mtext>
<mo>)</mo>
</mrow>
</mfrac>
</math>
</div>

<p>The TF-IDF score is a product of these two terms. Thus, <em>TF-IDF score = TF * IDF</em>. Let’s compute TF-IDF scores for our toy corpus. Some terms appear in only one document, some appear in two, while others appear in three documents. The size of our corpus is N=4. Hence, corresponding TF-IDF values for each term are shown in <a data-type="xref" href="#tf_idf_values_for_our_toy_corpus">Table 3-2</a>.</p>

<table class="border" id="tf_idf_values_for_our_toy_corpus">
	<caption><span class="label">Table 3-2. </span>TF-IDF values for our toy corpus</caption>
	<thead>
		<tr>
			<th>Word</th>
			<th>TF score</th>
			<th>IDF score</th>
			<th>TF-IDF score</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>dog</td>
			<td>⅓ = 0.33</td>
			<td>log<sub>2</sub>(4/3) = 0.4114</td>
			<td>0.4114 * 0.33 = 0.136</td>
		</tr>
		<tr>
			<td>bites</td>
			<td>⅙ = 0.17</td>
			<td>log<sub>2</sub>(4/2) = 1</td>
			<td>1* 0.17 = 0.17</td>
		</tr>
		<tr>
			<td>man</td>
			<td>0.33</td>
			<td>log<sub>2</sub>(4/3) =0.4114</td>
			<td>0.4114 * 0.33 = 0.136</td>
		</tr>
		<tr>
			<td>eats</td>
			<td>0.17</td>
			<td>log<sub>2</sub>(4/2) =1</td>
			<td>1* 0.17 = 0.17</td>
		</tr>
		<tr>
			<td>meat</td>
			<td>1/12 = 0.083</td>
			<td>log<sub>2</sub>(4/1) =2</td>
			<td>2* 0.083 = 0.17</td>
		</tr>
		<tr>
			<td>food</td>
			<td>0.083</td>
			<td>log<sub>2</sub>(4/1) =2</td>
			<td>2* 0.083 = 0.17</td>
		</tr>
	</tbody>
</table>

<p>The TF-IDF vector representation for a document is then simply the TF-IDF score for each term in that document. So, for D<sub>1</sub> we get </p>

<table class="border">
		<thead>
		<tr>
			<th>Dog</th>
			<th>bites</th>
			<th>man</th>
			<th>eats</th>
			<th>meat</th>
			<th>food</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>0.136</td>
			<td>0.17</td>
			<td>0.136</td>
			<td>0</td>
			<td>0</td>
			<td>0</td>
		</tr>
	</tbody>
</table>

<p>The following code (<em>Ch3/TF_IDF.ipynb</em>) shows how to use TF-IDF to represent text:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>

<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
<code class="n">bow_rep_tfidf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">processed_docs</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">tfidf</code><code class="o">.</code><code class="n">idf_</code><code class="p">)</code> <code class="c1">#IDF for all words in the vocabulary</code>
<code class="k">print</code><code class="p">(</code><code class="n">tfidf</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code> <code class="c1">#All words in the vocabulary.</code>

<code class="n">temp</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s2">"dog and man are friends"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Tfidf representation for 'dog and man are friends':</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">temp</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code></pre>

<p>There are several variations of the basic TF-IDF formula that are used in practice. Notice that the TF-IDF scores that we calculated for our corpus in <a data-type="xref" href="#tf_idf_values_for_our_toy_corpus">Table 3-2</a> might not match the TF-IDF scores given by scikit-learn. This is because scikit-learn uses a slightly modified version of the IDF formula. This stems from provisions to account for possible zero divisions and to not entirely ignore terms that appear in all documents. An interested reader can look into the TF-IDF vectorizer documentation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609188840-marker" href="ch03.xhtml#idm45969609188840">4</a>] for the exact formula.</p>

<p>Similar to BoW, we can use the TF-IDF vectors to calculate similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information retrieval and text classification. However, despite the fact that TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, it still suffers from the curse of high dimensionality.</p>

<div data-type="tip"><h6>Tip</h6>
<p>Even today, TF-IDF continues to be a popular representation scheme for many NLP tasks, especially the initial versions of the solution.</p>
</div>

<p>If we look back at all the representation schemes we’ve discussed so far, we notice three fundamental drawbacks:</p>

<ul>
	<li>
	<p>They’re discrete representations—i.e., they treat language units (words, n-grams, etc.) as atomic units. This discreteness hampers their ability to capture relationships between words.</p>
	</li>
	<li>
	<p>The feature vectors are sparse and high-dimensional representations. The dimensionality increases with the size of the vocabulary, with most values being zero for any vector. This hampers learning capability. Further, high-dimensionality representation makes them computationally inefficient.</p>
	</li>
	<li>
	<p>They cannot handle OOV words.</p>
	</li>
</ul>

<p>With this, we come to the end of basic vectorization approaches. Now, let’s start looking at distributed representations<a contenteditable="false" data-primary="text representation" data-secondary="basic vectorization approaches" data-startref="ch04_term4" data-type="indexterm" id="idm45969609180584"/><a contenteditable="false" data-primary="TF-IDF (Term Frequency–Inverse Document Frequency)" data-startref="ch04_term10" data-type="indexterm" id="idm45969609178872"/>.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Distributed Representations"><div class="sect1" id="distributed_representations">
<h1>Distributed Representations</h1>

<p><a contenteditable="false" data-primary="distributed representations" data-type="indexterm" id="ch04_term11"/><a contenteditable="false" data-primary="text representation" data-secondary="distributed representations" data-type="indexterm" id="ch04_term12"/>In the previous section, we saw some key drawbacks that are common to all basic vectorization approaches. To overcome these limitations, methods to learn low-dimensional representations were devised. These methods, covered in this section, gained momentum in the past six to seven years. They use neural network <span class="keep-together">architectures</span> to create dense, low-dimensional representations of words and texts. But before we look into these methods, we need to understand some key terms:</p>

<dl>
	<dt><a contenteditable="false" data-primary="distributional similarity" data-type="indexterm" id="idm45969609170680"/>Distributional similarity</dt>
	<dd>This is the idea that the meaning of a word can be understood from the context in which the word appears. This is also known as <em>connotation<a contenteditable="false" data-primary="connotation" data-type="indexterm" id="idm45969609168600"/></em>: meaning is defined by context. This is opposed to <em>denotation<a contenteditable="false" data-primary="denotation" data-type="indexterm" id="idm45969609167080"/></em>: the literal meaning of any word. For example: “NLP rocks.” The literal meaning of the word “rocks” is “stones,” but from the context, it’s used to refer to something good and fashionable.</dd>
	<dt><a contenteditable="false" data-primary="distributional hypothesis" data-type="indexterm" id="idm45969609165336"/>Distributional hypothesis [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609164040-marker" href="ch03.xhtml#idm45969609164040">5</a>]</dt>
	<dd>In linguistics, this hypothesizes that words that occur in similar contexts have similar meanings. For example, the English words “dog” and “cat” occur in similar contexts. Thus, according to the distributional hypothesis, there must be a strong similarity between the meanings of these two words. Now, following from VSM, the meaning of a word is represented by the vector. Thus, if two words often occur in similar context, then their corresponding representation vectors must also be close to each other.</dd>
	<dt><a contenteditable="false" data-primary="text representation" data-secondary="distributional" data-type="indexterm" id="idm45969609161656"/>Distributional representation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_3_5-marker" href="ch03.xhtml#footnote_3_5">6</a>]</dt>
	<dd>This refers to representation schemes that are obtained based on distribution of words from the context in which the words appear. These schemes are based on distributional hypotheses. The distributional property is induced from context (textual vicinity). Mathematically, distributional representation schemes use high-dimensional vectors to represent words. These vectors are obtained from a co-occurrence matrix that captures co-occurrence of word and context. The dimension of this matrix is equal to the size of the vocabulary of the corpus. The four schemes that we’ve seen so far—one-hot, bag of words, bag of n-grams, and TF-IDF—all fall under the umbrella of distributional representation.</dd>
	<dt>Distributed representation [<a data-type="noteref" href="ch03.xhtml#footnote_3_5">6</a>]</dt>
	<dd>This is a related concept. It, too, is based on the distributional hypothesis. As discussed in the previous paragraph, the vectors in distributional representation are very high dimensional and sparse. This makes them computationally inefficient and hampers learning. To alleviate this, distributed representation schemes significantly compress the dimensionality. This results in vectors that are compact (i.e., low dimensional) and dense (i.e., hardly any zeros). The resulting vector space is known as <em>distributed representation</em>. All the subsequent schemes we’ll discuss in this chapter are examples of distributed representation.</dd>
	<dt>Embedding</dt>
	<dd>For the set of words in a corpus, <em>embedding<a contenteditable="false" data-primary="embedding" data-type="indexterm" id="idm45969609153176"/></em> is a mapping between vector space coming from distributional representation to vector space coming from distributed representation.</dd>
	<dt>Vector semantics<a contenteditable="false" data-primary="vector semantics" data-type="indexterm" id="idm45969609151384"/></dt>
	<dd>This refers to the set of NLP methods that aim to learn the word representations based on distributional properties of words in a large corpus.</dd>
</dl>

<p>Now that you have a basic understanding of these terms, we can move on to our first method: word embeddings.</p>

<section data-type="sect2" data-pdf-bookmark="Word Embeddings"><div class="sect2" id="word_embeddings-id00051">
<h2>Word Embeddings</h2>

<p><a contenteditable="false" data-primary="word embeddings" data-type="indexterm" id="ch04_term14"/><a contenteditable="false" data-primary="embedding" data-secondary="word" data-type="indexterm" id="ch04_term13"/>What does it mean when we say a text representation should capture “distributional similarities between words”? Let’s consider some examples. If we’re given the word “USA,” distributionally similar words could be other countries (e.g., Canada, Germany, India, etc.) or cities in the USA. If we’re given the word “beautiful,” words that share some relationship with this word (e.g., synonyms, antonyms) could be considered distributionally similar words. These are words that are likely to occur in similar contexts. In 2013, a seminal work by Mikolov et al. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_3_3-marker" href="ch03.xhtml#footnote_3_3">7</a>] showed that their neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships such as:</p>

<ul class="simplelist">
<li>King – Man + Woman ≈ Queen</li>
</ul>
<p>Their model was able to correctly answer many more analogies like this. <a data-type="xref" href="#word2vec_based_analogy_answering_system">Figure 3-5</a> shows a snapshot of a system based on Word2vec answering analogies. The Word2vec model<a contenteditable="false" data-primary="Word2vec model (Google)" data-type="indexterm" id="ch04_term15"/><a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="ch04_term17"/> is in many ways the dawn of modern-day NLP.</p>



<p>While learning such semantically rich relationships, Word2vec ensures that the learned word representations are low dimensional (vectors of dimensions 50–500, instead of several thousands, as with previously studied representations in this chapter) and dense (that is, most values in these vectors are non-zero). Such representations make ML tasks more tractable and efficient. Word2vec led to a lot of work (both pure and applied) in the direction of learning text representations using neural <span class="keep-together">networks.</span> These representations are also called “embeddings.” Let’s build an intuition of how they work and how to use them to represent text.</p>



<p>Given a text corpus, the aim is to learn embeddings for every word in the corpus such that the word vector in the embedding space best captures the meaning of the word. To “derive” the meaning of the word, Word2vec uses distributional similarity and distributional hypothesis. That is, it derives the meaning of a word from its context: words that appear in its neighborhood in the text. So, if two different words (often) occur in similar context, then it’s highly likely that their meanings are also similar. Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and words with very different meanings are far from one another.</p>


<figure><div id="word2vec_based_analogy_answering_system" class="figure"><img alt="Word2vec-based analogy-answering system" src="Images/pnlp_0305.png" width="629" height="347"/>
<h6><span class="label">Figure 3-5. </span>Word2vec-based analogy-answering system</h6>
</div></figure>

<p>Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus. Given a word <em>w</em> and the words appearing in its context <em>C</em>, how do we find the vector that best represents the meaning of the word? For every word <em>w</em> in corpus, we start with a vector v<sub>w</sub> initialized with random values. The Word2vec model refines the values in v<sub>w</sub> by predicting v<sub>w</sub>, given the vectors for words in the context <em>C</em>. It does this using a two-layer neural network. We’ll dive deeper into this by discussing pre-trained embeddings before moving on to train our own.</p>



<section data-type="sect3" data-pdf-bookmark="Pre-trained word embeddings"><div class="sect3" id="pre_trained_word_embeddings">
<h3>Pre-trained word embeddings</h3>

<p>Training your own word embeddings<a contenteditable="false" data-primary="word embeddings" data-secondary="pre-trained" data-type="indexterm" id="ch04_term16"/> is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it’s not necessary to train your own embeddings, and using pre-trained word embeddings often suffices. What are pre-trained word embeddings? Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia<a contenteditable="false" data-primary="Wikipedia" data-type="indexterm" id="idm45969609091848"/>, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings can be downloaded and used to get the vectors for the words you want. Such embeddings can be thought of as a large collection of key-value pairs, where keys are the words in the vocabulary and values are their corresponding word vectors. Some of the most popular pre-trained embeddings are Word2vec by Google [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609090168-marker" href="ch03.xhtml#idm45969609090168">8</a>], GloVe<a contenteditable="false" data-primary="GloVe " data-type="indexterm" id="idm45969609088664"/><a contenteditable="false" data-primary="Stanford Natural Language Processing Group" data-secondary="GloVe" data-type="indexterm" id="idm45969609087592"/> by Stanford [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609086056-marker" href="ch03.xhtml#idm45969609086056">9</a>], and fasttext embeddings by <a contenteditable="false" data-primary="fastText embeddings (Facebook)" data-type="indexterm" id="idm45969609084552"/><a contenteditable="false" data-primary="Facebook" data-secondary="fastText embeddings" data-type="indexterm" id="idm45969609083448"/>Facebook [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969609081944-marker" href="ch03.xhtml#idm45969609081944">10</a>], to name a few. Further, they’re available for various dimensions like d = 25, 50, 100, 200, 300, 600.</p>

<p class="pagebreak-before"><em>Ch3/Pre_Trained_Word_Embeddings.ipynb</em>, the notebook associated with the rest of this section, shows an example of how to load pre-trained Word2vec embeddings and look for the most similar words (ranked by cosine similarity) to a given word. The code that follows covers the key steps. Here, we find the words that are semantically most similar to the word “beautiful”; the last line returns the embedding vector of the word “beautiful”:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">Word2Vec</code><code class="p">,</code> <code class="n">KeyedVectors</code>
<code class="n">pretrainedpath</code> <code class="o">=</code> <code class="s2">"NLPBookTut/GoogleNews-vectors-negative300.bin"</code>
<code class="n">w2v_model</code> <code class="o">=</code> <code class="n">KeyedVectors</code><code class="o">.</code><code class="n">load_word2vec_format</code><code class="p">(</code><code class="n">pretrainedpath</code><code class="p">,</code> <code class="n">binary</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'done loading Word2Vec'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">w2v_model</code><code class="o">.</code><code class="n">vocab</code><code class="p">))</code> <code class="c1">#Number of words in the vocabulary.</code>
<code class="k">print</code><code class="p">(</code><code class="n">w2v_model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">[</code><code class="s1">'beautiful'</code><code class="p">])</code>
<code class="n">W2v_model</code><code class="p">[</code><code class="s1">'beautiful'</code><code class="p">]</code></pre>

<p><code>most_similar('beautiful')</code> returns the most similar words to the word “beautiful.” The output is shown below. Each word is accompanied by a similarity score. The higher the score, the more similar the word is to the query word:</p>

<pre data-type="programlisting">
[('gorgeous', 0.8353004455566406),
 ('lovely', 0.810693621635437),
 ('stunningly_beautiful', 0.7329413890838623),
 ('breathtakingly_beautiful', 0.7231341004371643),
 ('wonderful', 0.6854087114334106),
 ('fabulous', 0.6700063943862915),
 ('loveliest', 0.6612576246261597),
 ('prettiest', 0.6595001816749573),
 ('beatiful', 0.6593326330184937),
 ('magnificent', 0.6591402292251587)]</pre>

<p><code>w2v_model</code> returns the vector for the query word. For the word “beautiful,” we get the vector as shown in <a data-type="xref" href="#vector_representing_the_word_quotation">Figure 3-6</a>.</p>

<p>Note that if we search for a word that is not present in the Word2vec model (e.g., “practicalnlp”), we’ll see a “key not found” error. Hence, as a good coding practice, it’s always advised to first check if the word is present in the model’s vocabulary before attempting to retrieve its vector. The Python library we used in this code snippet, gensim<a contenteditable="false" data-primary="gensim library" data-type="indexterm" id="idm45969609030584"/>, also supports training and loading GloVe pre-trained models.</p>

<div data-type="tip"><h6>Tip</h6>
<p>If you’re new to embeddings, always start by using pre-trained word embeddings in your project. Understand their pros and cons, then start thinking of building your own embeddings. Using pre-trained embeddings will quickly give you a strong baseline for the task at hand.</p>
</div>

<figure><div id="vector_representing_the_word_quotation" class="figure"><img alt="Vector representing the word “beautiful” in pre-trained Word2vec" src="Images/pnlp_0306.png" width="667" height="851"/>
<h6><span class="label">Figure 3-6. </span>Vector representing the word “beautiful” in pre-trained Word2vec<a contenteditable="false" data-primary="Word2vec model (Google)" data-startref="ch04_term15" data-type="indexterm" id="idm45969609026264"/><a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-startref="ch04_term17" data-type="indexterm" id="idm45969609024888"/></h6>
</div></figure>

<p>Now let’s look at training our own word embeddings<a contenteditable="false" data-primary="word embeddings" data-secondary="pre-trained" data-startref="ch04_term16" data-type="indexterm" id="idm45969609022632"/>.</p>

</div></section>

<section data-type="sect3" class="pagebreak-before" data-pdf-bookmark="Training our own embeddings"><div class="sect3" id="training_our_own_embeddings">
<h3 class="less_space">Training our own embeddings</h3>

<p><a contenteditable="false" data-primary="word embeddings" data-secondary="training" data-type="indexterm" id="ch04_term19"/>Now we’ll focus on training our own word embeddings. For this, we’ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:</p>

<ul>
	<li>
	<p>Continuous bag of words (CBOW)</p>
	</li>
	<li>
	<p>SkipGram</p>
	</li>
</ul>

<p>Both of these have a lot of similarities in many respects. We’ll begin by understanding the CBOW model, then we’ll look at SkipGram. Throughout this section, we’ll use the sentence “The quick brown fox jumps over the lazy dog” as our toy corpus.</p>

<section data-type="sect4" data-pdf-bookmark="CBOW"><div class="sect4" id="cbow">
<h4>CBOW</h4>

<p><a contenteditable="false" data-primary="continuous bag of words (CBOW)" data-type="indexterm" id="ch04_term20"/><a contenteditable="false" data-primary="CBOW (continuous bag of words)" data-type="indexterm" id="ch04_term21"/>In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. What is a language model? It is a (statistical) <em>model</em> that tries to give a probability distribution over sequences of words. Given a sentence of, say, <em>m</em> words, it assigns a probability Pr(w<sub>1</sub>, w<sub>2</sub>, ….., w<sub>n</sub>) to the whole sentence. The objective of a language model is to assign probabilities in such a way that it gives high probability to “good” sentences and low probabilities to “bad” sentences. By good, we mean sentences that are semantically and syntactically correct. By bad, we mean sentences that are incorrect—semantically or syntactically or both. So, for a sentence like “The cat jumped over the dog,” it will try to assign a probability close to 1.0, whereas for a sentence like “jumped over the the cat dog,” it tries to assign a probability close to 0.0.</p>

<p>CBOW tries to learn a language model that tries to predict the “center” word from the words in its context. Let’s understand this using our toy corpus. If we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word—jumps—as shown in <a data-type="xref" href="#fig_3_7_cbow_given_the_context_wordscom">Figure 3-7</a>. CBOW tries to do this for every word in the corpus; i.e., it takes every word in the corpus as the target word and tries to predict the target word from its corresponding context words.</p>

<figure><div id="fig_3_7_cbow_given_the_context_wordscom" class="figure"><img alt="CBOW: given the context words, predict the center word" src="Images/pnlp_0307.png" width="1439" height="242"/>
<h6><span class="label">Figure 3-7. </span>CBOW: given the context words, predict the center word</h6>
</div></figure>

<p class="pagebreak-before">The idea discussed in the previous paragraph is then extended to the entire corpus to build the training set. Details are as follows: we run a sliding window of size 2<em>k</em>+1 over the text corpus. For our example, we took <em>k</em> as 2. Each position of the window marks the set of 2<em>k</em>+1 words that are under consideration. The center word in the window is the target, and <em>k</em> words on either side of the center word form the context. This gives us one data point. If the point is represented as <em>(</em><em>X,Y</em><em>)</em>, then the context is the <em>X</em> and the target word is the <em>Y</em>. A single data point consists of a pair of numbers: (2<em>k</em> indices of words in context, index of word in target). To get the next data point, we simply shift the window to the right on the corpus by one word and repeat the process. This way, we slide the window across the entire corpus to create the training set. This is shown in <a data-type="xref" href="#fig_3_8_preparing_a_dataset_for_cbow">Figure 3-8</a>. Here, the target word is shown in blue and <em>k</em>=2.</p>

<figure><div id="fig_3_8_preparing_a_dataset_for_cbow" class="figure"><img alt="Preparing a dataset for CBOW" src="Images/pnlp_0308.png" width="1418" height="495"/>
<h6><span class="label">Figure 3-8. </span>Preparing a dataset for CBOW</h6>
</div></figure>

<p>Now that we have the training data ready, let’s focus on the model. For this, we construct a shallow net (it’s shallow since it has a single hidden layer), as shown in <a data-type="xref" href="#cbow_model_left_square_bracket_3right_s">Figure 3-9</a>. We assume we want to learn <em>D</em>-dim word embeddings. Further, let <em>V</em> be the vocabulary of the text corpus.</p>

<figure><div id="cbow_model_left_square_bracket_3right_s" class="figure"><img alt="CBOW model [_3]" src="Images/pnlp_0309.png" width="1395" height="1018"/>
<h6><span class="label">Figure 3-9. </span>CBOW model [<a data-type="noteref" href="ch03.xhtml#footnote_3_3">7</a>]</h6>
</div></figure>

<p>The objective is to learn an embedding matrix E<sub>|</sub><sub>V| x d</sub>.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and <em>d</em> is the dimension of the embedding. Let’s break down the shallow net in <a data-type="xref" href="#cbow_model_left_square_bracket_3right_s">Figure 3-9</a> layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix E<sub>|V| x d</sub>. The vectors fetched are then added to get a single <em>D</em>-dim vector, and this is passed to the next layer. The next layer simply takes this <em>d</em> vector and multiplies it with another matrix E’<sub>d x |V|</sub><sub>.</sub>. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses back propagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="ch03fn3-marker" href="ch03.xhtml#ch03fn3">iii</a></sup> we wanted to<a contenteditable="false" data-primary="continuous bag of words (CBOW)" data-startref="ch04_term20" data-type="indexterm" id="idm45969608947160"/><a contenteditable="false" data-primary="CBOW (continuous bag of words)" data-startref="ch04_term21" data-type="indexterm" id="idm45969608945768"/> learn.</p>


</div></section>

<section data-type="sect4" class="pagebreak-before" data-pdf-bookmark="SkipGram"><div class="sect4" id="skipgram">
<h4 class="less_space">SkipGram</h4>

<p><a contenteditable="false" data-primary="SkipGram" data-type="indexterm" id="ch04_term22"/>SkipGram is very similar to CBOW, with some minor changes. In SkipGram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word “jumps,” we try to predict every word in context—“brown,” “fox,” “over,” “the”—as shown in <a data-type="xref" href="#fig_3_10_skipgram_given_the_center_word">Figure 3-10</a>. This constitutes one step. SkipGram repeats this one step for every word in the corpus as the center word.</p>

<figure><div id="fig_3_10_skipgram_given_the_center_word" class="figure"><img alt="SkipGram: given the center word, predict every word in context" src="Images/pnlp_0310.png" width="1440" height="241"/>
<h6><span class="label">Figure 3-10. </span>SkipGram: given the center word, predict every word in context</h6>
</div></figure>

<p>The dataset to train a SkipGram is prepared as follows: we run a sliding window of size 2<em>k</em>+1 over the text corpus to get the set of 2<em>k</em>+1 words that are under consideration. The center word in the window is the <em>X</em>, and <em>k</em> words on either side of the center word are <em>Y</em>. Unlike CBOW, this gives us 2<em>k</em> data points. A single data point consists of a pair: (index of the center word, index of a target word). We then shift the window to the right on the corpus by one word and repeat the process. This way, we slide the window across the entire corpus to create the training set. This is shown in <a data-type="xref" href="#preparing_a_dataset_for_skipgram">Figure 3-11</a>.</p>

<figure><div id="preparing_a_dataset_for_skipgram" class="figure"><img alt="Preparing a dataset for SkipGram" src="Images/pnlp_0311.png" width="1214" height="822"/>
<h6><span class="label">Figure 3-11. </span>Preparing a dataset for SkipGram</h6>
</div></figure>

<p>The shallow network used to train the SkipGram model, shown in <a data-type="xref" href="#skipgram_architecture_left_square_brack">Figure 3-12</a>, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix E<sub>|V| x d</sub>. The vectors fetched are then passed to the next layer. The next layer simply takes this <em>d</em> vector and multiplies it with another matrix E’<sub>d x |V|</sub>. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses back propagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn.</p>

<figure><div id="skipgram_architecture_left_square_brack" class="figure"><img alt="SkipGram architecture [_3]" src="Images/pnlp_0312.png" width="1336" height="1120"/>
<h6><span class="label">Figure 3-12. </span>SkipGram architecture [<a data-type="noteref" href="ch03.xhtml#footnote_3_3">7</a>]</h6>
</div></figure>

<p>There are a lot of other minute details that go into both <a contenteditable="false" data-primary="CBOW (continuous bag of words)" data-type="indexterm" id="idm45969608924040"/><a contenteditable="false" data-primary="continuous bag of words (CBOW)" data-type="indexterm" id="idm45969608922888"/>CBOW and the SkipGram model. An interested reader can look at the three-part blog post by Sebastian <a contenteditable="false" data-primary="Ruder, Sebastian" data-type="indexterm" id="idm45969608921528"/>Ruder [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608920296-marker" href="ch03.xhtml#idm45969608920296">11</a>]. You can also refer to Rong (2016) [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608916088-marker" href="ch03.xhtml#idm45969608916088">12</a>] for a step-by-step derivation of Word2vec<a contenteditable="false" data-primary="Word2vec model (Google)" data-type="indexterm" id="idm45969608914616"/><a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="idm45969608913544"/> parameter learning. Another key aspect to keep in mind is hyperparameters of the model. There are several hyperparameters: window size, dimensionality of the vectors to be learned, learning rate, number of epochs, etc. It’s a well-established fact that hyperparameters play a crucial role in the quality of the final model [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608911704-marker" href="ch03.xhtml#idm45969608911704">13</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608910456-marker" href="ch03.xhtml#idm45969608910456">14</a>].</p>


<p>To use both the CBOW and SkipGram algorithms in practice, there are several available implementations that abstract the mathematical details for us. One of the most commonly used implementations is <a contenteditable="false" data-primary="gensim library" data-type="indexterm" id="idm45969608908552"/>gensim [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608907320-marker" href="ch03.xhtml#idm45969608907320">15</a>].</p>




<p>Despite the availability of several off-the-shelf implementations, we still have to make decisions on several hyperparameters (i.e., the variables that need to be set before starting the training process). Let’s look at two examples.</p>

<dl>
	<dt>Dimensionality of the word vectors<a contenteditable="false" data-primary="word vectors" data-type="indexterm" id="idm45969608904392"/></dt>
	<dd>As the name indicates, this decides the space of the learned embeddings. While there is no ideal number, it’s common to construct word vectors with dimensions in the range of 50–500 and evaluate them on the task we’re using them for to choose the best option.</dd>
	<dt>Context window<a contenteditable="false" data-primary="context window" data-type="indexterm" id="idm45969608902168"/></dt>
	<dd>How long or short the context we look for to learn the vector representation is.</dd>
</dl>

<p>There are also other choices we make, such as whether to use CBOW or SkipGram to learn the embeddings. These choices are more of an art than science at this point, and there’s a lot of ongoing research on methods for choosing the right hyperparameters.</p>

<p>Using packages like gensim, it’s pretty straightforward from a code point of view to implement Word2vec<a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="idm45969608899256"/><a contenteditable="false" data-primary="Word2vec model (Google)" data-secondary="training" data-type="indexterm" id="idm45969608897880"/>. The following code shows how to train our own Word2vec model using a toy corpus called <code>common_texts</code> that’s available in gensim. Assuming you have the corpus for your domain, following this code snippet will quickly give you your own embeddings:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#Import a test data set provided in gensim to train a model</code>
<code class="kn">from</code> <code class="nn">gensim.test.utils</code> <code class="kn">import</code> <code class="n">common_texts</code>
<code class="c1">#Build the model, by selecting the parameters.</code>
<code class="n">our_model</code> <code class="o">=</code> <code class="n">Word2Vec</code><code class="p">(</code><code class="n">common_texts</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">min_count</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">workers</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="c1">#Save the model</code>
<code class="n">our_model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"tempmodel.w2v"</code><code class="p">)</code>
<code class="c1">#Inspect the model by looking for the most similar words for a test word.</code>
<code class="k">print</code><code class="p">(</code><code class="n">our_model</code><code class="o">.</code><code class="n">wv</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="s1">'computer'</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">5</code><code class="p">))</code>
<code class="c1">#Let us see what the 10-dimensional vector for 'computer' looks like.</code>
<code class="k">print</code><code class="p">(</code><code class="n">our_model</code><code class="p">[</code><code class="s1">'computer'</code><code class="p">])</code></pre>

<p>Now, we can get the vector representation for any word in our corpus, provided it’s in the model’s vocabulary—we just look up the word in the model. But what if we have a phrase (e.g., “word embeddings”) for which we need a<a contenteditable="false" data-primary="word embeddings" data-startref="ch04_term14" data-type="indexterm" id="idm45969608889432"/><a contenteditable="false" data-primary="embedding" data-secondary="word" data-startref="ch04_term13" data-type="indexterm" id="idm45969608832984"/><a contenteditable="false" data-primary="word embeddings" data-secondary="training" data-startref="ch04_term19" data-type="indexterm" id="idm45969608831336"/><a contenteditable="false" data-primary="SkipGram" data-startref="ch04_term22" data-type="indexterm" id="idm45969608829688"/> vector?</p>
</div></section>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Going Beyond Words"><div class="sect2" id="going_beyond_words">
<h2>Going Beyond Words</h2>

<p><a contenteditable="false" data-primary="embedding" data-secondary="beyond words" data-secondary-sortas="words" data-type="indexterm" id="ch04_term25"/>So far, we’ve seen examples of how to use pre-trained word embeddings and train our own word embeddings. This gives us a compact and dense representation for words in our vocabulary. However, in most NLP applications, we seldom deal with atomic units like words—we deal with sentences, paragraphs, or even full texts. So, we need a way to represent larger units of text. Is there a way we can use word embeddings to get feature representations for larger units of text?</p>

<p>A simple approach is to break the text into constituent words, take the embeddings for individual words, and combine them to form the representation for the text. There are various ways to combine them, the most popular being sum, average, etc., but these may not capture many aspects of the text as a whole, such as ordering. Surprisingly, they work very well in practice (see <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>). As a matter of fact, in CBOW<a contenteditable="false" data-primary="CBOW (continuous bag of words)" data-type="indexterm" id="idm45969608822360"/><a contenteditable="false" data-primary="continuous bag of words (CBOW)" data-type="indexterm" id="idm45969608821288"/>, this was demonstrated by taking the sum of word vectors in context. The resulting vector represents the entire context and is used to predict the center word.</p>

<p>It’s always a good idea to experiment with this before moving to other representations. The following code shows how to obtain the vector representation for text by averaging word vectors using the library spaCy [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608819272-marker" href="ch03.xhtml#idm45969608819272">16</a>]:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">spacy</code>
<code class="kn">import</code> <code class="nn">en_core_web_sm</code>

<code class="c1"># Load the spacy model. This takes a few seconds.</code>
<code class="n">nlp</code> <code class="o">=</code> <code class="n">en_core_web_sm</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>

<code class="c1"># Process a sentence using the model</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="s2">"Canada is a large country"</code><code class="p">)</code>

<code class="c1">#Get a vector for individual words</code>
<code class="c1">#print(doc[0].vector) #vector for 'Canada', the first word in the text</code>
<code class="k">print</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">vector</code><code class="p">)</code> <code class="c1">#Averaged vector for the entire sentence</code></pre>

<p>Both pre-trained and self-trained word embeddings depend on the vocabulary they see in the training data. However, there is no guarantee that we will only see those words in the production data for the application we’re building. Despite the ease of using Word2vec or any such word embedding to do feature extraction from texts, we don’t have a good way of handling OOV words yet. This has been a recurring problem in all the representations we’ve seen so far. What do we do in such cases?</p>

<p>A simple approach that often works is to exclude those words from the feature extraction process so we don’t have to worry about how to get their representations. If we’re using a model trained on a large corpus, we shouldn’t see too many OOV words anyway. However, if a large fraction of the words from our production data isn’t present in the word embedding’s vocabulary, we’re unlikely to see good performance. This vocabulary overlap is a great heuristic<a contenteditable="false" data-primary="heuristics" data-type="indexterm" id="idm45969608809864"/> to gauge the performance of an NLP model.</p>

<div data-type="tip"><h6>Tip</h6>
<p>If the overlap between corpus vocabulary and embedding vocabulary is less than 80%, we’re unlikely to see good performance from our NLP model.</p></div>

<p>Even if the overlap is above 80%, the model can still do poorly depending on which words fall in the 20%. If these words are important for the task, then this is very possible. For example, say we want to build a classifier that can classify medical documents on cancer from medical documents on the heart. Now, in this case, certain terms like heart, cancer, etc., will become important for differentiating the two sets of documents. If these terms are not present in the word embedding’s vocabulary, our classifier might still do poorly.</p>

<p>Another way to deal with the <a contenteditable="false" data-primary="OOV (out of vocabulary) problem" data-type="indexterm" id="idm45969608806168"/>OOV problem for word embeddings is to create vectors that are initialized randomly, where each component is between –0.25 to +0.25, and continue to use these vectors throughout the application we’re building [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608804696-marker" href="ch03.xhtml#idm45969608804696">17</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608816024-marker" href="ch03.xhtml#idm45969608816024">18</a>]. From our own experience, this can give us a jump of 1–2% in performance.</p>

<p>There are also other approaches that handle the <a contenteditable="false" data-primary="out of vocabulary (OOV) problem" data-type="indexterm" id="idm45969608747080"/>OOV problem by modifying the training process by bringing in characters and other subword-level linguistic components. Let’s look at one such approach now. The key idea is that one can potentially handle the OOV problem by using subword information, such as morphological properties (e.g., prefixes, suffixes, word endings, etc.), or by using character representations. fastText<a contenteditable="false" data-primary="fastText embeddings (Facebook)" data-type="indexterm" id="idm45969608745384"/><a contenteditable="false" data-primary="Facebook" data-secondary="fastText embeddings" data-type="indexterm" id="idm45969608744264"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608742760-marker" href="ch03.xhtml#idm45969608742760">19</a>], from Facebook AI research, is one of the popular algorithms that follows this approach. A word can be represented by its constituent character n-grams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word’s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there’s a word, “gregarious,” that’s not found in the embedding’s word vocabulary. We break it into character n-grams—gre, reg, ega, ….ous—and combine these embeddings of the n-grams to arrive at the embedding of “gregarious.”</p>

<p>Pre-trained fastText models can be downloaded from their website [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608740200-marker" href="ch03.xhtml#idm45969608740200">20</a>], and gensim’s fastText wrapper can be used both for loading pre-trained models or training models using fastText in a way similar to Word2vec. We leave that as an exercise for the reader. In <a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>, we’ll see how to use fastText embeddings for text classification. Now, we’ll take a look at some distributed representations that move beyond words.<a contenteditable="false" data-primary="embedding" data-secondary="beyond words" data-secondary-sortas="words" data-startref="ch04_term25" data-type="indexterm" id="idm45969608737448"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Distributed Representations Beyond Words and Characters"><div class="sect1" id="distributed_representations_beyond_word">
<h1>Distributed Representations Beyond Words <span class="keep-together">and Characters</span></h1>

<p>So far, we’ve seen two approaches to coming up with text representations using embeddings. Word2vec<a contenteditable="false" data-primary="Word2vec model (Google)" data-type="indexterm" id="idm45969608733080"/><a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="idm45969608731976"/> learned representations for words, and we aggregated them to form text representations. fastText learned representations for character n-grams, which were aggregated to form word representations and then text representations. A potential problem with both approaches is that they do not take the context of words into account. Take, for example, the sentences “dog bites man” and “man bites dog.” Both receive the same representation in these approaches, but they obviously have very different meanings. Let’s look at another approach, Doc2vec, which allows us to directly learn the representations for texts of arbitrary lengths (phrases, sentences, paragraphs, and documents) by taking the context of words in the text into account.</p>

<p>Doc2vec<a contenteditable="false" data-primary="Doc2vec model" data-type="indexterm" id="ch04_term29"/> is based on the paragraph vectors framework [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608727464-marker" href="ch03.xhtml#idm45969608727464">21</a>] and is implemented in gensim. This is similar to Word2vec in terms of its general architecture, except that, in addition to the word vectors, it also learns a “paragraph vector” that learns a <span class="keep-together">representation</span> for the full text (i.e., with words in context). When learning with a large corpus of many texts, the paragraph vectors are unique for a given text (where “text” can mean any piece of text of arbitrary length), while word vectors will be shared across all texts. The shallow neural networks used to learn Doc2vec embeddings (<a data-type="xref" href="#doc2vec_architectures_left_parenthesisa">Figure 3-13</a>) are very similar to the CBOW and SkipGram architecture of Word2vec. The two architectures are called <em>distributed memory (DM)<a contenteditable="false" data-primary="distributed memory (DM)" data-type="indexterm" id="idm45969608698488"/><a contenteditable="false" data-primary="DM (distributed memory)" data-type="indexterm" id="idm45969608697384"/></em> and <em>distributed bag of words (DBOW)<a contenteditable="false" data-primary="distributed bag of words (DBOW)" data-type="indexterm" id="idm45969608695864"/><a contenteditable="false" data-primary="DBOW (distributed bag of words)" data-type="indexterm" id="idm45969608694792"/></em>. They are shown in <a data-type="xref" href="#doc2vec_architectures_left_parenthesisa">Figure 3-13</a>.</p>



<p>Once the Doc2vec model is trained, paragraph vectors for new texts are inferred using the common word vectors from training. Doc2vec was perhaps the first widely accessible implementation for getting an embedding representation for the full text instead of using a combination of individual word vectors. Since it models some form of context and can encode texts of arbitrary length into a fixed, low-dimensional, dense vector, it has found application in a wide range of NLP applications, such as text classification, document tagging, text recommendation systems, and simple chatbots for FAQs. We’ll see an example of training a Doc2vec representation and using it for text classification in the next chapter. Let’s look at other text representation methods that extend this idea of taking full text into<a contenteditable="false" data-primary="text representation" data-secondary="distributed representations" data-startref="ch04_term12" data-type="indexterm" id="idm45969608691368"/><a contenteditable="false" data-primary="distributed representations" data-startref="ch04_term11" data-type="indexterm" id="idm45969608689752"/> account.</p>

<figure><div id="doc2vec_architectures_left_parenthesisa" class="figure"><img alt="Doc2vec architectures: (a) DM and (b) DBOW" src="Images/pnlp_0313.png" width="1419" height="498"/>
<h6><span class="label">Figure 3-13. </span>Doc2vec architectures: DM (left) and DBOW (right)</h6>
</div></figure>
</div></section>

<section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Universal Text Representations"><div class="sect1" id="universal_text_representations">
<h1 class="less_space">Universal Text Representations</h1>

<p><a contenteditable="false" data-primary="text representation" data-secondary="universal representations" data-type="indexterm" id="ch04_term30"/><a contenteditable="false" data-primary="universal text representations" data-type="indexterm" id="ch04_term31"/>In all the representations we’ve seen so far, we notice that one word gets one fixed representation. Can this be a problem? Well, to some extent, yes. Words can mean different things in different contexts. For example, the sentences “I went to a bank to withdraw money” and “I sat by the river bank and pondered about text representations” both use the word “bank.” However, they mean different things in each <span class="keep-together">sentence.</span> With the vectorization and embedding approaches that we’ve seen so far, there’s no direct way to capture this information.</p>

<p>In 2018, researchers came up with the idea of <em>contextual word representations<a contenteditable="false" data-primary="contextual word representations" data-type="indexterm" id="idm45969608678520"/></em>, which addresses this issue. It uses “<a contenteditable="false" data-primary="language modeling" data-type="indexterm" id="idm45969608677208"/>language modeling,” which is the task of predicting the next likely word in a sequence of words. In its earliest form, it used the idea of n-gram frequencies to estimate the probability of the next word given a history of words. The past few years have seen the advent of advanced neural language models (e.g., transformers [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608675672-marker" href="ch03.xhtml#idm45969608675672">22</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608674328-marker" href="ch03.xhtml#idm45969608674328">23</a>]) that make use of the word embeddings we discussed <span class="keep-together">earlier</span> but use complex architectures involving multiple passes through the text and multiple reads from left to right and right to left to model the context of language use.</p>

<p>Neural architectures such as recurrent neural networks (RNNs)<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="idm45969608671464"/><a contenteditable="false" data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="idm45969608670312"/> and transformers were used to develop large-scale models of language (<a contenteditable="false" data-primary="ELMo" data-type="indexterm" id="idm45969608669064"/>ELMo [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608667752-marker" href="ch03.xhtml#idm45969608667752">24</a>], BERT<a contenteditable="false" data-primary="BERT (Bidirectional Encoder Representations from Transformers)" data-type="indexterm" id="idm45969608666232"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608664984-marker" href="ch03.xhtml#idm45969608664984">25</a>]), which can be used as pre-trained models to get text representations. The key idea is to leverage “transfer learning<a contenteditable="false" data-primary="transfer learning" data-type="indexterm" id="idm45969608663272"/>”—that is, to learn embeddings on a generic task (like language modeling) on a massive corpus and then fine-tune learnings on task-specific data. These models have shown significant improvements on some fundamental NLP tasks, such as question answering, semantic role labeling, named entity recognition, and coreference resolution, to name a few. We briefly described what some of these tasks are in <a data-type="xref" href="ch01.xhtml#nlp_a_primer">Chapter 1</a>. Interested readers can go through the references, including the upcoming book by Taher and Collados [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608660616-marker" href="ch03.xhtml#idm45969608660616">26</a>].</p>

<p>In the last three sections, we looked at the key ideas behind word embeddings, how to train them, and how to use pre-trained embeddings to get text representations. We’ll learn more about how to use these representations in different NLP applications in the coming chapters. These representations are very useful and popular in modern-day NLP. However, based on our experience, here are a few important aspects to keep in mind while using them in your project:</p>

<ul>
	<li>
	<p>All text representations are inherently biased based on what they saw in training data. For example, an embedding model trained heavily on technology news or articles is likely to identify Apple as being closer to, say, Microsoft or Facebook than to an orange or pear. While this is anecdotal, such biases stemming from training data can have serious implications on the performance of NLP models and systems that rely on these representations. Understanding biases that may be present in learned embeddings and developing methods for addressing them is very important. An interested reader can look at Tolga et al. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608656792-marker" href="ch03.xhtml#idm45969608656792">27</a>]. These biases<a contenteditable="false" data-primary="bias" data-type="indexterm" id="idm45969608655432"/> are an important factor to consider in any NLP software <span class="keep-together">development.</span></p>
	</li>
	<li>
	<p>Unlike the basic vectorization approaches, pre-trained <a contenteditable="false" data-primary="embedding" data-secondary="pre-trained" data-type="indexterm" id="idm45969608652504"/><a contenteditable="false" data-primary="pre-training" data-type="indexterm" id="idm45969608651160"/>embeddings are generally large-sized files (several gigabytes), which may pose problems in certain deployment scenarios. This is something we need to address while using them, <span class="keep-together">otherwise</span> it can become an engineering bottleneck in performance. The Word2vec model<a contenteditable="false" data-primary="Word2vec model (Google)" data-type="indexterm" id="idm45969608649048"/><a contenteditable="false" data-primary="Google" data-secondary="Word2vec model" data-type="indexterm" id="idm45969608647944"/> takes ~4.5 GB RAM. One good hack is to use in-memory databases<a contenteditable="false" data-primary="in-memory databases" data-type="indexterm" id="idm45969608646440"/><a contenteditable="false" data-primary="databases, in-memory" data-type="indexterm" id="idm45969608645256"/> like <a contenteditable="false" data-primary="Redis database" data-type="indexterm" id="idm45969608644024"/>Redis [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608642760-marker" href="ch03.xhtml#idm45969608642760">28</a>] with a cache on top of them to address scaling and latency issues. Load your embeddings into such databases and use the embeddings as if they’re available in RAM.</p>
	</li>
	<li>
	<p>Modeling language for a real-world application is more than capturing the information via word and sentence embeddings. We still need ways to encode specific aspects of text, the relationships between sentences in it, and any other domain- and application-specific needs that may not be addressed by the embedding <span class="keep-together">representations</span> themselves (yet!). For example, the task of <a contenteditable="false" data-primary="sarcasm" data-type="indexterm" id="idm45969608639512"/>sarcasm detection requires nuances that are not yet captured well by embedding techniques.</p>
	</li>
	<li>
	<p>As we speak, neural text representation<a contenteditable="false" data-primary="text representation" data-secondary="neural" data-type="indexterm" id="idm45969608637304"/><a contenteditable="false" data-primary="neural text representation" data-type="indexterm" id="idm45969608635928"/> is an evolving area in NLP, with rapidly changing state of the art. While it’s easy to get carried away by the next big model in the news, a practitioner needs to exercise caution and consider practical issues such as return on investment from the effort, business needs, and infrastructural constraints before trying to use them in production-grade applications.</p>
	</li>
</ul>

<p>An interested reader may refer to Smith [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608633512-marker" href="ch03.xhtml#idm45969608633512">29</a>] for a concise summary of the evolution of different word representations and the research challenges ahead for neural network models of text representation. Now, let’s move on to techniques for visualizing embeddings<a contenteditable="false" data-primary="text representation" data-secondary="universal representations" data-startref="ch04_term30" data-type="indexterm" id="idm45969608631864"/><a contenteditable="false" data-primary="universal text representations" data-startref="ch04_term31" data-type="indexterm" id="idm45969608630152"/>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Visualizing Embeddings"><div class="sect1" id="visualizing_embeddings">
<h1>Visualizing Embeddings</h1>

<p><a contenteditable="false" data-primary="embedding" data-secondary="visualizing" data-type="indexterm" id="ch04_term33"/><a contenteditable="false" data-primary="visualizing embeddings" data-type="indexterm" id="ch04_term34"/>So far, we’ve seen various vectorization techniques for representing text. The vectors obtained are used as features for the NLP task at hand, be it text classification or a question-answering system. An important aspect of any ML project is feature exploration. How can we explore the vectors that we have to work with? Visual exploration is a very important aspect of any data-related problem. Is there a way to visually inspect word vectors? Even though embeddings are low-dimensional vectors, even 100 or 300 dimensions are too high to visualize.</p>

<p>Enter t-SNE<a contenteditable="false" data-primary="t-distributed Stochastic Neighboring Embedding (t-SNE)" data-type="indexterm" id="ch04_term36"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608620888-marker" href="ch03.xhtml#idm45969608620888">30</a>], or <em>t-distributed Stochastic Neighboring Embedding</em><em>.</em> It’s a technique used for visualizing high-dimensional data like embeddings by reducing them to two- or three-dimensional data. The technique takes in the embeddings (or any data) and looks at how to best represent the input data using lesser dimensions, all while maintaining the same data distributions in original high-dimensional input space and low-dimensional output space. This, therefore, enables us to plot and visualize the input data. It helps to get a feel for the space of word embedding.</p>

<p>Let’s look at some visualizations using t-SNE. First, we look at feature vectors obtained from the MNIST digits dataset [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608618024-marker" href="ch03.xhtml#idm45969608618024">31</a>]. Here, the images are passed through a convolution neural network and the final feature vectors. <a data-type="xref" href="#visualizing_mnist_data_using_t_sne_left">Figure 3-14</a> shows the two-dimensional plot of the vectors. It clearly shows that our feature vectors are very useful since vectors of the same class tend to cluster together.</p>





<figure><div id="visualizing_mnist_data_using_t_sne_left" class="figure"><img alt="Visualizing MNIST data using t-SNE [_28]" src="Images/pnlp_0314.png" width="643" height="619"/>
<h6><span class="label">Figure 3-14. </span>Visualizing MNIST data using t-SNE [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608613368-marker" href="ch03.xhtml#idm45969608613368">32</a>]</h6>
</div></figure>

<p>Let’s now visualize word embeddings<a contenteditable="false" data-primary="word embeddings" data-secondary="visualizing" data-type="indexterm" id="ch04_term37"/>. In <a data-type="xref" href="#t_sne_visualizations_of_word_embeddings">Figure 3-15</a>, we show only a few words. The interesting thing to note is that the words that have similar meanings tend to cluster together.</p>

<figure><div id="t_sne_visualizations_of_word_embeddings" class="figure"><img alt="t-SNE visualizations of word embeddings. Left: Numbers; right: job titles [_29]" src="Images/pnlp_0315.png" width="1358" height="505"/>
<h6><span class="label">Figure 3-15. </span>t-SNE visualizations of word embeddings (eft: numbers, right: job titles [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608606664-marker" href="ch03.xhtml#idm45969608606664">33</a>]</h6>
</div></figure>

<p>Let’s look at another word embedding visualization, probably the most famous one in the NLP community. <a data-type="xref" href="#t_sne_visualization_shows_some_interest">Figure 3-16</a> shows two-dimensional visualization of embedding vectors for a subset of words: man, woman, uncle, aunt, king, queen. <a data-type="xref" href="#t_sne_visualization_shows_some_interest">Figure 3-16</a> shows not only the position of the vectors of these words, but also an interesting observation between the vectors—the arrows capture the “relationship” between words. t-SNE visualization helps greatly in coming up with such nice observations.</p>

<figure><div id="t_sne_visualization_shows_some_interest" class="figure"><img alt="t-SNE visualization shows some interesting relationships [_3]" src="Images/pnlp_0316.png" width="641" height="474"/>
<h6><span class="label">Figure 3-16. </span>t-SNE visualization shows some interesting relationships [<a data-type="noteref" href="ch03.xhtml#footnote_3_3">7</a>]</h6>
</div></figure>

<p>t-SNE works equally well for visualizing document embeddings<a contenteditable="false" data-primary="document embeddings" data-secondary="visualizing" data-type="indexterm" id="ch04_term38"/>. For example, we might take Wikipedia articles on various topics, obtain corresponding document vectors for each article, then plot these vectors using t-SNE. The visualization in <a data-type="xref" href="#_visualization_of_wikipedia_document_ve">Figure 3-17</a> clearly shows that articles in a given category are grouped together.</p>



<p><a contenteditable="false" data-primary="document embeddings" data-secondary="visualizing" data-startref="ch04_term38" data-type="indexterm" id="idm45969608595736"/>Clearly, t-SNE is very useful for eyeballing the quality of feature vectors. We can use tools like the embedding projector from TensorBoard<a contenteditable="false" data-primary="TensorBoard" data-type="indexterm" id="idm45969608593528"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608592296-marker" href="ch03.xhtml#idm45969608592296">34</a>] to visualize embeddings in our day-to-day work. As shown in <a data-type="xref" href="#screenshot_of_tensorboard_interface_for">Figure 3-18</a>, TensorBoard has a nice interface that is tailor made for visualizing embeddings. We leave it as an exercise for the reader to explore it further. For more on t-SNE, you can read the excellent article on using t-SNE more effectively by Martin et al. [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608589544-marker" href="ch03.xhtml#idm45969608589544">35</a>].<a contenteditable="false" data-primary="embedding" data-secondary="visualizing" data-startref="ch04_term33" data-type="indexterm" id="idm45969608588360"/><a contenteditable="false" data-primary="visualizing embeddings" data-startref="ch04_term34" data-type="indexterm" id="idm45969608586680"/><a contenteditable="false" data-primary="t-distributed Stochastic Neighboring Embedding (t-SNE)" data-startref="ch04_term36" data-type="indexterm" id="idm45969608585304"/></p>

<figure><div id="_visualization_of_wikipedia_document_ve" class="figure"><img alt=" Visualization of Wikipedia document vectors [_30]" src="Images/pnlp_0317.png" width="633" height="550"/>
<h6><span class="label">Figure 3-17. </span>Visualization of Wikipedia document vectors [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608582280-marker" href="ch03.xhtml#idm45969608582280">36</a>]</h6>
</div></figure>

<figure><div id="screenshot_of_tensorboard_interface_for" class="figure"><img alt="Screenshot of TensorBoard interface for visualizing embeddings [_33]" src="Images/pnlp_0318.png" width="1442" height="724"/>
<h6><span class="label">Figure 3-18. </span>Screenshot of TensorBoard<a contenteditable="false" data-primary="TensorBoard" data-type="indexterm" id="idm45969608578904"/> interface for visualizing embeddings [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608577624-marker" href="ch03.xhtml#idm45969608577624">37</a>]</h6>
</div></figure>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Handcrafted Feature Representations"><div class="sect1" id="handcrafted_feature_representations">
<h1>Handcrafted Feature Representations</h1>

<p><a contenteditable="false" data-primary="feature representation" data-secondary="handcrafted" data-type="indexterm" id="ch04_term39"/><a contenteditable="false" data-primary="handcrafted feature representations" data-type="indexterm" id="ch04_term40"/>So far in this chapter, we’ve seen various feature representation schemes that are learned from a text corpus, large or small. These feature representations are mostly not dependent on the NLP problem or application domain.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="ch03fn4-marker" href="ch03.xhtml#ch03fn4">iv</a></sup> The same approach works whether we want text representation for information extraction or text classification and whether we work with a corpus of tweets or scientific articles.</p>

<p>However, in many cases, we do have some domain-specific knowledge about the given NLP problem, which we would like to incorporate into the model we’re building. In such cases, we resort to handcrafted features. Let’s take an example of a real-world NLP system: TextEvaluator<a contenteditable="false" data-primary="TextEvaluator software (ETS)" data-type="indexterm" id="idm45969608568504"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="footnote_3_34-marker" href="ch03.xhtml#footnote_3_34">38</a>]. It’s software developed by Educational Testing Service (ETS)<a contenteditable="false" data-primary="Educational Testing Service (ETS)" data-type="indexterm" id="idm45969608565576"/><a contenteditable="false" data-primary="ETS (Educational Testing Service)" data-type="indexterm" id="idm45969608564440"/>. The goal of this tool is to help teachers and educators provide support in choosing grade-appropriate reading materials for students and identifying sources of comprehension difficulty in texts. Clearly, this is a very specific problem. Having general-purpose word embeddings will not help much. It needs specialized features extracted from text to model some form of grade appropriateness. The screenshot in <a data-type="xref" href="#textevaluator_software_output_requiring">Figure 3-19</a> shows some of the specialized features that are extracted from text for various dimensions of text complexity they model. Clearly, measures such as “syntactic complexity,” “concreteness,” etc., cannot be calculated by only converting text into BoW or embedding representations. They have to be designed manually, keeping in mind both the domain knowledge and the ML algorithms to train the NLP models. This is why we call these <em>handcrafted feature representations</em>.</p>

<figure><div id="textevaluator_software_output_requiring" class="figure"><img alt="TextEvaluator software output requiring handcrafted features [_34]" src="Images/pnlp_0319.png" width="1157" height="597"/>
<h6><span class="label">Figure 3-19. </span>TextEvaluator software output requiring handcrafted features [<a data-type="noteref" href="ch03.xhtml#footnote_3_34">38</a>]</h6>
</div></figure>

<p>Another software tool from <a contenteditable="false" data-primary="ETS (Educational Testing Service)" data-type="indexterm" id="idm45969608557576"/><a contenteditable="false" data-primary="Educational Testing Service (ETS)" data-type="indexterm" id="idm45969608556456"/>ETS that’s popular for grading is the automated essay scorer used in online exams, such as the Graduate Record Examination (GRE) and Test of English as a Foreign Language (TOEFL), to evaluate test-taker essays [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608554984-marker" href="ch03.xhtml#idm45969608554984">39</a>]. <a contenteditable="false" data-primary="automated scoring" data-type="indexterm" id="idm45969608553544"/>This tool, too, requires handcrafted features. Evaluating an essay for various aspects of writing requires specialized features that address those needs. One cannot rely only on n-grams or word embeddings. Another NLP application where one may need such specialized feature engineering is the <a contenteditable="false" data-primary="spelling- and grammar-correction tools" data-type="indexterm" id="idm45969608552008"/><a contenteditable="false" data-primary="grammar-correction tools" data-type="indexterm" id="idm45969608550840"/>spelling and grammar correction we use in tools such as Microsoft Word<a contenteditable="false" data-primary="Microsoft Word" data-type="indexterm" id="idm45969608549592"/>, <a contenteditable="false" data-primary="Grammarly" data-type="indexterm" id="idm45969608548280"/>Grammarly, etc. These are all examples of commonly used tools where we often need custom features to incorporate domain knowledge.</p>

<p>Clearly, custom feature engineering is much more difficult to formulate compared to other feature engineering schemes we’ve seen so far. It’s for this reason that vectorization approaches are more accessible to get started with, especially when we don’t have enough understanding of the domain. Still, handcrafted features are very common in several real-world applications. In most industrial application scenarios, we end up combining the problem-agnostic feature representations we saw earlier (basic vectorization and distributed representations) with some domain-specific features to develop hybrid features. Recent studies from IBM Research<a contenteditable="false" data-primary="IBM Research" data-type="indexterm" id="idm45969608545688"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608544456-marker" href="ch03.xhtml#idm45969608544456">40</a>] and Walmart<a contenteditable="false" data-primary="Walmart" data-type="indexterm" id="idm45969608543144"/> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="idm45969608541912-marker" href="ch03.xhtml#idm45969608541912">41</a>] show examples of how heuristics, handcrafted features, and ML are all used together in real-world industrial systems dealing with NLP problems. We’ll see some examples of how to use such handcrafted features in upcoming chapters, such as text classification (<a data-type="xref" href="ch04.xhtml#text_classification">Chapter 4</a>) and information extraction (<a data-type="xref" href="ch05.xhtml#information_extraction">Chapter 5</a>).<a contenteditable="false" data-primary="feature representation" data-secondary="handcrafted" data-startref="ch04_term39" data-type="indexterm" id="idm45969608538408"/><a contenteditable="false" data-primary="handcrafted feature representations" data-startref="ch04_term40" data-type="indexterm" id="idm45969608536728"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Wrapping Up"><div class="sect1" id="wrapping_up-id00063">
<h1>Wrapping Up</h1>

<p>In this chapter, we saw different techniques for representing text, starting from the basic approaches to state-of-the-art DL methods. A question that may arise organically at this point would be: when should we go for vectorization features and embeddings, and when should we look for handcrafted features? The answer is that it depends on the task at hand. For some applications, such as text classification, it’s more common to see vectorization approaches and embeddings as the go-to feature representations for text. For some other applications, such as information extraction, or in the examples we saw in the previous section, it’s more common to look for handcrafted, domain-specific features. Quite often, a hybrid approach that combines both kinds of features are used in practice. Having said that, vectorization-based approaches are a great starting point.</p>

<p>We hope the discussion and various perspectives presented in this chapter gave you a good idea about the role of text representation in NLP, different techniques for representing text, and their respective pros and cons. In the next chapters, we’ll move on to solving some of the essential NLP tasks (Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#text_classification">4</a>–<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.xhtml#topics_in_brief">7</a>), where we’ll see different text representations being put to use, starting<a contenteditable="false" data-primary="text representation" data-startref="ch04_term1" data-type="indexterm" id="idm45969608529640"/> with text representation.</p>
</div></section>
<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ch03fn1"><sup><a href="ch03.xhtml#ch03fn1-marker">i</a></sup> It is sometimes also referred to as the <em>term vector model<a contenteditable="false" data-primary="term vector models" data-seealso="vector space models" data-type="indexterm" id="idm45969609763720"/></em><em>,</em> but we’ll stick to the notation VSM.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ch03fn2"><sup><a href="ch03.xhtml#ch03fn2-marker">ii</a></sup> This mapping is arbitrary. Any other mapping works just as well.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ch03fn3"><sup><a href="ch03.xhtml#ch03fn3-marker">iii</a></sup> Technically speaking, both E and E’ are two different learned embeddings. You can use either of them or even combine the two by simply averaging them.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ch03fn4"><sup><a href="ch03.xhtml#ch03fn4-marker">iv</a></sup> Unless they’ve been fine-tuned for the task at hand.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609795816">[<a href="ch03.xhtml#idm45969609795816-marker">1</a>] Bansal, Suraj. <a href="https://oreil.ly/8dbxV">“Convolutional Neural Networks Explained”</a>. December 28, 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609761560">[<a href="ch03.xhtml#idm45969609761560-marker">2</a>] Manning, C., Hinrich Schütze, and Prabhakar Raghavan. <em>Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press, 2008. ISBN: 978-0-521-86571-5</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609621832">[<a href="ch03.xhtml#idm45969609621832-marker">3</a>] Jurafsky, Dan and James H. Martin. <a href="https://oreil.ly/Ta16f"><em>Speech and Language Processing</em>, Third Edition (Draft)</a>, 2018.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609188840">[<a href="ch03.xhtml#idm45969609188840-marker">4</a>] scikit-learn.org. <a href="https://oreil.ly/ukjam">TFIDF vectorizer documentation</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609164040">[<a href="ch03.xhtml#idm45969609164040-marker">5</a>] Firth, John R. “A Synopsis of Linguistic Theory 1930–1955.” <em>Studies in Linguistic Analysis</em> (1968).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_3_5">[<a href="ch03.xhtml#footnote_3_5-marker">6</a>] Ferrone, Lorenzo, and Fabio Massimo Zanzotto. <a href="https://oreil.ly/c4x8M">“Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: A Survey”</a>, (2017).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_3_3">[<a href="ch03.xhtml#footnote_3_3-marker">7</a>] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. <a href="https://oreil.ly/q35QS">“Efficient Estimation of Word Representations in Vector Space”</a>, (2013).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609090168">[<a href="ch03.xhtml#idm45969609090168-marker">8</a>] Google. <a href="https://oreil.ly/tYfdH">Word2vec pre-trained model</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609086056">[<a href="ch03.xhtml#idm45969609086056-marker">9</a>] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. <a href="https://oreil.ly/3f1E5">“GloVe: Global Vectors for Word Representation”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969609081944">[<a href="ch03.xhtml#idm45969609081944-marker">10</a>] Facebook. <a href="https://oreil.ly/9qj4C">fastText pre-trained model</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608920296">[<a href="ch03.xhtml#idm45969608920296-marker">11</a>] Ruder, Sebastian. Three-part blog series on word embeddings. <a href="https://oreil.ly/OkJnx"><em class="hyperlink">https://oreil.ly/OkJnx</em></a>, <a href="https://oreil.ly/bjygp"><em class="hyperlink">https://oreil.ly/bjygp</em></a>, and <a href="https://oreil.ly/GHgg9"><em class="hyperlink">https://oreil.ly/GHgg9</em></a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608916088">[<a href="ch03.xhtml#idm45969608916088-marker">12</a>] Rong, Xin. <a href="https://oreil.ly/Z8KUe">“word2vec parameter learning explained”</a>,  (2014).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608911704">[<a href="ch03.xhtml#idm45969608911704-marker">13</a>] Levy, Omer, Yoav Goldberg, and Ido Dagan. “Improving Distributional Similarity with Lessons Learned from Word Embeddings.” <em>Transactions of the Association for Computational Linguistics</em> 3 (2015): 211–225.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608910456">[<a href="ch03.xhtml#idm45969608910456-marker">14</a>] Levy, Omer and Yoav Goldberg. “Neural Word Embedding as Implicit Matrix Factorization.” <em>Proceedings of the 27th International Conference on Neural Information Processing Systems</em> 2 (2014): 2177–2185.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608907320">[<a href="ch03.xhtml#idm45969608907320-marker">15</a>] RaRe Technologies. <a href="https://oreil.ly/4dG6S">gensim: Topic Modelling for Humans</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608819272">[<a href="ch03.xhtml#idm45969608819272-marker">16</a>] Explosion.ai. <a href="https://spacy.io/">“spaCy: Industrial-Strength Natural Language Processing in Python”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608804696">[<a href="ch03.xhtml#idm45969608804696-marker">17</a>] <a href="https://oreil.ly/OKbU8">word2vec-toolkit Google Group discussion</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608816024">[<a href="ch03.xhtml#idm45969608816024-marker">18</a>] Code for: Kim, Yoon. <a href="https://oreil.ly/QWqwT">“Convolutional neural networks for sentence classification”</a>. (2014).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608742760">[<a href="ch03.xhtml#idm45969608742760-marker">19</a>] Facebook Open Source. <a href="https://fasttext.cc">“fastText: Library for efficient text classification and representation learning”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608740200">[<a href="ch03.xhtml#idm45969608740200-marker">20</a>] Facebook Open Source. <a href="https://oreil.ly/sycR0">“English word vectors”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608727464">[<a href="ch03.xhtml#idm45969608727464-marker">21</a>] Le, Quoc, and Tomas Mikolov. “Distributed Representations of Sentences and Documents.” <em>Proceedings of the 31st International Conference on Machine Learning</em> (2014): 1188–1196.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608675672">[<a href="ch03.xhtml#idm45969608675672-marker">22</a>] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30 (NIPS 2017): 5998–6008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608674328">[<a href="ch03.xhtml#idm45969608674328-marker">23</a>] Wang, Chenguang, Mu Li, and Alexander J. Smola. <a href="https://oreil.ly/HZ4Mh">“Language Models with Transformers”</a>, (2019).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608667752">[<a href="ch03.xhtml#idm45969608667752-marker">24</a>] Allen Institute for AI. <a href="https://oreil.ly/PKRst">“ELMo: Deep contextualized word representations”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608664984">[<a href="ch03.xhtml#idm45969608664984-marker">25</a>] Google Research. <a href="https://oreil.ly/7oYQo">bert: TensorFlow code and pre-trained models for BERT</a>, (GitHub repo). Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608660616">[<a href="ch03.xhtml#idm45969608660616-marker">26</a>] Pilehvar, Mohammad Taher and Jose Camacho-Collados. “Embeddings in Natural Language Processing: Theory and Advances in Vector Representation of Meaning.” <em>Synthesis Lectures on Human Language Technologies</em>. Morgan &amp; Claypool, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608656792">[<a href="ch03.xhtml#idm45969608656792-marker">27</a>] Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” <em>Advances in Neural Information Processing Systems</em> 29 (NIPS 2016): 4349–4357.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608642760">[<a href="ch03.xhtml#idm45969608642760-marker">28</a>] <a href="https://redis.io">Redis</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608633512">[<a href="ch03.xhtml#idm45969608633512-marker">29</a>] Smith, Noah A. <a href="https://oreil.ly/DIA_h">“Contextual Word Representations: A Contextual Introduction”</a>, (2019).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608620888">[<a href="ch03.xhtml#idm45969608620888-marker">30</a>] Maaten, Laurens van der and Geoffrey Hinton. “Visualizing Data Using t-SNE.” <em>Journal of Machine Learning Research</em> 9, Nov. (2008): 2579–2605.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608618024">[<a href="ch03.xhtml#idm45969608618024-marker">31</a>] Le Cun, Yann, Corinna Cortes, and Christopher J.C. Burges. <a href="https://oreil.ly/qv6ao">“The MNIST database of handwritten digits”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608613368">[<a href="ch03.xhtml#idm45969608613368-marker">32</a>] Rossant, Cyril. <a href="https://oreil.ly/0tN2S">“An illustrated introduction to the t-SNE algorithm”</a>. March 3, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608606664">[<a href="ch03.xhtml#idm45969608606664-marker">33</a>] Turian, Joseph, Lev Ratinov, and Yoshua Bengio. “Word Representations: A Simple and General Method for Semi-Supervised Learning.” <em>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</em> (2020): 384–394.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608592296">[<a href="ch03.xhtml#idm45969608592296-marker">34</a>] TensorFlow. <a href="https://oreil.ly/JLXGL">“Word embeddings”</a> tutorial. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608589544">[<a href="ch03.xhtml#idm45969608589544-marker">35</a>] Wattenberg, Martin, Fernanda Viégas, and Ian Johnson. “How to Use t-SNE Effectively.” <em>Distill</em> 1.10 (2016): e2.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608582280">[<a href="ch03.xhtml#idm45969608582280-marker">36</a>] Dai, Andrew M., Christopher Olah, and Quoc V. Le. <a href="https://oreil.ly/gyaiC">“Document Embedding with Paragraph Vectors”</a>, (2015).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608577624">[<a href="ch03.xhtml#idm45969608577624-marker">37</a>] TensorFlow. <a href="https://oreil.ly/eGpUV">“Embedding Projector”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="footnote_3_34">[<a href="ch03.xhtml#footnote_3_34-marker">38</a>] Educational Testing Service. <a href="https://oreil.ly/cJ6uK">“TextEvaluator”</a>. Last accessed June 15, 2020.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608554984">[<a href="ch03.xhtml#idm45969608554984-marker">39</a>] Educational Testing Service. <a href="https://oreil.ly/dksDo">“Automated Scoring of Written Responses”</a>, 2019.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608544456">[<a href="ch03.xhtml#idm45969608544456-marker">40</a>] Chiticariu, L., Yunyao Li, and Frederick R. Reiss. “Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!” <em>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em> (2013): 827–832.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45969608541912">[<a href="ch03.xhtml#idm45969608541912-marker">41</a>] Suganthan G.C., Paul, Chong Sun, Haojun Zhang, Frank Yang, Narasimhan Rampalli, Shishir Prasad, Esteban Arcaute, et al. “Why Big Data Industrial Systems Need Rules and What We Can Do About It.” <em>Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</em> (2015): 265–276.</p></div></div></section></div>



  </body>
</html>